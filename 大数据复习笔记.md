# 面试笔记

### 一、数仓

##### 1.两套架构

```
hive  on spark RDD计算
spark on hive  Datefream DateSet计算
hivesever2主要是给客户端提供jdbc接口的，如果使用beeline客户端连接hive，走的是jdbc，如果不用beeline客户端jdbc就可以不起
```

##### 2.修改环境变量

```
source etc/profile.d/my_env.sh
```

##### 3.lzop

```
lzo不支持切片
lzp支持切片
```

##### 4.单双引号

```
只看最外层，最外层是双引会解析，最外层是单引都不会解析
```

##### 5.两种服务

```
1.HiveServer2 客户端连接到服务,连接到hive
2.thirftServer 客户端连接到服务，连接到sparkSql
```

##### 6.ods原始数据层

```
DWD（data warehouse detail）
明细 <==> 汇总
流水账
汇总：对账单进行统计，明细数据进行聚合
DWD所有的明细数据进行汇总
DWS（data warehouse service ）汇总数所谓的需求都是汇总的结果；
DWT（data warehouse topic）汇总数据，比dws汇总更加的聚合
ADS(aolication data store )数据应用层
DIM（dimension）维度层
```

##### 7.数据仓库进行维度建模 

```
DWD层需要维度层的数据
DWS层需要维度层的数据======> 从ODS层就已经是各种各样的表了，就已经保存在hive中
DWT层需要维度层的数据
ODS层原始数据层：存放原始数据， 直接加载原始数据,保持数据原貌
```

##### 8.ODS

```
往ODS中倒数据怎样导数据
往hive的一张表中写数据使用load方法
往hive中写数据的两种方法：
1.insert +select
2.load
hdfs 中的文件加载到hive中使用load
```

##### 9.DWD

```
对ODS中数据进行清洗
清洗：对数据进行转化，脱敏，把数据整理干净整齐
ODS中的数据还是JSON字符串的方式，从ODS到DWD层写数据的时候对日志进行解析，放在清洗里边去说；
数据规整：去空值，仓数据，超过极限范围的数据
脱敏：脱除去除敏感信息，例如用户表的手机号，身份证号 等信息不能明文存储在数据仓库当中，有泄露用户隐私的风险
脱敏：MD5加密，手机号中间数据进行加*号处理，身份证号最后四位进行加*号处理
对日志字符串进行解析处理，不能以JSON字符串的方式处理，要对JSON中的每一个字段解析到表当中
DWD中保存的是最明细的数据	，保存业务事实的明细数据
业务事实：订单表，订单业务，退款表，支付表，加购物车表，收藏业务
DWD层每一层代表一次业务行为
总结：
DWD => 1.DWD需要对ODS层中的数据进行规整，包括了数据的过滤，数据的脱敏，日志的解析
	   2.规整到只存储事实明细
```

##### 10.DWS

```
以DWD为基础，对数据进行轻度汇总，按天汇总，一个主题对象的一天的行为汇总到一起
最后得到的结果 => 一个用户一天的下单次数
 			  => 一个商品一天被下单了多少次
 			  => 一个商品一天被支付了多少次，被退款了多少次
 			  => 每个优惠券每天被领用了多少次，使用了多少次，过期了多少次 
```

##### 11.DIM

```
维度，对业务事实的描述信息，何人何时何地
有几个维度，就会有几个主题
数仓中划分主题，主要按照维度划分，用户维度，地区为度，时间维度，商品维度，火毒维度，优惠券维度
主题对象指的是一个一个的维度对象
```

##### 12.DWT

```
对数据进行累计汇总 => 把一个主题对象的累计行为
得到结果 => 1.一个用户从注册那天开始至今一共下了多少次单
		   2.一个商品从上架至今一共被下了多少次单，一共被支付了多少次，汇款了多少次，好评了多少次，差评了多少次
```

##### 13.ADS

```
用来存储结果
总结：一个主题一张表：用户主题表，商品主题表，地区主题表 
分层的好处：
1.复杂问题简单化（分层不需要写大sql，分层把sql分开写，具体看那一层出错了就可以，方便定位为题）
2.减少重复开发（可以从中间层去拿数据，避免从原始数据中拿数据）
3.隔离原始数据（ODS），【对数仓进行权限管理，一般数仓对ODS层设限，因为ODS层会有很多敏感数据，手机号，身份证号】
```

##### 14.分层

```
ODS：存储原始数据
DWD:最明细的业务事实数据
DIM:维度数据，对业务的描述信息
DWS：按天汇总的数据
DWT：累计汇总的数据
ADS：存储的最终结果
```

##### 15.数据集市

```
数据集市是微型的数据仓库：是部门级别的
数仓是企业级别的
数据集市的两种模式：
1.独立型数据集市【不需要额外搭建数仓，每个部门之间是独立的，可能出现不同部门间数据不一致，称之为数据孤岛】
2.从属型数据集市【各个部门之间的数据一致，不会产生歧义；需要搭建数仓，部署需要更久一点，消耗的资源多一些】
```

##### 16.数据理论

```
关系建模：范式理论是给关系模型使用的
维度建模：维度和事实
```

```
范式理论：设计数据库表的时候需要遵循的章法；
范式的目的 => 降低数据的冗余性
范式是给关系模型使用的 => 关系模型是伴随着关系型数据库使用的
数据冗余 => 数据的一致性就得不到保证 => 同一条信息出现在多个地方
```

```
关系模型：现在存储的代价没有那么高，也有了较好的扩展性【搭建集群】，因此叶润许一部分的冗余 => 数据冗余，查询时不需要join，查询的性能就会提高 => 很多关系型数据库在查询表时也不会遵循表的三范式； 
```

##### 17.三范式

```
第一范式必须满足：表属性不可切割
第二范式：不能存在非主键字段部分函数依赖主键字段的现象
【字段部分依赖函数主键】
姓名 => 部分依赖 主键：学号+姓名
系名 => 部分依赖 主键：学号+姓名
分数 => 完全依赖 主键：学号+姓名
第三范式：不能存在传递函数依赖【不能存在非主键字段传递函数依赖主键字段的现象】
函数 => 以主键作为x，非主键作为y;
```

##### 18.维度建模

```
事实表里边的信息：维度外键+度量值
维度表里边存储的时描述信息
维度模型的优点：
1.结构简单，join少，大数据情况下shuffle少，查询性能就会好
2.维度模型的设计是面向业务的，更容易理解
3.维度模型是以数据分析为出发点，维度模型更容易做数据分析，尤其是多维分析
例如：对分区进行分组，对度量值进行聚合 => 每个地区的订单金额
	 对品类名称进行分组，对salesAmount进行聚合 => 得到的不同品类的订单总额

	 找到事实表，找到对应的维度，对其join，对维度进行分组，对事实表里边的度量值进行聚合
维度模型更适合做数据分析
维度表是对事实的描述信息，事实表是对应不同的业务
```

##### 19.事实表的特征

```
事实表数据不稳定：每天都有大量的数据产生；数据量较大
维度表数据相对稳定，数据量较小
事实表中每行数据代表一个业务事件（下单、支付、退款、评价）
度量值：次数、个数、金额
  
1.量非常的大【每天都会有新增】
2.内容相对较窄：列数较少【主要是外键和度量值salesAmount】
3.经常变化，每天都会新增
一个业务事件 => 业务事实表[维度外键，自己的度量值] 
维度表各种各样的描述信息，描述业务事实的 

维度表：数据量比较小，内容相对稳定，字段比较多
事实表：数据量比较大，内容不固定，字段比较少
```

##### 20.事实表的分类

```
1.事务型事实表 => 【存储所有的明细操作】一行数据指代的是具体的业务事件，特点：业务写进来就不会再变了
2.周期型快照事实表=> 不会保留所有数据，只会保留固定时间间隔的数据
例如：对购物车的添加修改，删除等
一天一个快照，不保留具体明细操作，只保留时间间隔的数据
3.累计型快照事实表=> 累计快照适用于跟踪业务事实的变化,累积型快照事实表是分多次累积写入的
```

```
事务型事实表 => 适用于不会发生变化的业务，比如评价 => 增量同步
周期型快照事实表 => 不需要记录所有的明细操作，只关注结果 => 全量同步
累计型快照事实表 => 会发生周期型变化的业务 => 业务量大，也会修改 => 新增及变化
```

##### 21.维度模型的分类

```
1.星型模型 => 数据冗余，查询效率比较高
2.雪花模型 => 对维度表进行了规范化，维度表进行了切开 => 冗余度比较低,查询时需要join,查询效率比较低
3.星座模型 => 多个星型模型交织在一起,组成的星座模型
```

```
[不同事实表之间有共用的维度]
星座模型不是一个单独的模型,是很多数据仓库的常态;多个事实表公用几个维度表
星座模型就是基于多个事实表
选择:性能优先 => 选择星型模型,维度更小的星型模型，减少join，减少shuffle
```

##### 22.数仓的搭建

```
数据仓具建模:
1.需要建哪些表
2.哪些表有哪些个字段
3.哪些表跟哪些表有关联关系
原始数据有两类:
1.用户行为日志
2.业务数据
```

```
ODS
ODS层的日志表:只有一个表,按照天进行分区
存储hdfs的路径:/origin_data/gmall/topic_log/2020-06-14
ODS层的业务数据表: 
一天一个路径:/origin_data/gmall/db/activity_info/2020-06-14
建表:mysql中有哪些表就建哪些表,有哪些字段就建哪些字段,MySQL中有哪些关系就建哪些关系
关于ODS层分区的问题?
日志中没有历史数据,业务数据中有历史数据
```

```
DIM层和DWD层
主要进行维度建模
DWD层主要存储的是事实表
DIM层主要存储的是维度表
维度建模?
1.需要建哪些表?
2.表中间有什么字段?事实表有什么字段?维度表有什么字段?
3.哪个事实表跟哪个维度表有关系?
维度建模一般按照的四个步骤:
1.选择业务过程
2.声明粒度
3.确认维度
4.确认事实

[1]选择业务过程:确定需要建立哪些表?一条业务线对应一张事实表[下单业务,退款业务,物流业务]

数据越明细,说明粒度越细
数据越汇总,说明粒度越粗

[2]声明粒度:
声明粒度意味着精确定义事实表中的一行数据表示什么,应该尽可能选择最小粒度
粒度最小,能够满足后边个各种各样的汇总需求

先确定需要哪些表?
确定哪些表的一行数据表示什么?

订单事实表中,最细的粒度是一行数据表示的是一个订单中的一个商品项
支付事实表中一行数据表示的是一个支付记录

[3]确认维度:
确认每一张事实表的相关的维度
订单相关的:用户维度,时间维度,地区维度,有没有使用优惠券,有没有参与活动
评价业务:哪个用户,什么时候,评价了什么商品
领券相关的业务:谁哪个时候,领了哪些券

确定了维度,就相当于确定了每张事实表的一类字段
事实表一共有两类字段:
1.维度外键
2.度量值

[4]确认事实
事实指的是次数,个数,件数,金额,可以进行累加的
可以进行累加:不是所有的数字都可以进行累加的,例如,不能对商品单价进行累加聚合作为度量值
确定了事实表中的第二类字段,度量值

所有的事实表都放在DWD中,所有的维度表都放在DIM层

维度建模的过程:
选择业务过程 => 声明粒度 => 确认维度 => 确认事实

数据都选择最细粒度,满足各种需求,但是最细粒度数据量肯定特别多,有些需求没必要从最细粒度中拿数据[order_detail],从[order_info]中也能拿数也能得到结果;
所以建立两个事实表,一个为最细粒度,一个为粗一点的粒度;如果需要最小粒度就从最小粒度中拿数,不需要最小粒度就会从order_info中拿数据
如果数据都只保留最细粒度,数据量确实会比较大,会存在一定的计算资源的浪费

维度建模只针对业务数据,对关系型数据库导过来的数据进行建模

日志还在ODS层放着,将日志解析成一张一张的表,把JSON里边一个一个的字段给他提取出来,
解析日志在哪个位置做的?
解析完日志的表存=存放在哪里?

日志是在ODS往DWD层写入的时候进行解析的,解析完的日志表也放在DWD层
```

```
日志应该如何解析?
页面日志,启动日志都混动放在ODS层中,将不同的日志解析成不同的表,每张不同的字段

采集用户行为日志,按照内容分为类?
1.页面数据
2.错误数据
3.曝光数据
4.动作数据
5.启动数据
按照日志的结构,分为两类:
1.页面日志
2.启动日志

把日志一个一个字段解析成表结构:按照内容解析比较合适,所有的曝光都是那几个字段,所有的启动都是那几个字段,所有的错误都是那几个字段
将日志解析成5张日志表:
1.页面数据表
2.错误数据表
3.曝光数据表
4.动作数据表
5.启动数据表
  5张日志表其实也属于事实表
```

```
======================DWS和DWT需要如何进行建模呢?
DWS和DWT由后续需求决定的

按照维度进行分组,按照度量值进行聚合,会造成重复计算问题.
DWT层与DWS层避免数据的重复计算,提高数据的复用性.
设计一张地区宽表:
其主键为地区ID,字段包含为
维度模型当中,跟地区维度相关的,事实表 的度量值的聚合值

宽表的主键就是维度的ID,
其余字段是维度模型中,跟该维度相关的所有事实表的度量值的聚合值;
DWT和DWS设计之后就能够保存数据的重复计算了吗?
现在有一个大宽表,现在从DWD层,DIM层来拉取数据,把数据聚合起来放到宽表当中,宽表中的有些字段能够一次性的求出多个值,求出来后把结果保存在宽表当中,之后再有需求直接拿结果就行,不用每次都从DWD和DIM层拿数据去进行计算,相当于对中间结果保存下来,避免数据的重复计算;
DWS和DWT层两个表是上下对应的
DWS和DWT总结:
1.需要建哪些宽表:以维度为基准
2.宽表里边的字段:是站在不同维度的角度去看事实表,重点关注事实表聚合后的度量值
3.DWS和DWT层的区别:DWS层存放的所有主题对象当天的汇总行为,例如每个地区当天的下载次数,下单金额等,DWT层存放的是所有主题对象的累积行为,例如每个地区最近7天(15天,30天,60天)的下单次数,下单金额等;

```

```
维度建模是以业务为驱动的[业务:下单,谁在什么时候,哪些地方,买了什么东西,这种下单业务通过后事实表维度表把这些表达出来]
DWT和DWS宽表层是以需求为驱动:后边大部分需求都是分组聚合的需求,所以里边会有很多的重复计算,才建的宽表
```

##### 23.数仓的运行环境分为2类

```
:[两套架构]
1.hive on spark
使用hive的客户端,建表写sql都是通过hive的客户端去做,不管是belinee还是普通的客户端,一般情况下会使用spark引擎,tize引擎
2.spark on hive
直接使用sparkSql,建表写sql等等,都是使用sparkSql做,spark SQL也会利用到hive的元数据,通过hive的元数据去找hdfs的文件,找到文件后使用spark SQL底层的计算引擎进行计算
这两种架构哪一个更好一些?
spark SQL会更好一些
hive on spark架构:hive客户端,底层配置的是spark引擎,底层是spark RDD API,但是sparksql底层使用的dataframe,dataset,相对于RDD有一些特殊的优化,因为dataframe,dataset存储了结构化数据的每一列的相关信息,类型等等,spark底层会针对这些信息做一些额外的优化,理论上spark SQL性能高于hive on spark
```

```
DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的表格。DataFrame和RDD的主要区别为：前者带有Schema的元数据信息，即DataFrame所表示的二维数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构的信息，从而针对藏于dataframe背后的数据源以及作用于dataframe之上的变换进行针对性的优化，最终大幅度提升运行时的效率的目标。
反观RDD由于无从得知所存储数据元素的具体内部结构，spark core只能在stage层面进行简单、通用的流水线优化。

DataSet是分布式数据集合，DataSet是Spark SQL1.6中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD的优势（强类型，使用强大的lambda函数能力）以及spark SQL优化执行引擎的优点。DataSet可以使用功能性转换（map,flatmap,filter）
DataSet
DataSet是强类型：DataSet[Car]，DataSet[Person]
DataFrame是DataSet的特列，DataFrame=DataSet[Row]

spark-shell => 自动获取spark session对象 => spark session实质上是SQL Context和hiveContext的组合。
spark session内部封装了spark context，所以计算是由spark context完成的
```



##### 24.thriftserver和hiveserver2

```
都是给我们提供jdbc的接口，通过客户端去连接thriftserver和hiveserver2
sql => hiveserver2 => 走hive的计算引擎；mapreduce,spark,taize
sql => thriftserver => 走的的是spark SQL，是dataset,dataframe的API
其实thrift server是对hive的一个封装，两个默认的端口都是10000，两者起一个，否则会有接口冲突
```

##### 25.mysql的驱动说明

```
在启动spark SQL黑窗口客户端的时候，尝试获取hive的元数据，获取hive元数据的方式有两种：
1.直连MySQL；需要mysql的驱动
2.通过hive的meta store获取元数据；不需要驱动，meta store这个服务需要驱动，meta store这个服务有驱动，hive的lib目录下已经放下了这个jar包驱动
因此启动meta store能够在hive的lib目录下找到驱动
但是元数据只有一份，不管使用哪种方式获取，元数据都在MySQL中

总结：
如果说/opt/module/spark/conf，里边的hive-site.xml配置中没有配置meta store服务，那么是直连MySQL，需要拷入驱动；如果配置了meta store服务，那么不需要拷入jar包驱动，但是需要启动meta store服务
```

##### 26.hive的动态分区

```
当往分区表里边写数据时，两种方式：
1.静态分区：指定写入的分区
2.动态分区：指定字段作为分区的字段，会根据分区字段的值决定往哪个分区
配置hive动态分区的参数
/opt/module/spark/conf
    <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
    </property>
```

##### 27.hive部署

```
1.
数据仓库的根目录：hdfs上的路径，当创建一个表时，不指明表的路径，默认仓库路径
<name>hive.metastore.warehouse.dir</name>
<value>/user/hive/warehouse</value>
是否对hive的元数据的表结构进行校验
<name>hive.metastore.schema.verification</name>
<value>false</value>
```

```
1.1
执行加载建表语句
/opt/module/hive/bin/schematool 
. "$bin"/hive --service schemaTool "$@"
hive中包含建表语句
执行时不需要MySQL的主机名，端口号，用户名，密码，通过hive_home环境变量直接找到配置文件
```

```
1.2
hive客户端
beeline客户端
启动hive客户端，是否需要启动两个服务hiveserver2,meta store?
启动beeline客户端，是否需要启动两个服务hiveserver2,meta store?
```

```
1.3
hiveserver2是给我们提供jdbc接口的服务；
hive客户端不是使用jdbc协议，
hive是一个客户端，帮助我们解析sql,解析成MR，提交数据，并不是走的jdbc协议；
```

```
1.4
beeline客户端，走的是jdbc协议，必须得启动hiveserver2；
除了beeline客户端，还有hive的JUI工具，类似于SQLyog,navicat这中工具，hive的DataGrip这些种工具连接hive的时候，
也是走的JDBC协议，想要使用这些工具，hiveserver2服务需要启动；
如果不用这些客户端，就是用普通的hive客户端，那么hiveserver2不需要启动；
```

```
1.5
hivemeta store服务，元数据服务，什么时候需要启动？
meta store服务是给我们提供元数据接口的服务，
通过访问meta store服务能够访问hive的元数据信息
客户端获取元数据的方式有两种：
	(1)直接连MySQL获取元数据，不需要启动metastore
	(2)客户端请求meta store，meta store去读取MySQL元数据,再通过别的协议，thrift协议把数据提供给客户端
```

```
1.6
怎样配置直连MySQL还是通过meta store访问？
配置一个参数：/opt/module/hive/conf/hive-default.xml.template
这个文件中有所有的参数：hive-default.xml.template
hive元数据服务的地址：hive.metastore.uris，这个参数是给客户端配置的，
如果给客户端配置的参数，配置元服务地址，告诉客户端从这个地址中读取元数据，hive客户端就会根据这个地址请求元数据服务，
这个时候就会启动元数据服务；
如果不配置这个参数，那么就不会请求元数据服务，就会直连MySQL
  <property>
    <name>hive.metastore.uris</name>
    <value/>
    <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
  </property>
```

```
1.7
 配置元数据服务，元数据服务的默认端口是9083,hiveserver2的端口号是10000
 <property>
    <name>hive.metastore.uris</name>
    <value/>thrift://hadoop102:9083</value>
    <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
  </property>
  配置元数据参数后，再启动hive客户端，则不能查出数据，需要启动metastore
  查看hive日志：cd /tmp/atguigu => ll hive.log=>tail -500 => could not connect meta store 
启动meta store服务的命令：hive --service metastore
```

##### 28.为什么开发hive

```
HIve能做的事情，Hadoop也能做，，为了开发效率的提高
使用sql提高Hadoop，mapreduce的开发效率
sql要求数据结构化，有行有列有字段
Hive的底层就是mapreduce，就是hadoop的一部分
关心运行效率
开发效率
Hive开发效率快，因为写SQL,不需要写程序
因此sparksql产生
```

```
1.1
hive的sql能够帮助hadoop提高开发效率，
select * from t_user => MR
hive底层生成mapreduce
帮助spark的RDD提高开发效率，使用sql文，再生成RDD
select * from t_user => RDD
Spark([更新快]+Hive(更新慢) => Shark【导致更新慢】
因此衍生出
sparksql[除去hive] and Hive on Spark[hive的引擎用的是spark]【hive的引擎可以使用不同的类型：MR，Spark】 
```

##### 29.RDD

```
RDD只关心数据，不关心数据内部具体的含义：1,zhangsan,30
DATAFram关心数据的结构，是对RDD的一种包装;id,name,age
DataSet：可以理解为面向对象的方式操作sql,把DataFram又包装了一下
区别在于他有类型了，Person
把他们三个合在一起像一个对象
DataSet=>对象
DATAFram=>属性
RDD=>属性值
操作Data就像操作对象一样
操作DataFram就像操作表一样
其实底层就是做了一下封装
```

##### 30.DataFrame运行时间

```
reduceByKey、groupByKey、DataFrame三者的运行时间
groupByKe运行时间最长
reduceByKey有预聚合功能，落盘数据量减小，所以shuffle变快；
DataFrame:底层的优化，比reduceBykeya稍快一些
```

##### 31.视图

```
对于spark应用程序什么会提高性能：shuffle
视图：就是查询结果集，只能查询，不能更改
```

##### 32.sparksession

```
jdbc => connection 连接数据库
spark => 提供sparksession[会话] 连接数据库
```

##### 33.RDD,DateFrame,DataSet

```
RDD怎样转换为DataFram ? 增加结构化信息，即列名
val rdd = sc.makeRDD(List(1,2,3,4))
val df = rdd.toDF/rdd.toDF("id")
df.show
df.createTempView("ids")/df.createGlobalTempView("ids")
spark.sql("select * from ids").show

```

```
val rdd = sc.makeRDD(List((1,"zhangsan",30),(2,"lisi",25)));
val df = rdd.toDF("id","name","age")
```

```
DataStream怎样转化为RDD ? 
df.rdd
此时得到的RDD的存储类型是Row
变成了行对象：org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]
行可以根据顺序，把第一个，第二个，第三个取出来
```

```
DataFrame的两种使用方式
sql:spark.sql("select * from t_user")
dsl:df.printSchema,df.select("id").show
```

```
DataSet是强类型的数据集合
DataFrame是弱类型的数据集合，因为它只有列的概念，没有类型的概念
```

```
DataFrame怎样转换成DataSet ?
val ds = list.toDS => ds.show
ds.createTempView("person")
spark.sql("select avg(age) from person").show

case class Person(name:String,age:long)
val list = List(Person("zhangsan",30),Person("lisi",25))
为什么可以直接构建？list.toDS ?
因为RDD里边有数据，DataFrame里边有结构，DataSet里边有类型
list里边有类型Person，有结构Person里边有结构，也有值
```

```
RDD转换成DataSet
List[Person] = List(Person(zhangsan,30),Person(lisi,40))
val rdd = sc.makeRDD(list)
rdd 里边又有结构又有数据
rdd.toDF
toDF里边的列名可加，也可不加，加的话有列名，不加的话就是默认的列名，1，2，3
val df = rdd.toDF
val ds = df.as[Person]变成DataSet
ds.toDF
val ds = rdd.toDS因为RDD里边是person有结构，有类型，所以能直接转换成DS
```

```
但是如果RDD里边有数据没有结构呢？
val rdd = sc.makeRDD(List(1,2,3,4)) 也可以直接转换
rdd.toDS
res47:org.apache.spark.sql.DataSet[Int] = [value:int]
val rdd1 = sc.makeRDD(List((1,"zhangsan"),(2,"lisi")))
rdd1.toDS
org.apache.spark.sql.DataSet([Int,String]) = [1:int,2:string]
```

```
DataSet转换成RDD
ds.rdd
```

##### 34.spark任务的划分

```
(1)Application:初始化一个spark context就会生成一个application
(2)Job:一个行动算子就会产生一个job
(3)Stage:宽依赖的个数+1
(4)Task:一个stage中最后一个RDD的分区个数就是Task的个数
1.执行main方法 => 初始化sc => 执行action算子
      main    
      => val sc = new SparkContext(new SparkConf().setAppname("SparkContextTest").setMaster("local[*]")) 
      val resultRDD = sc.textFile("input").flactmap(_.split(" ")).map((_,1)).reduceBykey(_+_)
      => resultRDD.collect()
2.DAGScheduler对job切分stage产生Task
3.TaskScheduler通过TaskSet获取job0的所有Task，然后序列化的发往Executor
4.job个数等于action算子的个数     job0:collect() job1:saveasTextFile("output")  
5.job中stage的个数等于宽依赖的个数+1
6.job中Task的个数=一个stage中最后一个RDD的分区个数就是Task的个数
```

##### 35.sparkstreaming

```
sparkstreaming
有了RDD就会有了阶段划分的，作业执行的
DataStream就是管子，RDD就是水流，是离散化的，其实就是对RDD的一种封装
```

##### 36.spark streaming架构

```
采集器[executor]
sparkstreaming中将Spark context包装了一下流的上下文环境对象，streamingContext
接收器[executor]接收数据，将数据包装形成离散化流[不连续]，通过sparkcontext变成RDD，再形成任务发送给executor
```

##### 37.背压机制

```
sparkstreaming的背压机制[spark streaming Backpress]
JobScheduler反馈作业的执行信息，来动态调整Reciver的接收率
spark.streaming.backpressure.enabled = true
当采集数据过快，消费能力不够的时候怎么办？
比如采集数据1秒采集100条数据，但是形成rdd的时候，一秒只能处理10条数据，这样数据就会越积越多
容易造成网络阻塞，就产生生产消费不均衡

spark1.5版本以前：
spark.streaming.receiver.maxRate reciver的动态接收率，限制接受率来适配当前的处理能力，防止内存溢出；
但是如果producer的数据生产高于maxrate,集群处理能力也高于maxRate,就会造成资源利用率下降
1.5以后：
协调资源接收和资源处理能力，可以动态控制数据接收速率适配集群处理能力spark.streaming.backpressure.enabled = true
```

##### 38.优雅的关闭

```
 driver发送数据，executor处理数据
 当executor执行stop的时候，driver不在给executor发送数据，executor处理完当前的任务的时候再关闭
 ssc.stop(stopSparkContext=true,stopGracefully=true)
kill命令：yarn application -kill 后面跟applicationID
 ssc.start
 new Thread(new Runable){
    override def run():Unit={
      Thread.sleep(million=3000)
      val state:StreamingContextState = ssc.getState()
      if(state==StreamingContextState.ACTIVE){
        ssc.stop(stopSparkContext= true,stopGracefully=true)
        System.exit(0)表示将当前的虚拟机都停掉
      }
      
    }
 }.start()

 ssc.awaitTermination()
```

##### 39.sparkstreaming消费Kafka数据方式

```
 DirectAPI:由计算节点的executo来主动消费Kafka的数据，速度由自身控制
 使用阻塞式队列：达到生产消费数据平衡
 当队列的数据满时，不再往队列里边发送数据，当队列里边的数据为空时，不在消费数据
 ReciverAPI:需要一个专门的Executor去接收数据，然后发送给其他的executor做计算，可能能力不匹配
 特别是接收数据的Executor速度大于计算的Executor，会导致计算数据的节点内存溢出
```

```
DirectAPI
  (1)简化并行读取
    如多需要读取多个partition，不需要创建多个DStream然后对他们进行union操作。spark会创建跟Kafkapartition一样多的RDDpartition，
    ，并且会并行的从Kafka中读取数据。所以在Kafka partition和RDD partition之间，有一个1对1的映射关系；
  (2)高性能：如果需要保证数据零丢失，receiver需要开启WAL'write ahead log'机制[该机制会将接收到的数据写入分布式文件系统(HDFS)，
      如果底层失败，从hdfs上边恢复]这种方式效率低，因为数据实际上被复制了两份，Kafka本身就有高可靠性机制，会对数据备份一次，
      而这里又备份到hdfs上边一份；而基于direct的方式，不依赖与receiver，不依赖wal,数据只需要在Kafka中复制了一份，那么就可以通过
      Kafka的数据进行备份；
```

```
spark streaming的分区个数
spark streaming的分区个数与对接的Kafka topic分区个数一致，
spark streaming里一般不会使用repartition算子增加分区，因为repartition会进行shuffle增加耗时
```

##### 30.缩减分区coalesce

```
三个分区的数据经过filter后,剩余三个分区1、2、999条数据，会产生数据倾斜，
但是计算任务需要等到全部计算完成才会进入到下一个阶段
因此需要缩减分区
spark最开始是根据数据集来申请资源的，但是执行后数据减少，它不知道，再运行开始前数据资源先申请好，
有三个分区需要申请3个节点，他并不知道做过滤后数据变没了，如果stage这个作业阶段没有执行完，资源是不会释放的
1、2分区的cpu的核即使计算完毕了，也不会释放，也会等待分区3计算完成后，才会计算下一个任务，浪费资源
=> 缩减分区，把分区1、2的计算任务少的数据缩减成一个分区，申请一个核计算
分区1：1000 filter => 1
分区2：1000 filter => 2
分区3：1000 filter => 999
val rdd = sc.makeRDD(List(1,2,3,4,5,6))
rdd.coalesce(numPartitions = 2)
newRDD.saveasTextFile('output")
shuffle是将数据打乱重新组合，但是coalesce只是重新组合，并没有打乱，所以没有进行shuffle
因此coalesce可以缩减分区，但是数据不会被打乱重新组合
```

```
coalesce合并的原理
如果合并不同节点的数据，那么不管怎么样都需要去不同的节点拉取数据的，这样如果数据多，那么网络中就会传输大量的数据，性能就会变差
coalesce因此缩减分区时，需要合并分区,合并的原则依赖于计算的位置
```

```
coalesc缩减分区
所谓的缩减分区，就是以前的分区数量多，但是由于数据没有那么均衡，希望把一些数据给合并到一块之后呢，资源可以合理的应用，减小任务之间的调度
但是如果coalesce依赖计算的位置合并的话，可能导致，合并后数据倾斜就会更严重的现象，
因此  =>
coalesce默认时没有shuffle的,但是如果想要数据更加均衡一些，可以设定shuffle
rdd.coalesce(numPartitions=2,shuffle=true)
```

```
coalesce方法可以扩大分区，但是默认情况下不起作用
因为coalesce默认不shuffle,数据不会被打乱，所以还是在一起，所以存在空分区
rdd.coalesce(numPartitons=6)
在任务执行之前，就会划分，多少个分区就会被设定好了，再去启动Task,所以知道有6个分区的话，会去申请6个executor或者6个核,
那么就会浪费资源，因为默认不shuffle，还是会维持原来的3个分区，就会有三个分区浪费掉,因为里边没有数据，但是资源已经被申请到；
因此存在空分区，会浪费资源
总结：
[coalesce默认没有shuffle，增大分区后，会申请增大分区后的资源，但是coalesce实际还是原来的分区，所以会浪费资源]
解决方案 ？
rdd.coalesce(numPartitions=6,shuffle=true)
如果传递参数为true，那么可以扩大分区的
```

##### 31.扩大分区

```
repartition重分区
为了区分，扩大缩小分区，不易弄混，repartition就是coalesce的扩大分区的
coalesce就是主要使用缩小分区
rdd.repartition(numPartitions=6)的底层就是 
coalesce(numPartitions,shuffle=true)
因为repartition就是肯定shuffle的coalesce
```

```
repartition和coalesce区别
(1)关系
    两者都是用来改变RDD的Partitioni数量，repartition底层调用的就是coalesce方法：coalesce(numPartitions,shuffle=true)
(2)区别
repartition一定会发生shuffle，coalesce会根据参数判断是否发生shuffle，
一般情况下rdd的repartition数量增大使用repartition，减小partition数量时使用coalesce
```

##### 32.预聚合

```
reduceBykey和groupBykey的区别
reduceBykey具有预聚合功能
groupBykey没有预聚合
在不影响业务的情况下，优先采用reduceBykey进行预聚合
```

##### 33.Kryo序列化

```
Java的序列化比较重（字节比较多），字节多的情况下会增加网络传输的压力，操作反序列化就会更慢
Kryo序列化比Java序列化更加的紧凑，序列化后的字节数比较少，kryo速度是Serializable的10倍
```

```
序列化
分布式当中涉及到网络传输
需要将Driver端的对象发送给executor去执行
```

```
Kryo序列化
Kryo序列化比Java序列化更加的紧凑，序列化后字节数较少，网络传输较快；spark默认的序列化的方式是Java的序列化；并不是spark的序列化
，因为spark并不支持所有类型的序列化；而且每次使用的时候都需要注册；注册针对RDD；
DataFrame和Dataset自动实现了Kyro序列化
```

##### 34.RDD的算子与Scala集合方法的区别

```
  (1)将RDD方法称之为算子的主要原因，就是为了和Scala中集合的方法进行区分
  (2)Scala集合的方法是单点操作，但是RDD的方法是分布的
  (3)RDD算子之内的代码执行在executo端，
```

##### 35.样例类case

```
  (1)默认实现了可序列化接口
  (2)自动生成了伴生对象
  (3)自动生成伴生对象的apply和unplay方法
```

##### 36.spark分区

```
设置分区的位置：
  (1)方法后边传一个参数 => val dataRDD = sc.makeRDD(List("hello", "world", "hello", "scala", "hello"), 2)
  (2)在spark的配置上设定 => sparkConf.set("spark.default.parallelism","2")
  (3)在环境上设定 => SparkConf().setMaster("local[*]").setAppName("wahaha") // local[4]表示4个线程，不写代表1个线程，*表示当前cpu的最大核数
```

##### 37.4核8线程

```
说明当前电脑中有8个虚拟的核， 4个真正的物理核，这种情况下整个电脑都在使用这8个核，你的idea在用，Java进行在用，word在用，小工具在用，
虚拟机在用，都在用的情况下你能真正的使用到这些东西吗，你可能只能使用一个，有时候发现电脑很卡，可能是因为抢到的核被释放了，被别的抢到了，这边就
卡住了，所以在某些情况下没有办法把这8个核都利用到；
但是能做到的就是，在你的这个进程当中增加几个线程，就有机会抢到多个核；
假如8个核都空闲下来，假如你的进程只有一个线程，也没有任何意义；
假如你想利用多个核，前提条件是你得是多线程；
你想要并行计算，想要多核计算，前提是你得要多线程
```

##### 38.stage的耗时

```
sparkstreaming 一个stage的耗时
由最慢的Task决定，所以数据倾斜时某个task运行慢会导致整个sparkstreaming都运行非常的慢
```

##### 39.sparkStreaming窗口函数的原理

```
窗口函数就是在原来定义的spark streaming计算批次大小的基础上再次进行封装，每次计算多个批次的数据，同时还要传递一个滑动步长的参数，
用来设置当次计算完成后，下一批次计算开始的地方；
time1就是spark streaming计算批次的大小，窗口的大小为计算批次的整数倍；计算批次完成窗口滑动的时间就是步长；
```

##### 40.sparkstreaming精准一次性消费

```
Kafka自动提交偏移量，5秒提交一次
1.手动提交偏移量
2.处理完数据之后，在进行偏移量的提交操作
极端情况，如果提交偏移量的时候断网或停电，会造成spark程序第二次启动时重复消费问题，
所以在精确性较高的场合会，使用事务保证精准一次性
```

```
(1)
在未做转换前获取偏移量结束点:
    var offsetRanges: Array[OffsetRange] = null
    //todo 顺手牵羊把本批次的偏移量结束点存入全局变量中。 ConsumerRecord[k, v] k是kafka的分区键，v是值
    val inputDStreamWithOffsetDStream: DStream[ConsumerRecord[String, String]] = inputDStream.transform(
      rdd => {
        //todo 在driver中周期性执行 可以在transform中，或者从rdd中提取数据比如偏移量
        val hasOffsetRanges: HasOffsetRanges = rdd.asInstanceOf[HasOffsetRanges]
        offsetRanges = hasOffsetRanges.offsetRanges
        rdd
      }
    )
(2)
数据处理完成后提交偏移量:
OffsetManagerUtil.savOffset(topic,groupId,offsetRanges)
 //todo 把偏移量写入redis中
  def savOffset( topic:String,groupId:String,offsetRanges: Array[OffsetRange]):Unit ={
    val jedis: Jedis = RedisUtil.getJedisClient
    //todo 把偏移量存储到redis type hash 写入的api?
    // key: topic+consumer_group
    // field partition value offset
    val offsetKey = topic+":"+groupId
    //todo 取分区和偏移量的集合
    val offsetMapForRedis: util.HashMap[String, String] = new util.HashMap[ String,String ]()
    for ( offsetRange <- offsetRanges ) {
      val partition: Int = offsetRange.partition
      val offset: Long = offsetRange.untilOffset
      offsetMapForRedis.put(partition.toString,offset.toString)
    }
    //todo 写入redis
    println("写入偏移量结束点" + offsetMapForRedis)
    jedis.hmset( offsetKey,offsetMapForRedis)
    jedis.close()
  }
(3)
读取偏移量
  //todo 读取redis中的偏移量
  def getOffset( topic:String,groupId:String ) : Map[ TopicPartition,Long ] ={
    val jedis: Jedis = RedisUtil.getJedisClient
    //todo redis type? hash key? topic:consumer_group  filed? partition  value?
    // offset expire? no api hgetall
    val offsetKey = topic+":"+groupId
    val offsetMapOrigin: util.Map[String, String] = jedis.hgetAll(offsetKey)
    jedis.close()
    if ( offsetMapOrigin!=null && offsetMapOrigin.size()>0 ){
      import collection.JavaConverters._
      //todo 转换结构把redis中取出的结构 转换成Kafka中要求的结构
      val offsetMapForKafka: Map[TopicPartition, String] = offsetMapOrigin.asScala.map {
        case (partitionStr, offsetStr) => {
          val topicPartition: TopicPartition = new TopicPartition(topic, partitionStr.toInt)
          (topicPartition, offsetStr)
        }
      }.toMap
      println("读取的偏移量：：" + offsetMapForKafka)

    }
    null
  }
=================================================================
hset <key>  <field>  <value>       给<key>集合中的  <field>键赋值<value>
hget <key1>  <field>              从<key1>集合<field> 取出 value 

```

##### 41.累加器

```
val conf = new sparkconf().setAppname().setMaster("local[1]")
val sc = new SparkContext(conf)
val dataRDD = sc.makeRDD(List(("a",1),("a",2),("a",3),("a",4)))
  (1)打印单词出现次数，代码执行了shuffle，效率低
    dataRDD.reduceBykey(_+_).collect().foreach(println)
  (2)不使用shuffle,打印在executor端
    dataRDD.foreach{
      case(a,count)=>{
        sum = sum + count
      }
    }
    println("sum=" + sum)
  (3)使用累加器实现数据的聚合，累加器求和在Driver端
    //声明系统累加器
    val sum：LongAccumulator = sc.longAccumulator("sum")

     dataRDD.foreach{
      case(a,count)=>{
        //使用累加器
        sum.add(count)
      }
    }
    //获取累加器的值
    println(sum.value)
```

##### 42.RDD持久化

```
  1.序列化，也算作是持久化，把内存中的文件保存到文件中去就叫做序列化，序列化就是持久化的一种方式；
  2.把数据保存到数据库中也算作是持久化，因为数据库中都要放在磁盘中，都叫做持久化
```

##### 43.RDD缓存

```
  RDD不保存数据
  wordToOne.cache()
  wordToOne.persist(StorageLevel.DISK_ONLY)
  // 缓存
  // cache底层调用的是persist方法
  // 这两个方法默认情况下都是将数据保存到内存中。
  // DISK持久化操作只对当前应用有效，应用程序一旦执行完毕，文件就会删除。
```

##### 44.存储的级别

```
  StorageLevel
  DISK_ONLY：只能放在磁盘里边
  MEMORY_ONLY：只能放在内存中；当内存不够的时候，丢弃
  MEMORY_ONLY2：副本
  MEMORY_ONLY_SER：序列化；
  把一个user存在内存中和把一个user序列化后存在内存中是不一样的，序列化可以使用压缩算法存储的内容更少，但是序列化和反序列化都需要占用性能，可以存更多的对象，但是执行就会变慢
  MEMORY_AND_DISK：当memmory不够的时候，会自动的溢写磁盘
```

##### 45.RDD CheckPoint检查点

```
  设置检查点存储路径、设置检查点，但是会执行两次；
  原因：checkpoint功能为了数据的安全，需要重新执行一遍作业
  sc.setCheckpointDir("cp")
  wordToOne.checkpoint()
  一般情况下，checkpoint不是独立使用的，而是和缓存联合使用；
  这样checkpoint功能为了数据的安全，需要重新执行一遍作业的时候只需要在缓存中拿去取数据即可
  wordToOne.cache()
  wordToOne.checkpoint()
```

##### 46.缓存和检查点的区别

```
  (1)Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。
  Checkpoint切断检查点的原因：当数据出现错误时，当我们把数据保存到检查点，检查点是分布式文件系统的话，
  比本地文件更安全，本地文件一旦断电还不能读到，但是分布式文件系统只要整个集群不挂掉，都可以读；这种情况·  下更安全一些，既然更安全，等同于改变了数据源，不需要从头开始读了，从中间的文件开始读，更加安全，还增加   效率了，所以不需要增加血缘
  (2)Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。
    存储的本地磁盘，不是分布式磁盘，所以可靠性相对较低
  (3)建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。
    原因：checkpoint()会重新执行一边作业，所以性能比较差，所以跟cache一起使用
```

##### 47.日志

```
/opt/module/applog_off/log => flume-kafka-flume-hdfs => load data inpath '/origin' into table partition(); 
=> 在表的location位置建压缩
```

```
drop table if exists ods_log;
CREATE EXTERNAL TABLE ods_log (`line` string)
PARTITIONED BY (`dt` string) -- 按照时间创建分区
STORED AS -- 指定存储方式，读数据采用LzoTextInputFormat；
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/warehouse/gmall/ods/ods_log'  -- 指定数据在hdfs上的存储位置
;
```

```
load data inpath '/origin_data/gmall/log/topic_log/2020-06-14' into table ods_log partition(dt='2020-06-14');
```

##### 48.db业务

```
MySQL => sqoop => HDFS路径 => load data inpath'/original/gmall' into table => table表中
```

```
location路径
  (1)sqoop中采用lzo压缩
     -target-dir /origin_data/$APP/db/$1/$do_date \
     hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
     }
  (2)建表table中，采用lzo压缩
    STORED AS -- 指定存储方式，读数据采用LzoTextInputFormat；
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
  (3)在表table位置建立lzo压缩
    hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer 
    /warehouse/gmall/ods/ods_log/dt=2020-06-14
```

##### 49.Sqoop

```
#! /bin/bash

APP=gmall
sqoop=/opt/module/sqoop/bin/sqoop

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$2" ] ;then
   do_date=$2
else 
   echo "请传入日期参数"
   exit
fi 

import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/$APP \
--username root \
--password 123456 \
--target-dir /origin_data/$APP/db/$1/$do_date \
--delete-target-dir \
--query "$2 where \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}

import_order_info(){
  import_data order_info "select
                            id, 
                            total_amount, 
                            order_status, 
                            user_id, 
                            payment_way,
                            delivery_address,
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            expire_time,
                            tracking_no,
                            province_id,
                            activity_reduce_amount,
                            coupon_reduce_amount,                            
                            original_total_amount,
                            feight_fee,
                            feight_fee_reduce      
                        from order_info"
}


import_order_refund_info(){
  import_data order_refund_info "select
                                id,
                                user_id,
                                order_id,
                                sku_id,
                                refund_type,
                                refund_num,
                                refund_amount,
                                refund_reason_type,
                                refund_status,
                                create_time
                              from order_refund_info"
}


import_favor_info(){
  import_data favor_info "select
                          id,
                          user_id,
                          sku_id,
                          spu_id,
                          is_cancel,
                          create_time,
                          cancel_time
                        from favor_info"
}


case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "base_region")
     import_base_region
;;
  "base_trademark")
     import_base_trademark
;;
  "activity_info")
      import_activity_info
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;
  "order_detail_activity")
      import_order_detail_activity
;;
  "order_detail_coupon")
      import_order_detail_coupon
;;
  "refund_payment")
      import_refund_payment
;;
  "sku_attr_value")
      import_sku_attr_value
;;
  "sku_sale_attr_value")
      import_sku_sale_attr_value
;;
  "all")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_region
   import_base_province
   import_base_trademark
   import_activity_info
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
   import_order_detail_activity
   import_order_detail_coupon
   import_refund_payment
   import_sku_attr_value
   import_sku_sale_attr_value
;;
esac

```

##### 50.建模工具是什么

```
powerdesigner/SQLYog/EZDML
```

##### 51.ODS层[Operation Data Store]

```
ODS层[Operation Data Store]原始数据层
  (1)保持数据原貌，不做任何修改，起到数据备份的作用
  (2)数据采用lzo压缩，减小存储空间[例如：原始数据100G，可以压缩到10G左右]
  (3)建立分区表，防止后边全表扫描
```

##### 52.DWD层 [data warehouse detail]

```
DWD层 [data warehouse detail]最明细的业务事实数据
DIM:维度数据，对业务的描述信息
DWD层：1.DWD需要对ODS层中的数据进行规整，包括了数据的过滤，数据的MD5脱敏md5(name),md5(phone_num),md5(email)
       1，日志的解析
       2.规整到只存储事实明细
DWD层：需要构建维度模型，一般采用星型模型
维度建模的四个步骤：
  (1)选择业务 =>
  (2)声明粒度 => 
  (3)确认维度 =>时间维度、用户维度、地区维度
  (4)确定事实 =>度量值

DWD层中以业务过程为建模驱动，构建最细粒度的明细层事实表、事实表可以适当的宽表化处理

注1: a.粒度:
      下单 => 一行数据，一个订单order_info
      明细 => 一行数据，一个订单的一个商品项order_detail
      退单 => 一行数据，一个支付记录
      退款 => 一行数据，一个支付记录
      加购 => 周期型快照，一行数据，一个用户的购物车的一个商品
      收藏 => 周期型快照，一行数据，一个用户的收藏的一个商品
      评价 => 一行数据，一个用户对一个订单里边一个商品的一个评价
      领券 => 一行数据，一个用户对一个优惠券的一次领用
     b.维度：
       => 时间维度
       => 用户维度
       => 地区维度
       => 商品维度
       => 优惠券维度
       => 活动维度
       => 商品仓库
     c.事实[度量值]：
       下单 => 商品原价，活动优惠，优惠券优惠，最终价格
       明细 => 商品件数，商品原价，活动优惠，优惠券优惠，最终价格
       退单 => 件数，金额
       退款 => 退款金额
       加购 => 件数，金额
       收藏 => 1（次数）
       评价 => 1（次数）
       领券 => 1（次数）

注3：a.9张事实表放在DWD中，8张维度表放在DIM中
     b.数仓会保留历史数据，所以会有时间维度；数据分析时候就会多一个分析数据的维度，时间维度，可以看到数据随时间的推移
     c.业务系统中存储的是所有数据的最新状态，例如订单状态，改了之后就会把在原来的状态覆盖掉了
     d.下单时候会有多个sku，退单时需要一个sku一个sku的退
     d.事实表得有度量值，没有明显的度量值时有一个隐含的度量值
注4：维度退化
      根据维度建模中的星型模型思想，将维度进行退化。例如下图所示：地区表和省份表退化为地区维度表，
      商品表、品类表、spu表、商品三级分类、商品二级分类、商品一级分类表退化为商品维度表，活动信息表和活动规则表退化为活动维度表。

===> 数仓的维度建模已经完毕，DWS、DWT和ADS和维度建模已经没有关系了
     DWS和DWT都是建宽表，宽表都是按照主题去建。主题相当于观察问题的角度。对应着维度表。
```

##### 53.DWS层[data warehouse service]

```
DWS层[data warehouse service]:按天汇总的数据
服务于DWT层的主题宽表
DWS层的宽表字段，是站在不同维度的视角去看事实表，重点关注事实表的度量值，通过与之关联的事实表，获得不同的事实表的度量值。
```

##### 54.DWT层[data warehouse topic]

```
DWT层[data warehouse topic]：累计汇总的数据;汇总数据，比dws汇总更加的聚合
以分析的主题对象为建模驱动，基于上层的应用和产品的指标需求，构建主题对象的全量宽表;
```

##### 55.ADS

```
ADS：存储的最终结果
```

##### 56.hive

```
1.
-e不进入hive的交互窗口执行sql语句
bin/hive -e "select id from student;"
================================================================================================
2.
“-f”执行脚本中sql语句
bin/hive -f /opt/module/datas/hivef.sql
=================================================================================================
3.
执行文件中的sql语句并将结果写入文件中
bin/hive -f /opt/module/datas/hivef.sql  > /opt/module/datas/hive_result.txt
```

##### 57.Redis缓存穿透

```
  1.缓存穿透
    缓存穿透是指一个一定不存在的数据；由于缓存命不中会去查询数据库，查不到数据则写不到缓存；这将导致这个查不到的数据每次
    每次请求都要到数据库去查询，造成缓存穿透；
    => 解决方案：
      (1)将空对象也缓存起来，并且设置一个很短的过期时间，最长不超过5分钟
      (2)采用布隆过滤器，将所有有可能的数据hash到一个足够大的bitmap中，一个一定不存在的数据会被这个bitMap拦截掉，从而
      避免对底层系统的存储压力
  2.缓存雪崩
    如果缓存集中在一段时间失效，发生大量的缓存穿透，所有的查询都会落在数据库上，就会造成缓存雪崩
    解决方案：
      (1)尽量让失效时间部分不到同一个时间点
  3.缓存击穿
    指一个key非常的热点，在不停的扛着大并发，当这个key的失效瞬间，持续的大并发就会穿透缓存，直接请求数据库，就像在屏障上，
    直接凿了一个洞；
    解决方案：
    (1)设置key永不过期
```

##### 58.Redis哨兵模式

```
主从复制中反客为主的自动版，如果主机Down掉，哨兵会从从机中选择一台作为主机，并且它设置为其他从机的主机，如果原来的主机再次
  启动的化，也会变成从机；
```

##### 59.Redis数据类型

```
  (1)string
  (2)list
  (3)set
  (4)hash
  (5)zset[带分数的set]
```

##### 60.Redis持久化

```
  (1)RDB持久化 => 快照的形式
    
  (2)AOF:以日志的形式记录每一个更新操作
```

##### 61.maxwell

```
Maxwell-bootstrap:
Maxwell同步数据的方式：
  (1)到MySQL的binlog中读取数据
  (2)到MySQL的binlog中没有数据，到某张固定的表中把表的数据给查出来
    bin/maxwell-bootstrap --user maxwell  --password 123456 --host hadoop102  
    --database gmall_rt_flink --table user_info --client_id maxwell_1
    参数解析：
      bootstrap本身只是把表的数据给读到，但是并不具备处理的功能，处理还得交给Maxwell本身maxwell_1
      在配置文件中：client_id=maxwell_1
      流程：先通过bootstr从user表中，把数据读出来，读完之后由maxwell_1把数据发送给ods层 	
```

##### 62.binlog同步数据的方式

```
binlog_format=statement 语句 执行的sql 
有可能会造成数据的不一致

update xxxx set xxx=123 where id=’99’
update xxxx set xxx=now() where id=’99’
                                 rand()
update xxxx set xxx=123 where id>’99’


update xxxx set xxx=123 where id=’99’
update xxxx set xxx=now() where id=’99’
                                rand()

```

```
binlog_format=row   行变化  保证一致性
大量的冗余
id=99 xx=123 xxx=343 aa=bbb
id=100 xx=123 xxx=343 aa=bbb
..
id=111 xx=123 xxx=343 aa=bbb
...
id=222 xx=123 xxx=343 aa=bbb


id=99 xx=123 xxx=343 aa=bbb
id=100 xx=123 xxx=343 aa=bbb
..
id=111 xx=123 xxx=343 aa=bbb
...
id=222 xx=123 xxx=343 aa=bbb

```

```
binlog_format=MIXED 混合
一般情况使用statement 
特殊情况使用row 

```

##### 63.数据倾斜产生的原因

```
  我们以spark和hive的使用场景为例。
  他们做数据运算的时候会涉及到，count distinct、group by、join on等操作，这些都会触发shuffle操作。一旦触发shuffle，所有
  相同的key就会被拉到一个或者几个Reducer节点上，容易发生单点计算问题，导致数据倾斜。
  一般说数据倾斜的原因：
    (1)key分布不均匀
    (2)建表时考虑不周
      a.user（用户信息表）：userid，register_ip
        ip（IP表）：ip，register_user_cnt
        两个不同人员开发的数据表，一个取不到数据默认为null,另一个默认为0，一旦做关联，这个任务在关联的阶段，也就是sql的on阶段
        卡死
    (3)业务数据激增
      比如订单场景，在某两个省推广订单量增长，统计不同省市的订单状态，groupby 后发生数据倾斜，
```

##### 64.解决数据倾斜的思路

```
  数据预处理
  异常值过滤
  (1)业务逻辑
    两次mr,第一次打散，第二次最终聚合计算。
  (2)程序层面
    select name,count(distinct name)from user;
    select count(1) from (select name from user group by name) tmp;
  (3)调参方面
      haoop压缩，计算密集的少用压缩，IO密集的多用压缩
      Spark官方推荐，task数量应该设置为Spark作业总CPU core数量的2~3倍
  (4)从业务和数据上解决数据倾斜
      很多数据倾斜都是在数据的使用上造成的，我们举几个例子
        a.有损的方法：找到异常数据，比如ip=0,过滤掉
        b.无损的方法：对分布不均匀的数据单独计算
        c.数据预处理
        d.先对key做一层hash,先将数据打散让他的并行度变大，再会集
```

##### 65.定位导致数据倾斜的代码

```
spark数据倾斜只会发生在shuffle中，
一些常用触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition
出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的
```

##### 66.采样sample

```
先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。
  val sampledPairs = pairs.sample(false, 0.1)
  val sampledWordCounts = sampledPairs.countByKey()
  sampledWordCounts.foreach(println(_))
```

##### 67.spark数据倾斜的解决方案

```
  提前在Hive ETL中进行group by或者join等shuffle操作时，避免了在Spark中执行shuffle类算子
  Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。
  我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。
```

##### 68.过滤少数导致倾斜的key

```
比如99%的key就对应10条数据，就只有一个key对应100万数据，导致数据倾斜
如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。
比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。
如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，
然后计算出每个key的数量，取数据量最多的key过滤掉即可。
优缺点：
优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。
缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。
项目实践
在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，
追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，
计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。
OOM，全称“Out Of Memory”，意思是“内存用完了”
```

##### 69.提高shuffle操作的并行度

```
如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。
```

##### 70.两阶段聚合(局部聚和+全聚合)

```

```

##### 71.将reduce join转换为map join

```

```



##### 72.MySQL事务[ACID]

```
 (1)原子性Atomicity:事务开始后所有操作，要么全部做完，要不全部不做，不可能停滞在中间环节。事务
    执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生过一样。也就是说事务是一个
    不可分割的整体，就行化学中学过的原子，是物质构成的基本单位；
 (2)一致性Consistency：事务开始前后结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A
    扣了钱，B却没有收到；
 (3)隔离性Isolation:同一时间，只允许一个事务请求统一数据，不同事务之间彼此没有任何干扰。比如A正从
    一张银行卡取钱，在A取钱的过程结束前，B不可能向这张卡转账；
 (4)持久性Durability:事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚；
```

##### 73.事务的并发问题:

```
  (1)脏读:A读取了事务B更新的数据，然后B回滚，那么A读到的数据是脏数据;
  (2)不可重复读:事务A多次读取同一数据，事务B在事务A多次读取的过程中，对数据做了更新并提交，导致
      事务A多次读取同一条数据时，结果不一致
  (3)幻读:系统管理员对数据库中的所有同学的成绩从具体分数改为ABCDE等级，
      但是系统管理员B就在这个时候插入一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有
      改过来，就像出现幻觉一样，叫做幻读；
小结：不可重复读和幻读容易混淆，不可重复侧重与修改，幻读侧重与新增或者删除。解决不可重复读只需要锁住
    满足条件的行，解决幻读需要锁表;
```

##### 74.hive中内部表和外部表的区别

```
EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），
Hive创建内部表时，会将数据移动到数据仓库指向的路径；
若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。
在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。
```

##### 75.YARN资源调度器

```
Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作平台，而MapReduce等运算程序则相当于
运行于操作系统之上的应用程序；
ApplicationMaster,用户作业生命周期的管理者，它的主要功能就是向ResourceManager(全局的)申请计算资源(Containers),
并且和NodeManager交互来执行和监控具体的task;
架构：
(1)ResourceManager:
    .处理客户端的请求
    .监控NodeManager
    .启动或监控ApplicationMaster
    .资源的分配与调度
(2)NodeManager:
    .管理单个节点的资源
    .处理来自ResourceManager的命令
    .处理来自ApplicationMaster的命令
(3)ApplicationMaster:
    .负责数据的切分
    .为应用程序申请资源并分配给内部的任务
    .任务的监控和容错
(4)Container:
    .Container是YARN中资源抽象，它封装了某个节点上的多维度资源，如内存，CPU,磁盘,网络等
```

##### 76.YARN工作机制

```
(1)MR程序提交到客户端所在的节点
(2)YarnRunner向ResourceManager申请一个Application
(3)RM将该程序的资源路径返回给RarnRunner
(4)该程序将运行所需的资源提交到HDFS
(5)资源提交完毕，申请运行MRAppMaster
(6)RM将用户的请求初始化一个Task
(7)一个NodeManager领取到Task任务
(8)该NodeManager创建容器Container,并产生MRAppMaster
(9)Container从HDFS上拷贝资源到本地
(10)MRAppMaster向RM申请运行MapTask资源
(11)RM将MapTask任务分配给另外两个NodeManager,另外两个NodeManager分别领取任务并创建容器
(12)MRAppMaster向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask,MapTask对数据进行分区排序
(13)MRappMaster等待所有的MapTask运行完毕后，向RM申请容器，运行ReduceTask
(14)ReduceTask向MapTask获取相应的分区的数据
(15)程序运行完毕，MR会向RM申请注销自己
```

##### 77.flink函数

```
 * 自定义函数：
 * UDF：1对1，例如转换大小写                =>FlinkSql标量函数
 * UDTF：一条数据过来转换成多条数据          =>FlinkSql表值函数
 * DAF：聚合函数，sum()、count()求和，求总值 =>FlinkSql聚合函数
```

##### 78.倒排索引

```
倒排索引：三级索引；
		一级【Term Index】，字母树形结构，定位到拆分的单词
		二级【Term Dictionary】，定位哪些行包含这些单词
		三级【posting list】，通过行号找到对应的单词内容
==================================================
ES使用的是倒排索引和Mysql的B+Tree索引不同
如果是btree(二叉树)索引,log2 N ，如果是hash(哈希)索引，时间复杂度是1。
https://blog.csdn.net/weixin_44617285/article/details/105507811
```

```
分词 => TermIndex => TermDictonary =>Postion
通过倒排索引查找到ID，再经过正排索引查到文档
涉及搜索的使用倒排索引：描述，名称，标题
elasticsearch
天然分片 => 把数据分成很多份，shard
elasticsearch如果是单机，也是只有一个节点的集群
倒排索引：
  搜索关键词需要分词，存储的标题也需要分词
```

```
term是分词的意思
TermIndex => TermDictonary => Posting list
TermIndex:是对TermDictonary单词表建了一个二叉树的索引，能够快速定位到查询的位置
三层索引
树形找单词，单词找id，得到id后找到一整行的数据
```

##### 79.关键词匹配

```
1.传统数据库，如果要对关键词匹配，
  select * from skuinfo where skuName like %手机%
  这种方式会严格匹配
  有和没有索引对于MySQL来说就是一个灾难 
  like %%会让索引失效，查询慢
  弊端：
    (1)传统的关系型数据库，对于关键词的查询，只能逐字逐行查询，匹配，性能非常差
    (2)匹配方式不合理，比如搜索“小密手机”，用like匹配，根本匹配不到;
倒排索引：
  1.全文搜索引擎目前主流的索引技术就是倒排索引的方式。
  2.ES是全索引字段数据库
  倒排索引过程：
  (1)数据保存的时候，就已经是按照分词的方式进行存储的
    operation  1,2
    red 1,3
    sea 1,3
    meigong 2
    river 2
    incident 3
  (2)查询条件过来时 会把查询条件进行分词
    operation
    red
    sea
  (3)用两组分词一次匹配
    operation 1,2
    red 1,3
    sea 1,3
  (4)进行相关度评分
    1号文档相关度最高
    其次是3号文档
    最后是2号文档
```

##### 80.Hbase架构

```
RegionServer的作用：
  (1)Data:get,put,delete
  (2)Region:splitRegion,compactRegion
Master的作用：
  (1)Table:create,delete,alter
  (2)RegionServer:分配regions到每个RegionServer,监控每个RegionServer的状态
```

```
HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。
```

##### 80.1hbase为什么这么快

```
列式存储，内存，lsm树
(1)
NoSQL在分布式存储上,可以通过增加价节点的方式进行扩展,很好的进行水平切分
(2)
BlockCache读缓存
```

##### 80.2hbase读写流程

```
(1)
写流程：
1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。
2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。
3）与目标Region Server进行通讯；
4）将数据顺序写入（追加）到WAL；
5）将数据写入对应的MemStore，数据会在MemStore进行排序；
6）向客户端发送ack；
7）等达到MemStore的刷写时机后，将数据刷写到HStoreFile。
(2)
读流程
1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。
2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。
3）与目标Region Server进行通讯；
4）分别在MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。
5）将查询到的新的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。
6）将合并后的最终结果返回给客户端。

```



##### 81.Rowkey设计原则

```
==================================================================
(1)
对uid取hash,
再进行md5加密，
再substring(0,5),再加uid。
MD5【MD5加密，也是散列原则】
(2)
  def getDimRowkey(id:String):String={
    //todo 用数据的主键 1 先补0 补的位数预计是数据增上限  再反转
    val rowkey: String = StringUtils.leftPad(id,10,"0").reverse
    rowkey
  }

==================================================================
一、Hbase热点(数据倾斜问题	)，读写请求会集中到一个RegionServer上，如何处理？
产生问题的原因：
1.	Hbase中rowkey是按照字符串的字典顺序排列和存储的。所以当大量连续的rowkey数据集中写到个别的region中，导致region数据不平衡。(时间戳，随着时间的推移，放的数据在最后一个region中，大量的数据往里边放，导致热点问题)
2.	创建表时，没有预分区。只有一个region
3.	存在预分区，设计的rowkey没有打散分布，可能放置在一个region中(按照规律往一个region中放)
解决方案：
1.	Rowkey设计时，尽量不要采用时间戳，最好使用随机数+业务主键(时间戳可能会导致数据倾斜)
2.	Rowkey设计时不要太长(10-100Byte)(列存储中，每一个列中都要保存rowkey,如果rowkey设计很长的话就会导致大量的数据冗余，占用更多的内存空间，占的空间越多就会容易触发切分，合并，溢出等操作)
3.	Rowkey设计时将同一批(查询)数据保存在一起
（把昨天和今天的数据分开，便于查询
时间戳反转
20210113号数据放在一个region
31101202xxx
31101202yyy
31101202zzz
20210112号数据放在一个region
21101202aaaaa
21101202bbbb
21101202ccccc
查询：{STARTROW => ‘21101202’,STOPROW => ‘31101202’}
两种数据局部有规律，整体没有规律
）
4.	设计预分区（没做预分区，导致每个数据都往最后一个里边放，导致热点问题；假如提前设定16个分区，每个分区的范围都给规定好，那么数据就都不会往最后一个region里边放了，就会均衡的往region里边放 ）
二、
	Hbase查询一条记录的方法是什么？Hbase写入一条记录的方法时什么？
1.	Scan查询一个范围（API）
2.	Get底层也是scan,查询一条
	描述hbase的rowkey的设计原理
1.	唯一性原则：保证rowke不重复，最好将相关的rowkey保存在一起
2.	长度原则：rowkey不要太长，会导致有大量的内存冗余。不要太短，会导致查询不方便，推荐10-100Byte（最好是8的整数倍【因为是字节数组，8的整数倍会更好一些】）
3.	散列原则：为了能够打散数据，让数据分布的更加均匀
	哈希
	MD5【MD5加密，也是散列原则】
	字符串反转（时间戳反转）【列如都是2021年的数据：20210112，20210113，开头相同会导致数据都放到一块了；但是反转之后开头不同，3开头的放一块，2开头的放一块；
{查询：
当1001 <=rowkey <1002条件时，
1001010101，
1001010102，
1001010103，
10010101a1等这些数据都会被查询出来
因为是按照字典顺序进行排列，而且按照字节码的方式来进行比对，那么1001010101就会比1001大，但是又不到1002，因此这些数据就都会被查询出来}】
	Hbase中compact的用途是什么，什么时候触发，分为哪两种，有什么区别？
1.	当hbase中memstore进行flush时，会产生很多的文件，如果文件过多会导致性能下降，所以需要进行合并操作（Hbase自我调节）
2.	Minior Compact & Major Compact
【MIniorCompact会选择小的文件及靠的比较近的文件进行合并，Major Compact会选择所有的文件合并形成一个大的文件】
3.	触发时机
	Flush后，判断是否需要compact【】
	内部线程周期轮询【Major7天时间会有一个合并，但是我们一般不使用】
	讲一下Hbase,Hbase二级索引用过吗
1.	一般会将rowkey作为一级索引
2.	所谓的二级索引，其实就是将业务字段放置在rowkey中，形成冗余，提高查询效率。
	
Rowkey	name	age
1001	zhangsan	30
1002	lisi	40
1003	zhangsna	50
怎样设数据能够快速查询where name = zhangsan
rowkey	Rk1	Rk2
zhangsan	1001	1003
当查询张三时，能够得到对应的每一个真正的rowkey,在拿1001，1003去表1查询，速度更快，效率更高；拿空间换时间（多增加一张表，存储空间更大，但是查询效率会好很多；
例如表一有100万条数据，那么100万条数据挨个去查太慢，而且hbase本身查询效率就慢，但是如果转换rowkey，zhangsan那么效率就会好很多 ,这种方式就叫做二级索引，这种二级索引并不是真正的索引，而是用转换将zhangsan当成rowkeyt，感觉是二级索引的感觉，但不是真正的索引
{所谓的二级索引，就是用一些框架帮你创建新的表，实现了查询的优化}
）
	Hbase如何优化的
	JVM：GC,内存大小【内存大小，内存越多，溢写磁盘的可能性越小】
	预分区，禁用major合并，手动合并【不使用重量级的合并，手动合并】
	客户端批处理操作【别一个一个的处理，map跟mapPaitition的区别，批处理比一个一个处理的快】
	Hbase-size配置文件 
	Hbase表设计有哪些注意点
	Rowkey如何设计
1.	唯一性原则
2.	长度原则
3.	散列原则
	Region：预分区
设置预分区，提前把分区的范围规定好，那么数据就不会往一个region里边放，不会存在数据倾斜问题
	Colum family：包含哪些列族，最好不要太多，推荐2-3个【每个列族都会形成一个store，一个store就会有一个metastore,太多的话每一个metastore放不了太多东西就会溢写磁盘；
所有的metastore有一个内存大小的限制，metastore过多就会导致数据没放太多，就会溢写磁盘】
	Cell:判断列祖中有哪些列
	Timestamp：时间戳【拿当前的系统时间还是别的时间】
	Hbase与mySQL的区别
	存储方式
	MySQL行存储，利于查询
	Hbase列存储，利于统计和压缩
	数据关系
	MySQL存储关系型数据，结构化数据
	Hbase存储的结构化数据和非结构化数据，非关系型数据【因为底层使用的是字节数组】
	事务处理
	MySQL存在事务
	Hbase侧重于海量数据的存储，所以没有事务【插入同一行数据保证是有事务的，保证同时成功】
	存储容量
	MySQL的存储容量取决于硬件【硬件容量有限】
	Hbase基于hdfs，容量非常大【可以扩容】
	Hbase有put方法，那如何批量put数据进hbase里，用什么方法
	Put(List<Put>)
	API
	Hbase RoeweKey的设计原则（）
	长度原则【10-100 Byte不能太长，也不能太短】
	散列原则【没有规律，可以均匀分配，不产生数据倾斜和热点问题】
	唯一原则【类似于关系型数据库的主键，具备唯一性】
	Hbase,Hive和redis的区别？
	Redis称之为内存数据库
	Hive是一个工具，可以简化mapreduce的开发，把数据按照一些表的结构来进行存储
	Hbase就是一个数据库
	Hbase的读写原理
	写数据流程
1)	Client先访问zookeeper，获取hbase:meta表位于哪个RegionServer;
2)	访问对应的RegionServer，获取hbase:meta表，根据请求的namespace：table/rowkey,查询出目标数据位于哪个RegionServer中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问；
3)	与目标RegionServer进行通信；
4)	将数据顺序写入（追加）到WAL【预写日志】；
5)	将数据写入到对应的MemStore,数据会在MemStore进行排序；
6)	向客户端发送ack;
7)	等达到MemStore的刷写时机后，将数据刷写到HstoreFile。
	读数据流程
1)	Client先访问zookeeper，获取hbase:meta表位于哪个RegionServer;
2)	访问对应的RegionServer,获取hbase:meta表，根据读请求的namespace：table/rowkey,查询出数据位于哪个RegionServer中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache,方便下次访问；
3)	与目标RegionServer进行通询；
4)	分别在MemStore,StoreFile中查询目标数据，并将查询到的所有数据进行合并，此处的所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）;
5)	将查询到的新的数据块（Block,HFile数据存储单元，默认大小为64KB）缓存到BlockCache;
6)	将合并后的最终结果返回给客户端。
	预分区的作用和原理
	提前把分区规划好，创建出来，防止频繁的切分split，影响性能；事先创建不同的分区也可以实现数据均衡，防止数据的倾斜
	Hbase的region划分，每次划分的大小都一样吗？
	有可能不一样：因为Hbase要保证同一个rowkey的数据在同一个store中
	切分的时候要找到midkey进行切分，不能保证完全一半，因为切分原则一个rowkey的数据要放在一个region中
	Hbase的rowkey怎么创建比较好？列族怎么创建比较好？
	Rowkey看散列，唯一性，长度原则
	有的时候为了查询方便把一些列的值放在rowkey中
列如业务中通过名称查询
Rowkey	name	age
Zhangsan_1001	zhangsan	30
Lisi_1002	lisi	40
	描述hbase的scan和get功能的区别
	Get是单一查询
	Scan是通过STARTROW,STOPROW范围查询
	面试涉及到的点
1)	Rowkey的设计
2)	Hbase的读写原理：读流程，写流程
3)	优化
4)	二级索引的设计

```

##### 82.查看端口被占用解决方法

```
lsof -i:8081

ps -ef | grep 1760

kill -9 3024
```

##### 83.meaven仓库

```
mvnrespository
https://mvnrepository.com/
```

##### 84.flinkCDC

```
* CDC Change Data Capture变化数据捕获
 * canal,maxwell都是CDC的变化工具
 * 传统数据实现：下单=>将业务写入数据库，把数据写道esticsearch,写入缓存，这样一致性得不到保证
 * 优化=>下单=>将业务写入数据库=>（通过canal,maxwell采集）数据写到esticsearch,写入缓存
 * flinkcdc和maxwell,canal的作用是一样的，优点是把数据采集做了更加深层次的风封装，底层是Debezium
 *  1.未封装之前
 *      Debezium把业务数据库库数据采集过来发往Kafka，交给计算引擎计算最后把数据保存到hadoop或者elasticsearch
 *  2.flinkCDCfe封装之后
 *      CDC直接采集业务数据发往flink
 * 在官网是看不到，是社区的
 * 的底层是
```

```
http://www.dreamwu.com/post-1594.html
```

```
 CDC的种类change data Capture变化数据捕获
 CDC主要分为基于查询和基于Binlog两种方式，基于查询的话，需要发起一个select语句到数据库去查询，会对数据库造成压力
 Binlog开启会有1%的性能损耗
Canal、Maxwell、Debezium会伪装成从机，去binlog里边读取数据
                        基于查询的CDC                    基于Binlog的CDC
开源产品                 Sqoop、Kafka JDBC Source         Canal、Maxwell、Debezium
执行模式                 Batch                              Streaming
是否可以捕获所有数据变化    否                                   是
延迟性                   高延迟                               低延迟
是否增加数据库压力         是                                    否
```

##### 85.mapValues()和mapGroups()

```
mapValues()表示对每个元素做map操作
mapGroups()表示对整一组数据做map操作
```

##### 86.spark的默认缓存级别

```
分两种情况：
  (1)RDD:集成内存MEMORY
  (2)DF,DS：使用内存和磁盘MEMORY_AND_DISK，当内存不够的情况下，写道磁盘
如果使用RDD，即使使用内存，RDD有2G的数据，YARN的可用内存只有1G，这时做一个cache操作，RDD会OOM吗？不会
因为：当内存不够用的时候，尽可能的缓存数据，也是按照分区缓存，假如2G有4个分区，此时我们有1G的内存去做缓存，
可能缓存2个分区，此时的缓存是50%，还有50%怎么办？下边有个action算子，触发action算子，50%走血缘依赖，从头开始去拉取数据；
RDD的一个弊端，当内存不够用的情况下，他是去走血缘依赖的，
当采用dateFrame和dateset后，处理数据，如果数据不够用，它会溢写磁盘：MEMORY_AND_DISK
它的判断依据是：我先去内存中拿，再去磁盘里边拿，这两个地方都找不到再去找血缘依赖
对比：
一份日志文件1G
使用RDD去处理数据，做缓存，可能需要2G内存
拿DS,DF可能需要300M内存
开发中，能不使用RDD尽量不使用RDD，因为其性能比较低
```

##### 87.sql无法对缓存级别更改

```
从第一个算子会走血缘依赖，底下的几个算子不会走血缘依赖[宽依赖(shuffle)，宅依赖]，它们是从缓存里边拿数据的，
实际把很多的shuffle的stage阶段给规避掉了;
但是如果你只是写sql的话，实际上是很难做这件事情的，只有把业务逻辑转换为DS流的这种方式，才能很好的控制这种东西；
如果说在表里边，做缓存，对缓存级别做更改，没法改；
```

##### 88.性能调优

```
(1)RDD cache()
   useRddCache(sparkSession)
   result.cache()
   while (true) {
      //因为历史服务器上看不到，storage内存占用，所以这里加个死循环 不让sparkcontext立马结束
    }
  通过spark ui看到，rdd使用默认cache缓存级别，占用内存4.3GB,并且storage内存还不够，只缓存了75%
(2)kryo+序列化缓存
  .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    .registerKryoClasses(Array(classOf[CoursePay]))
     useRddKryo(sparkSession)
     result.persist(StorageLevel.MEMORY_ONLY_SER)
     打成jar包在yarn上运行，查看storage所占内存，内存占用减少了1445.8mb并且缓存了100%。使用序列化缓存配合kryo序列化，可以优化存储内存占用
(3)DataFrame、DataSet
  根据官网描述，DataSet类似RDD，但是并不使用JAVA序列化也不使用Kryo序列化，而是使用一种特有的编码器进行序列化对象。那么来使用DataSet进行缓存
  1.cache
    userDataSet(sparkSession)
    result.cache()
    提交任务，在yarn上查看spark ui，查看storage内存占用。内存使用612.3mb;
    并且DataSet的cache默认缓存级别与RDD不一样，是MEMORY_AND_DISK
  2.序列化缓存
    userDataSet(sparkSession)
    result.persist(StorageLevel.MEMORY_AND_DISK_SER)
    打成jar包，提交yarn。查看spark ui,storage占用内存646.2mb。和默认cache缓存级别差别不大。所以Dataframe可以直接使用cache。
综上：
  所以从性能上来讲，DataSet,DataFrame是大于RDD的建议开发中使用DataSet、DataFrame
```

##### 89.spark SQL广播join优化

```
(0)广播join的参数
    广播join禁用参数设置
      spark.sql.autoBroadcastJoinThreshold -1
    广播join默认值
    spark.sql.autoBroadcastJoinThreshold 10m
      http://spark.apache.org/docs/3.0.0/configuration.html
      spark.apache.org => 搜索configuration => 搜索broadcast
      或者使用API直接broadcast(小表)
  (1)分区：y'y'y'y'y'y'y'y'y'y'y'y'y'yy'y'y'y'y'y'y'y'y'y'y'y'y'y'y'y
    分区的个数是vcore的2-3倍，才能让资源充分利用起来；
    24个合理，25，可能造成其他虚拟核数在等最后一个虚拟核运行最后一个task,不合理
  (2)shuffle及广播join
    shuffle的过程本质就是一个网络传输，将相同的key放到同一个分区里边，这时就会产生一个shuffle，因为有网络数据传输，如果104端key的数据量非常的多，就会导致task的耗时非常的久，此时需要想办法把shuffle优化掉；
    spark SQL中有广播join的方式：先将 B表的数据查出来，加载到driver的内存里边去，再将B表广播分发；
    广播的过程是推过去的还是抓取的？
    广播
    抓取：不需要分流，每台机器抓取都是100M；101抓取带宽100M跟102没有关系
    推：需要分流，假如driver端带宽是100M，下边5台机器，那么分流每台20M
    广播的方式一定是抓取的，而不是推的

  (3)广播方式
    (1)HTTP广播：比较老的，都去driver端抓取
    (2)洪流广播：网状广播，executor端既可以去driver端，也可以去executor端抓取

  (4)sparkjoin策略
    (1)hash join 1.4之后被淘汰
    (2)broadcast hashjoin 小表join大表
    (3)sortMerge join:大表join大表。join之前会去排序

  (5)其他

    广播大表driver端OOM，如果driver端内存足够大，分发的时间久，可能操作正常join的时间
    广播变量核广播join
    databricks公司,掌握spark第一手资料的公司，会把资料发给apache

    不能使用广播大变量
    执行计划图，stage优化
    如果增加缓存cache(),就会看不到执行计划图；如果想要看到执行计划图，就需要注掉cache
```

##### 90.大表与大表进行join => SMB join 

```
SMB join?
分桶join
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization
打开参数
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
使用条件：（1）两表进行分桶，桶的个数必须相等
     （2）两边进行join时，join列==排序列==分桶列
注意：
  当使用分桶的时候，spark必须使用saveAsTable
  saveAsTable支持分桶,不需要你去建表；如果spark使用分桶，并且使用saveAsTable，在hive中不需要去建表，建的话会报错
  insert intoTable不支持分桶
spark读取原始数据的时候，分区是不能控制的，原因是spark去读取文件的分区下边是由切片控制的，128M，所以分区的个数是由数据量的大小/128M；
切片10%冗余，假如数据块129M，它1个切片[逻辑]，落到hadoop上是2个数据块[物理存储，一个块128M];
分桶后，它的分区也会有变化的，分区不再收参数set("spark.sql.shuffle.partitions", "36")控制，分区的个数是由桶的个数来控制的，有几个桶就会产生几个分区；
SMBjoin优化的地方？
  (1)优化2张表走sortMergeJoin时候，前边的排序时间的sort；
  (2)join几个T的表效果比较明显
  (3)SMB join涉及到分桶的，数据量都是比较大的，几个T级别的，GB级别的不会做这种优化的
缺点：
  (1)分桶会导致小文件过多，文件数量=分区的个数*桶的个数[桶的个数是12，分区的个数为下边的切片控制的，默认128M，分区的个数是数据量/块大小]

缺点小文件个数比较多，数据量不是很多，不会做分桶
优化join前的排序时间
```

##### 91.堆外内存的使用

```
--executor-memory 4g 不会占用堆外内存
```

```
堆内外内存的使用：
一个容器最多可以申请多大资源，是由yarn参yarn.scheduler.maximum-allocation-mb决定，这个参数指的是堆内+堆外内存的和
容器的内存上限不能超过yarn.scheduler.maximum-allocation-mb
如果超过报错，假如内存上线设置为7g
  IllegalArgumentException:Required executor memory(4096MB),offHeap memory(4096MB),overhead(409MB) is above the max threshold(7186MB) of this cluster !
spark的提交命令：--executor-memory 4g => 4g默认是堆内内存，不包含堆外；它去yarn申请的时候，会比4g大，因为它的堆外内存还有默认值；
这也就是为什么spark命令提交之后，再跳转到8088上边去，看内存的使用和命令不一样，原因其内存的申请还包含堆外；
```

##### 92.在spark中有哪些参数决定堆内存的大小呢？

```
  (1)spark.executor.memoryOverhead => 
    [spark堆外内存参数，内存额外开销，默认开启，默认值为spark.executor.memory*0.1并且会与最小值384mb做对比，取最大值。这就是为什么spark on yarn任务堆内内存填写申请1个g，而实际去yarn申请的内存不是1个g的原因。]
  (2)spark.executor.memory =>  spark提交任务时指定的堆内内存
  (3)spark.memory.offHeap.size => 堆外内存参数，spark中默认关闭，需要将spark.memory.enable.offheap.enable参数设置为true
  2.4版本和3.0版本有区别：
    2.4版本，spark.executor.memoryOverhead包含spark.memory.offHeap.size
    3.0版本，spark.executor.memoryOverhead与spark.memory.offHeap.size独立开

  使用：
  http://spark.apache.org/docs/3.0.0/configuration.html
  想使用offHeap.size，需将开关offHeap.enabled设置为true
  spark.memory.offHeap.enabled true，默认值是false
  spark.memory.offHeap.size 默认值是0

为什么要有两块堆外内存？
  (1)spark.memory.offHeap.size这块堆外内存 => 推荐存储使用的，因为存储级别还有一块堆外缓存
  (2)spark.executor.memoryOverhead,这块堆外缓存是用来字节开销使用的
    spark.executor.memoryOverhead 默认值：executorMemory * 0.10, with minimum of 384
代码实现堆外缓存
  (1)缓存级别调成堆外缓存：result.persist(StorageLevel.OFF_HEAP)
  (2)把堆外缓存的参数打开：--conf spark.memory.offHeap.enabled=true[命令行输入的命令]
  (3)分配内存： --conf spark.memory.offHeap.size=2g  --executor-memory 2g
另一种方式=> 在代码里边直接conf.set("spark.memory.offHeap.enabled",true).set("spark.memory.offHeap.siz",2g)
在yarn监控界面可以看到，申请堆内2g,--executor-memory 2g,但是页面中storage中堆内(on Heap Storage Memory)可用912m,因为由内存管理比值乘出来的，因为申请2g，但是2g不是都用于缓存的;例如2g*0.5出一块内存用于缓存，还有0.5是用于计算的
```

##### 93.使用堆外缓存的原因

```
  涉及到gc的问题
  一个容器去yarn中申请资源，容器里边的内存分为2份：堆内+堆外
  堆内 => 消耗jvm内存,受gc的限制[没用的对象会被垃圾或收掉]，gc会频繁访问堆内缓存，影响计算效率
  堆外 => 消耗系统内存，不受gc影响
  讲将计算和缓存剥离开，堆内主要用于计算，堆外主要用于缓存，这样计算就不会受到缓存的影响了
三个参数相加实际才等于容器到yarn上边申请的内存
即spark.executor.memoryOverhead+spark.executor.memory+spark.memory.offHeap.size的值必须小于等于yarn.scheduler.maximum-allocation-mb
```

##### 94.谓词下推

```
A表：一共10万条数据；选择id<1000[0-9999]
B表：一共100万条数据
A表有范围过滤条件，id<1000，那么A跟Bjoin时候，join的结果是1000条数据，那么说明B表里边实际有用的数据就只有1000条，有90多万条是没用的；
如果不开启谓词下推 => 就会扫描B表的99万条数据
开启谓词下推 => 先将A表的范围过滤条件将A表查出，那这1000条数据当作B表的过滤条件，B表过滤后也是1000条数据
最后拿B表的过滤后的数据跟A表过滤后的数据join；
综上：
  谓词下推，会将有范围过滤条件的表当成一个子查询，用子查询去过滤B表，B表数据量少了之后再跟A表进行join
```

##### 95.spark3.0改版,新特性

```
(1)ANSI SQL:spark2.4,int值达到上限+1，会变成负数
      spark3.0,int值达到上限+1，会报错，更加符合SQL
(2)spark streaming不再更新
Databricks（Spark的商业化公司）

```

```
AQE动态自适应查询[Adaptive Query Execution]：三个特性
(1)Dynamic coalescing shuffle partitions
  动态合并shuffle分区
  spark SQL默认的分区个数是200，有时可能太多，有时可能太少需要反复跑达到标准 
  开始把动态分区设置非常大1000或者10000，让spark自动的合并，当它认为表的数据量非常少的时候，它会缩减分区
  动态合并shuffle分区
  AQE可以在运行时将相邻的小分区合并成为更大分区
(2)Dynamic switching join strategies
  动态选择join策略
  动态选择join策略:sort-merge join,broadcast-hash join
  spark2.4:A表joinB表，A表过滤后就剩余1000条数据，变成小表，那么2.4会选择sort-merge join 策略；它的选择策略会根据初始值选择
  spark3.0:A表joinB表，A表过滤后就剩余1000条数据，变成小表，那么3.0会选择将sort-merge join 转换为broadcast-has join策略
(3)Dynamic optimzing skew joins
  动态优化数据倾斜，设置参数认为join产生数据倾斜，自动会做一个拆分，将倾斜的task做成小的task,再去join，解决数据倾斜
```

##### 96.使用spark3.0AQE

```
 (1)把AQE参数打开
    A.AQE参数开关
      http://spark.apache.org/docs/3.0.0/configuration.html#runtime-sql-configuration
      spark.sql.adaptive.enabled 默认false
      set("spark.sql.adaptive.enabled","true")
  (2)打开动态缩小分区
    A.缩小分区参数开关
      spark.sql.adaptive.coalescePartitions.enabled 默认是true
      set("spark.sql.adaptive.coalescePartitions.enabled","true")
    B.期望分区大小
      将多个小任务合并成一个分区后，期望这个分区多大，有参数spark.sql.adaptive.advisoryPartitionSizeInBytes，控制
      set("spark.sql.adaptive.advisoryPartitionSizeInBytes","10mb"),期望小任务合并后是10mb
    C.初始化分区个数
      初始化参数spark.sql.adaptive.coalescePartitions.initialPartitionNum，当spark任务开始之前，可以给它设置较大的分区个数，让它进行自动合并
      set("spark.sql.adaptive.coalescePartitions.initialPartitionNum","1000")spark任务初始化的时候是1000个分区
      它的默认值是200：By default it equals to spark.sql.shuffle.partitions
    C.1开启动态申请资源
      A.
      如果资源充足的情况下，开启spark的动态申请资源spark.dynamicAllocation.enabled
      .set("spark.dynamicAllocation.enabled","true")
      .set("spark.dynamicAllocation.shuffleTracking.enabled","true")
      开启动态分配资源，这两个参数都得开
      [参数开启的效果：spark –submit的提交参数，当开启动态分配后，分配的一些参数不受控制，如果yarn集群每天只能用到60%的资源，用不完，参数可以打开；如果YARN的任务用到90%，-100%，那么参数不要开，任务打开的效果，多个spark任务，保证每个spark任务都是最快的速度，比如跑一个离线任务，明明只要1G就够了，这时他认为1G需要半个小时，速度比较慢，强制性给你申请6G，这样导致每个任务都占最大的资源，最后一个任务YARN的资源不够了，导致部分任务卡死，没有资源]
      B.
      参数不受控的现象
      --num-executors 3 --executor-cores 4   --executor-memory 2g --queue spark
      提交命令使，申请的executor的个数是3个，但是动态申请资源开启后，实际申请的为6个executor，达到最优效果，同时开始同时结束，不产生数据倾斜情况
    AQE动态缩小分区和spark动态分配资源结合一起使用，前提yarn资源够用，跑的速度是最快的，不产生数据倾斜的情况；
  (3)动态选择join策略
    sort-merge join,broadcast-hash join
```

##### 97.Spark3.0~AQE实测

```
  (1)动态缩小分区
    查询数据，shuffle会对查询数据查产生非常昂贵的影响，SQL里边join，groupby等会产生shuffle，会产生网络传输，相同的key会放在同一个分区里边；
    分区太少 => 每个分区产生的数据量可能会很大，可能会造成OOM；轻的可能内存存不下，溢写到磁盘，降低了查询效率
    分区太多 => 导致资源不能被合理利用起来；例如shuffle默认分区是200，Vcore利用不起来，导致很多vcore是空转的；
          每个分区的数据量很小，有大量的小的数据块频繁的打开关闭IO，IO的效率可能非常低
    为了解决这个问题，任务开始的时候设置很多个shuffle分区的个数，让spark自动帮我们缩小
  (2)动态选择join策略
    sort-merge join,broadcast-hash join
    A.
    打开AQE开关
    .set("spark.sql.adaptive.enabled","true")
    B.
    打开spark SQL自动选择join策略的开关
    .set("spark.sql.adaptive.localShuffleReader.enabled","true")
    参数解释：
      Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.
    Custom ShuffleReader会尝试读取数据块，当数据块足够小的话，就会变成广播的转换，将表变成广播join；如果表数据量足够大，那么读写器Custom ShuffleReader不会触发广播
    默认值即为true
```

##### 98.解决小文件过多问题

```
spark默认的分区个数是200；spark.sql.shuffle.partitions参数控制；此参数只能控制Spark sql、DataFrame、DataSet分区个数。不能控制RDD分区个数
    相同key的数据会聚合到1个task里边去，但是1个task里边不一定有一个key;
    一个stage的耗时由最慢的task决定
    一个分区会产生一个文件
    解决小文件过多问题
    使用coalesce(20)算子，默认shuffle分区200，会产生200个文件，那么使用算子之后会产生20个文件
```

##### 99.合理利用CPU资源

```
  跑数仓1：1资源分批有些奢侈，跑实时，分区：vcore=1：2/3
  更改参数spark.sql.shuffle.partitions
  --num-executors 3 --executor-cores 4,去向yarn申请的executor vcore资源个数为12个（num-executors*executor-cores）
  如果不修改spark sql分区个数，那么就会像上图所展示存在cpu空转的情况。这个时候需要合理控制shuffle分区个数
  为了合理利用资源，一般会将分区（也就是task）设置成vcore的2倍到3倍。
  set("spark.sql.shuffle.partitions", "36")
```

##### 100.spark sql当中修改分区的方式

```
  (1)coalesce
  (2)repartition
  (3)spark.sql.shuffle.partitions
```

##### 101.DPP(Dynamic Partition Pruning)动态分区裁剪

```
  A.
  使用场景：
  打开，优化效果，日常任务还用不到，跑1年2年的任务能够使用的到
  B.
  核心思路：
  核心思路就是先将join一侧作为子查询计算出来，再将其所有分区用到join另一侧作为表过滤条件，从而实现对分区的动态修剪
  C.
  使用条件：
    将select t1.id,t2.pkey from t1 join t2 on t1.pkey =t2.pkey and t2.id<2 优化成了
  select t1.id,t2.pkey from t1 join t2 on t1.pkey=t2.pkey and t1.pkey in(select t2.pkey from t2 where t2.id<2)
  D.
  触发条件：
  （1）待裁剪的表join的时候，join条件里必须有分区字段
  （2）如果是需要修剪左表，那么join必须是inner join ,left semi join或right join,反之亦然。但如果是left out join,无论右边有没有这个分区，左边的值都存在，就不需要被裁剪
  （3）另一张表需要存在至少一个过滤条件，比如a join b on a.key=b.key and a.id<2
  E.
  开启参数：
    .set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
```

##### 102.sparkstreaming高吞吐量

```
  A.1
  实时小文件优化合并 => IOl流的追加
  A.2
  实时数据采集
  (1)Java代码
  (2)flume
  (3)binlog
  flumebinlog都会先落一份日志文件，如果数据错误，可以回数
  A.3
  Binlog组件
  1.Oracle:OGG(收费)
  2.Sql Server ：没有binlog的概念
  3.Mysql:Canal,Maxwell
  Canal没有断点续传，需要搭建HA;不能导入历史数据
  Maxwell有断点续传;可以导入历史数据
  A.4
  Debezium => 可以监控上边所有数据的binlog
  kafka connector,kafka sql,kafka stream
```

##### 103.file协议

```
  A.
  resources里边配置了集群的配置文件，ssc.textFile()默认去找hdfs的路径
  B.
  如果想要找本地的resources的话需要增加file协议
  ssc.textFile("file://")
```

##### 104.每个分区每秒钟能够拉取的上限

```
  A.
  .set("spark.streaming.kafka.maxRatePerPartition", "100")
  控制每个分区每秒钟能够拉取的上限；
  可以反复测试，调整这个参数，使处理时间在批时间范围波动
  B.
  设置参数防止spark，OOM
  如果不设置参数，假如Kafka中有1亿条数据有，那么spark把1亿条数据全部拉取过来,导致spark内存溢出
```

##### 105.消费者组

```
  A.
  Kafka下游可以对接Java，flink，spark,每一个消费程序都会维护一个偏移量，groupid唯一标识，
  确定偏移量比如有Java，flink，spark不同组
  B.
  实时任务不同的groupid是不能重复的
  C.
  自动提交偏移量false => 反复测试
  精准一次性消费 => 手动提交偏移量 控制先后顺序，先处理业务数据worker端，再去提交偏移量driver端，不在同一台机器
  调用collect算子，把拉回driver端，再利用事务处理；sql失败情况断电断网
  D.
  sparkstreaming
    (1)提交偏移量
    (2)处理业务数据
    两种为异步，提交偏移量设置为true；提交偏移量成功，处理业务数据失败，修复后造成数据丢失
    解决方案：
    (1)自动提交偏移量改为false，控制先后顺序，保证提交偏移量是处理业务数据之后的 => 靳准一次性消费
```

##### 106.精准一次性消费

```
  A.
  (1)设置手动提交偏移量
  (2)控制先后顺序，先处理业务数据，再提交偏移量
  B.
  问题？
  处理业务数据跟提交偏移量不是捆绑的，业务数据是worker端，提交偏移量是在driver端，不在同一个机器上边，无法保证事务；
  调用collect算子，把数据拉回driver端，driver端两部绑定在一个事务里边，再利用事务处理；保证完全精准一次消费；sql失败情况断电断网
  缺点？
  (1)影响效率，原来往数据库里边写是分布式的，现在压力都在driver端，速度可能会变慢
  (2)下游的数据库需要有事务的功能；如果下游使用redis,hbase没有事务的功能
  一般的大公司，不会丢数据的，但是可能回数据重复消费；
  提交偏移量，实际是写SQL操作数据库的步骤，这个步骤如SQL语句写对是不会失败的，除非断电，断网
```

##### 107.Sparkstreaming可以监控多个topic

```
 sparkstreaming可以监控多个topic
```

##### 108.topic调大分区

```
  A.
  checkpoint是给updatestatebykey算子存储数据用的
  updateSatebykey使用需要开启checkpoint,会造成小文件，一个分区一个文件(不同程序消费Kafka，每个程序有不同的分区，所以会造成小文件)；一般公司内部不会使用
  A.1
  一般sparkstreaming会跟Kafka分区保持一致，一般不会特意更改分区；
  缩减coalesce，并行度就会减小，速度回变慢
  增大repartition，会产生shuffle，shuffle会耗时，也会变慢
  A.2
  如果想要spark streaming跑快一点，那么就使topic分区调大一点
  B.
  消费策略一般采用均匀拉取
  LocationStrategies.PreferConsistent
```

##### 109.检查点写入MySQL

```
  updateSatebykey不能使用，checkpoint 会产生很多小文件
  kafka => sparkstreaming + Hdfs(checkpoint) => mysql
  公司内不建议使用，那么处理历史总和数据的场景?
  解决方法，读取MySQL数据库，做累加，再写回MySQL数据库
  kafka => sparkstreaming + mysql => mysql
  stream.foreachRDD{}在driver端执行，算子外
  RDD.foreach在executor端执行，算子内
```

##### 110.UI界面

```
  A.
  跑离线是显示SQL,实时是streaming
  A.1
  sparkstreaming第一批数据延迟会高一些
  A.2
  处理时间在3秒(批的时间)以内，数据不会挤压
  保证实现不会超过虚线，处理时间不会超过批时间，那么就不会造成数据积压
  A.3
  Active Batches，正常0或者1，如果>1说明数据积压
```

##### 111.背压

```
  解决spark streaming的积压问题
  3s拉取一次，不管处理时间，如果处理时间>3s那么批次积压起来
  开启背压，第一次拉取3000条，处理时间>3s，那么第二次拉取2700条，如果处理时间<3s，加大拉取条数，来回波动
  背压只能解决spark streaming积压，不能解决Kafka积压
  如果Kafka积压，增加一次拉取参数；增加Kafka分区，提高vcore
```

##### 112.连接未关闭=>内存泄露

```
  闭包问题编写不会报序列化错误
  共享变量，数据库连接不能广播
  Scala特性，闭包，可以引用当前数据库外的变量，不报错是因为引用当前作用域外的变量不报错，Scala的闭包
  rdd.foreachpartition()
  连接不关，会造成内存泄露，当内存够时，跑任务能够解决；但是跑一段时间后，spark streaming跑不动，这时会出现内存泄露
  内存泄露导致的原因：可能连接未关闭
  创建连接池，按照分区去走；每一个分区从连接池里边拿到一个连接，避免每一个循环一个RDD都创建一个连接
  iterator使用多次，需要转换为数组
  分区操作数据库，各个分区独立运行，有线程安全的问题
  解决：
    (1)加锁，但是把分布式框架变成同步式
    (2)在操作数据库之前按照key进行聚合，保证分区1里边全是101，分区2全是102，把并发改成并行
```

##### 113.executor的两个核心功能

```
    (1)将计算结果返回给driver 
    (2)给RDD提供内存式存储
```

##### 114.spark通用运行流程概述

```
    (1)提交任务后都会先启动driver程序
    (2)随后driver向集群管理器注册应用程序
    (3)之后集群的任务管理器根据此任务的配置文件分配executor并启动
    (4)Driver开始执行main函数，spark执行为懒执行，当执行到action算子的时候开始反向推算，根据宽依赖进行划分stage，随后每一个
        stage对应一个TaskSet,TaskSet中有很多的task,查找可用资源executor进行调度；
    (5)根据本地化原则，Task会被分发到指定的executor执行，执行过程中executor与driver不断的进行通信，报告任务进行情况
```

##### 115.spark部署模式

```
    (1)独立部署模式：Standalone => Master Worker
    (2)Hadoop Yarn模式 => ResourceManager NodeManager
    (3)Apache Mesos：强大的资源管理框架，允许不同的框架放在其上，包含yarn
    (4)K8S：容器式部署模式
    其他：本地模式,测试环境；学习使用
```

##### 116.Flume拦截器

```
package com.atguigu.flume;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.Iterator;
import java.util.List;

public class ETLInterceptor implements Interceptor {
    @Override
    public void initialize() {
        
    }

    //todo 如果source支持bach size那么就使用events批量处理
    // 如果source不支持bach size那么就使用event单个处理
    @Override
    public Event intercept(Event event) {
        byte[] body = event.getBody();
        String log  = new String(body, StandardCharsets.UTF_8);
        if(LogUtils.isJSONValidate(log)){
            return event;
        }else{
            return null;
        }
    }

    @Override
    public List<Event> intercept(List<Event> events) {
        Iterator<Event> iterator = events.iterator();
        while (iterator.hasNext()){
            Event event = iterator.next();
            if(intercept(event)==null){
                iterator.remove();
            }
        }
           return events;
    }

    @Override
    public void close() {

    }
    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new ETLInterceptor();
        }

        @Override
        public void configure(Context context) {

        }
    }
}

```

```
package com.atguigu.flume;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONObject;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.List;

public class TimeStampInterceptor implements Interceptor {
    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {
        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);
        JSONObject jsonObject = JSON.parseObject(log);
        String timeStamp="";
        if(jsonObject.containsKey("ts")){
            timeStamp=jsonObject.getString("ts");
        }else {
            timeStamp=System.currentTimeMillis()+"";
        }
        event.getHeaders().put("timestamp",timeStamp);

        return event;
    }

    @Override
    public List<Event> intercept(List<Event> events) {
        for (Event event : events) {
            intercept(event);
        }
        return events;
    }

    @Override
    public void close() {

    }
    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new TimeStampInterceptor();
        }

        @Override
        public void configure(Context context) {

        }
    }
}

```

```
package com.atguigu.flume;


import com.alibaba.fastjson.JSONException;
import org.mortbay.util.ajax.JSON;

public class LogUtils {
    public static boolean isJSONValidate(String log){
        try {
            JSON.parse(log);
            return true;
        }catch(JSONException e){
            return false;
        }
    }
}
```

##### 117.Hive自定义函数

```

```

##### 118.解析Json,自定义UDTF函数

```
// 将动作日志炸开
package com.atguigu.hive.udtf;


import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.json.JSONArray;
import java.util.ArrayList;

public class ExplodeJSONArray extends GenericUDTF {
    @Override
    public void close() throws HiveException {

    }
    //todo 输入：json数组字符串(1个参数，string)
    // 输出：一个动作的json对象字符串（1列，string类型）
    @Override
    public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
        //todo 1.参数合法性检查
        if (argOIs.length != 1){
            throw new UDFArgumentException("ExplodeJSONArray 只需要一个参数") ;
        }
        //todo 2.第一个参数必须为string
        if (!"string".equals(argOIs[0].getTypeName())){
            throw new UDFArgumentException("json_array_to_struct_array的第1个参数应为string类型");
        }
        //todo 3.定义返回值名称和类型
        ArrayList<String> fieldNames = new ArrayList<String>();
        ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
        fieldNames.add("items");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,
                fieldOIs);
    }

    @Override
    public void process(Object[] args) throws HiveException {
        //todo 1.获取传入的数据
        String jsonArray = args[0].toString();
        //todo 2.将string转换为json数组
        JSONArray actions = new JSONArray(jsonArray);
        //todo 3.循环一次取出数组的一个json
        for (int i = 0; i < actions.length(); i++) {
            String[] result = new String[1];
            result[0] = actions.getString(i);
            forward(result);
        }
    }
}



```

##### 119.拉链表

```
初始化用户表，在用户表末尾增加两列，一列为开始日期，一列为结束日期，结束日期是一个永远也到不了的日期；
第二天开始导入新增和变化的数据，在末尾加上开始日期，结束日期，用初始化拉链表left out join 用户新增和变化这张表，如果能够join上那么就改初始化拉链表的结束时间为当前日期-1
```

##### 120.Hive小文件解决方案

```
(1)在执行Map前合并小文件，减小Map数，CombineHiveInputFormat对小文件的合并功能；
(2)Merge开启，输出合并小文件；
	map-only
	hive.merge.mapfiles=true
	map-reduce
	hive.merge.mapredfiles=true
	当输出文件平均大小小于16M时，启动merge，默认256M；
(3)JVM重用
```

##### 121.打散大表扩容小表

```
打散大表，扩容小表
flatmap压平(每条小表数据扩容35倍for(i<36;i++),形成一个List集合，) => 压平后List集合变成每条加前缀的数据
小表扩容了，所以更耗时了
使用场景：
数据倾斜，能跑出来，不使用这种方案
如果数据倾斜，结果集跑出不来了，需要使用此方案；
```

##### 122.HDFS读流程

```
（1）客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。
（2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
（3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。
（4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。

```

##### 123.HDFS写流程

```
（1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。
（2）NameNode返回是否可以上传。
（3）客户端请求第一个 Block上传到哪几个DataNode服务器上。
（4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。
（5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。
（6）dn1、dn2、dn3逐级应答客户端。
（7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。
（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。

```

##### 124.spark内存管理

```
在Spark最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在Spark应用程序运行期间均为固定的
Spark1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域
```

```
1.
JVM内存管理
spark本身就是基于内存的操作，所以内存管理上有更好的管理策略
构建一个普通的对象User，放在伊甸园区，放满后会溢出；
   =>防止溢出，触发垃圾回收minorGC(轻量级回收)
   =>把不能回收放在幸存者0区，给它一个数字1，表示经历了一次垃圾回收;其余的对象全部回收掉
   =>当伊甸园区再次满的时候，触发minorGC,回收伊甸园区和幸存者0中能被回收的对象，放在幸存者1区；其余的回收掉，此时幸存者1区变	成了幸存者0区，幸存者0区变成了1区
   =>不断循环，幸存了几次就被标记成几；
   =>当幸存者的年龄被标记为15的时候，就会防置在老年区
   =>当老年区满了后，有内存溢出的风险，就会触发重量级的回收majorGC/fullGC
   =>重量级GC这几块内存都会回收：新生代(伊甸园区+幸存者0区+幸存者1区)，老年代
   =>什么时候会出现内存溢出？当重量级GC，也叫做fullGC,回收后发现内存不够用，此时会形成OOM内存溢出
2.
Spark内存管理策略
主要executor内存，因为driver主要做调度，消耗的内存没有那么大，内存出问题的可能性比较小;executor主要做计算，消耗的内存比较大
内存三大块(executor):
	(1)存储内存Storage
		存储广播变量
		缓存(cache,persist)数据
	(2)执行内存Execution
		执行shuffle会涉及内存操作，因为要排序，内存不够要溢写磁盘
	(3)存储运行时的元数据信息，rdd,task的信息这些new出来的对象都属于元数据信息，它都会放在other里边
内存管理(executor):
	(1)静态内存管理(早期)
		配置好参数后，内存不会发生变化
		执行内存少问题？因为执行过程在shuffle中执行，内存越少，就会频繁的溢写磁盘，导致大量的磁盘IO，性能就会收到影响
	(2)动态内存管理(3.0)
		storage内存和executor内存之间有条虚线，内存先各占50%，实际运行过程中动态调整
		[1]动态占用机制：
			如果存储内存和执行内存都满了，溢写磁盘；persist把存储级别改成memoryanddisk,executor在排序过程中也会溢写磁盘
			<1>如果storage存贮内存占用较多
			占用了executor的内存，当executor计算排序需要内存时，会把内存归还;
			归还策略:将移除多出那部分数据，如果数据可以放到文件中，那么就溢写磁盘;如果数据时memoryoly，但是有不要你了，那么数据就真正的去除了，这个地方就会导致数据丢失
			<2>如果执行内存占用较多，
				当存储需要内存空间时，执行内存不会归还借用部分的内存;因为如果此时将内存归还给存储内存，那么计算过程中，计算的数据被释放了，丢了，那么数据就不会准确；不像其他的数据，数据丢失可以重复；但是计算数据部分丢失，就会导致计算的其他数据没用，因为其他数据要跟它汇总最终结果，借用内存部分数据丢失，executor自己内存的数据也不能正确计算，还得重新计算，比较麻烦；
				需要存储内存自己释放，或者溢写磁盘
		[2]堆内内存和堆外内存
			所谓堆内内存就是JVM管理的内存
			所谓的堆外就是不受到JVM管理的内存
			注:JVM管理，不受到我们的控制，你想让它释放内存不一定会释放，想让它分配它不能马上分配
				想要有一块内存，想怎样做就怎样做，想分配就分配，想释放就释放，堆外(spark自己完成对内存的操作)
```



##### 125.hive行转列，列转行

```
CONCAT(string A/col, string B/col…)
CONCAT_WS(separator, str1, str2,...)
COLLECT_SET(col)函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段
```



```
LATERAL VIEW udtf(expression) tableAlias AS columnAlias
```

##### 126.Hbase列族设置1-2个的原因

```
每个region默认第一次256M切分，第二次达到10M切分；
默认1-2个列族；
如果一个列族达到10G就会切分，那么其余的列族也会切分，如果列族太多，可能其余的列族还没到达列族就会被切分
```



##### 127.Hbase中Region分列的条件

```
具体的切分策略为：
第一次split：1^3 * 256 = 256MB 
第二次10GB 
后面每次split的size都是10GB了。
```

##### 128.数据倾斜的表现

```
某一个分区的处理时间很长，其余的分区处理时间很短，一个stage中其余分区任务执行完后都会等待数据倾斜的分区
```

##### 129.统计指标

```

```

##### 130.Kafka中Isr队列

```
采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？
	Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。

```

```
Kafka的ISR副本同步队列
ISR（In-Sync Replicas），副本同步队列。ISR中包括Leader和Follower。如果Leader进程挂掉，会在ISR队列中选择一个服务作为新的Leader。有replica.lag.max.messages（延迟条数）和replica.lag.time.max.ms（延迟时间）两个参数决定一台服务是否可以加入ISR副本队列，在0.10版本移除了replica.lag.max.messages参数，防止服务频繁的进去队列。
任意一个维度超过阈值都会把Follower剔除出ISR，存入OSR（Outof-Sync Replicas）列表，新加入的Follower也会先存放在OSR中。

```

```
LEO（Log End Offset ）：指的是每个副本最大的offset；
HW(High Watermark)：指的是消费者能见到的最大的offset，ISR队列中最小的LEO。
follower故障
follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。
leader故障
leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。

```



##### 131.宽表字段

```

```

##### 132.hive参数配置

```
(1)MapJoin,如果不指定MapJoin的条件，那么Hive解析器将join操作转换为Common Join;即在reduce阶段完成join，容易发生数据倾斜。可以使用MapJoin把小表全部加载到内存在map端进行join,避免reduce处理；
(2)行列过滤，选择只需要的列，
(3)列式存储，
(4)采用分区技术
(5)合理设置Map数
	合理设置map和ruduce =》 map => max（0， min(块大小，long最大值)）  =》  128m  =>1 g 
			ruduce （128m => 1g ）
(6)合理设置Reduce数
	越多就会多个输出文件，如果生成了很多个小文件，那么这些小文件作为下一个任务的输入，则会出现小文件过多的		问题
(7)小文件解决方案
	<1>输入:CombineHiveInputFormat具有对小文件进行合并的功能
	<2>输出:
		Merge
		mapfiles map_only 任务结束时合并
		mapredfiles map_reduce 任务结束时合并小文件
		256M
		小于16M
	<3>JVM重用
(8)开启map端conbiner,不影响最终业务
(9)压缩
(10)采用tez或者spark引擎
```

##### 133.Kafka参数配置

```
(1)日志保留3天
(2)默认1个副本，保存2个
(3)网络通信延迟时间
(4)默认1G，生产环境尽量不要超过6G
```

##### 134.hadoop参数配置

```
(1)增大环形缓冲区，由100m扩大到200m
(2)增大环形缓冲区的溢写比例。由80扩大到90
(3)减小对溢写文件merge的次数
(4)不影响业务的前提下，采用Combiner提前合并，减小I/O
```

##### 135.压缩

```
(1)支持切片的有Bzip2,Lzop
(2)gzip压缩率大
(3)
```



##### 134.你见过的任务的调度都有哪些

```

```

##### 135.集群配置规模数据量大小

```

```

##### 136.宽表字段宽表大小

```
'用户id',
'登录次数',
'加入购物车次数',
'收藏次数',
'下单次数',
'订单参与活动次数',
'订单减免金额(活动)',
'订单用券次数',
'订单减免金额(优惠券)',
'订单单原始金额',
'订单总金额',
'支付次数',
'支付金额',
'退单次数',
'退单件数',
'退单金额',
'退款次数',
'退款件数',
'退款金额',
'优惠券领取次数',
'优惠券使用(下单)次数',
'优惠券使用(支付)次数',
'好评数',
'中评数',
'差评数',
'默认评价数',
```

##### 137.WordCount

```
 val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("map")
    val sc = new SparkContext(sparkConf)

    val rdd1: RDD[String] = sc.makeRDD(List("hadoop scala", "spark spark hello", "scala"), 2)
    
```

```
    rdd1.flatMap(_.split(" "))
      .map((_, 1))
      .reduceByKey(_ + _)
      .collect()
      .foreach(println)
```

##### 137.0Spark算子

```
Spark的transformation算子（不少于8个）（重点）
1）单Value
    （1）map
    （2）mapPartitions
    （3）mapPartitionsWithIndex
    （4）flatMap
    （5）glom
    （6）groupBy
    （7）filter
    （8）sample
    （9）distinct
    （10）coalesce
    （11）repartition
    （12）sortBy
    （13）pipe
2）双vlaue
	（1）intersection
	（2）union
	（3）subtract
	（4）zip
3）Key-Value
	（1）partitionBy
	（2）reduceByKey
	（3）groupByKey
	（4）aggregateByKey
	（5）foldByKey
	（6）combineByKey
	（7）sortByKey
	（8）mapValues
	（9）join
	（10）cogroup
1.11.10 Spark的action算子（不少于6个）（重点）
（1）reduce：
（2）collect:
（3）count
（4）first：
（5）take：
（6）takeOrdered
（7）aggregate：
（8）fold
（9）countByKey：
（10）save
（11）foreach：
1.11.11 map和mapPartitions区别
	1）map：每次处理一条数据
	2）mapPartitions：每次处理一个分区数据

```



##### 138.离线统计指标

```
(1)品牌复购率
group by品牌 sum(order count)>=1,2,3 sum(if( ,1,0))
```

```
(2)漏斗分析，转化率
首页人数=>商品详情=>下单=>加购

```

```
(3)新老用户客单价
```

```
(4)留存率
```

```
(5)各渠道跳出率
```

```
(6)新老用户客单价
```

```
(7)10天连续5天活跃用户

```

##### 139.数仓维度主题

```
(1)访客主题
(2)用户主题
(3)商品主题
(4)优惠券主题
(5)活动主题
(6)地区主题
```

##### 140.flink实时项目

###### (1)VisitorStatsApp

```
DWS层中VisitorStatsApp，从dwd_page_log、dwm_unique_visit、dwm_user_jump_detail
将流进行union进行合并，对数据进行分组，开窗，聚合计算，将聚合结果保存到clickhouse中
对以上三条流进行合并,按照渠道、地区、版本、新老用户维度进行分组
```

```
UniqueVisitApp从dwd_page_log读取数据，
//todo 定义状态变量
// 首次访问时间
ValueState<String> firstVisitState;
如果上一页的pageid不为空，说明是从app的其他页面跳转过来，不是首次进入的第一个页面，需要过滤掉
如果firstVisitState不为空，并且与ts时间相同，那么是同一天的数据，需要过滤掉
如果firstVisitState为空，说名visit是今天第一次访问，更新状态
```

```
VisitorStats
//统计开始时间
    private String stt;
    //统计结束时间
    private String edt;
    //维度：版本
    private String vc;
    //维度：渠道
    private String ch;
    //维度：地区
    private String ar;
    //维度：新老用户标识
    private String is_new;
    //度量：独立访客数
    private Long uv_ct=0L;
    //度量：页面访问数
    private Long pv_ct=0L;
    //度量： 进入次数
    private Long sv_ct=0L;
    //度量： 跳出次数
    private Long uj_ct=0L;
    //度量： 持续访问时间
    private Long dur_sum=0L;
    //统计时间
    private Long ts;
```

###### (2)ProductStatsApp

```
形成以商品为准的统计  曝光 点击  购物车  下单 支付  退单  评论数 宽表

```



```
ProductStats
/窗口起始时间
 //窗口结束时间
//sku编号
//sku名称
//sku单价
 //spu编号
//spu名称
//品牌编号
//品牌名称
//品类编号
//品类名称


 //曝光数
//点击数
//收藏数
//添加购物车数
//下单商品个数
//下单商品金额
//订单数
//支付金额
//支付订单数
//退款订单数
//评论订单数
 //好评订单数
 //用于统计订单数

    @Builder.Default
    @TransientSink
    Set paidOrderIdSet = new HashSet(); //用于统计支付订单数

    @Builder.Default
    @TransientSink
    Set refundOrderIdSet = new HashSet();//用于退款支付订单数

//统计时间戳
```

##### (3)ProvinceStatsSqlApp

```
从dwm_order_wide读取数据

```



##### 141.字符串怎样转换成时间

```

```

##### 142.水位线的传递

```
flink认为时间戳小于水位线的都已经到达，有些窗口可以闭合，可以做聚合计算
水位线是一个逻辑时钟
水位线由程序员插入到数据流里边
在事件的事件里，水位线就是时间
水位线=观察的最大时间戳-延迟时间
对于窗口来说是左闭，右开区间
水位线超过窗口结束时间，窗口闭合，默认情况下，迟到元素会被丢弃
flink会在最开始插入时间戳为无穷小的时间戳
在流的最末尾插入时间戳为无穷大的时间戳，有可能处理的是离线数据，无穷大的水位线才可以将窗口触发
```

```
当输入(a,1)的时候，调用processElement,注册一个定时器，在11秒的时候，打印当前的水位线，负无穷大
过了200毫秒以后，插入新的水位线，1s-1ms等于999毫秒
由于注册的定时器是在11秒以后，想要触发它，水位线必须超过11s，
输入(a,12),注册22秒的定时器，打印当前的水位线999ms,此时时钟已经推进到12000-1=11999ms,超过11s,定时器出发了
ctx.timerService().registerEventTimeTimer(value.f1+10*1000L);
```

```
    public static class MyKeyed extends KeyedProcessFunction<String,Tuple2<String,Long>,String> {

        @Override
        public void processElement(Tuple2<String, Long> value, Context ctx, Collector<String> out) throws Exception {
            out.collect("数据来了！");
            //todo 注册定时器
            ctx.timerService().registerEventTimeTimer(value.f1+10*1000L);
            System.out.println("当前的水位线是"+ctx.timerService().currentWatermark());
        }

        @Override
        public void onTimer(long timestamp, OnTimerContext ctx, Collector<String> out) throws Exception {
            super.onTimer(timestamp, ctx, out);
            out.collect("定时器触发了！");
        }
    }
```

###### 水位线的传递

```
keyBy以后做了分流，或者分组，keyBy以后不同的key对应的分流可能在不同的机器上，相同的key所对应的支流一定在同一个任务槽里，不同的key对应的支流不一定在同一个任务槽里；有可能被分到不同任务槽里；当水位线从1条流keyBy不同的支流后，怎样向下游传递的，当union或者connect的时候水位线应该如何传播呢
四条流合成一条流的时候，水位线如何传播，union算子会维护一个数组，数组里边有4个元素，每个元素保留一条流的水位线，空位的初始值是负无穷大，多条流合并的时候发送最小的水位线；分流的时候将水位线复制，然后广播下去
多条流合并的时候，向下游传递最小的水位线，分流的时候广播，将水位线复制并且分发到不同的支流里或者不同的任务槽里
应该在单条流里边分发水位线；
假如source算子，数据源算子并行度设置为2，读取数据流的时候，随机的向source算子分发的，本来是一条流过来的，由于source算子两个并行度读取这条流，那么实际是随机的读取，那么在两个source算子里边插入水位线，顺序就会变乱
建议:在插入水位线之前，保证并行度是1

```

##### 143.HDFS读写流程

```

```

##### 144.RDD五大属性

```
(1)标记分区
(2)计算逻辑compute
(3)血缘关系DepenDency
(4)分区器Partitioner
(5)一个列表，存储每个Partition的优先位置，移动数据不如移动计算
```

##### 145.flinkjoin

```

```

##### 146.clickHouse怎样使用

```
=======================================
clickhouse数据导入时发生数据错误怎么办?
由于操作比较“重”，所以 Mutation语句分两步执行，同步执行的部分其实只是进行新增数据新增分区和并把旧分区打上逻辑上的失效标记。直到触发分区合并的时候，才会删除旧数据释放磁盘空间，一般不会开放这样的功能给用户，由管理员完成。
=======================================
OLAP
OLTP（On-Line Transaction Processing，联机事务处理
OLAP（On-Line Analytical Processing，联机分析处理
ES:倒排索引
KYRIN
PRESTO
CLINKHOUSE
HBASE:是基于列族

列式存储优点：
对列的聚合，计数求和优于行式存储
压缩高,由于数据压缩比更好，一方面节省了磁盘空间，另一方面对于cache也有了更大的发挥空间

提前向clink house写的时候准备好宽表
因为单表查询快，关联查询join时慢
=================================
char(100) hello 实际占的字符是100个
varchar(100) hello实际占的字符是5个
char的性能高点，但是耗费空间；典型的用空间更换时间
================================
MySQL引擎：InnoDB
clinkHoise引擎：
==============================
汇合不是连接，是合并
reduce两两进行计算
springnoot日志采集服务器落盘功能logback
==============================
clinkhouse
高吞吐
sudo systemctl start clinkhouse-server
clinkhouse-client -m
=========================
clinkhouse表的引擎:
MergeTree => 支持索引支持分区 ，相当于MySQL的innodb
MySQL
	主键的作用： 索引，唯一约束，标识数据库表中的唯一元素，密集索引
	orderby:排序
但是在clink house中
	主键的作用是：不具备唯一约束，提供数据的一级索引，给当前主键建立索引，稀疏索引提升数据查询效率
	orderby
===============================
参数解析：
partitionby:
降低全表扫描，优化查询效率
分区目录 => 以列文件、索引文件、表定义
并行处理：
以分区为单位并行处理，一个分区一个核
分区合并：optimize table xxxx final;
===============================
primaryKey：
MySQL提升查询效率的原理：在字段上边加索引，把当前字段所有的值排好序a、b、c、d，再查询时直接到d区对应位置去找;稠密索引；
clinkhouse索引：在建表时指定字段时主键，给主键加索引，不是稠密索引，是稀疏索引，1-50建索引1，50-100建一个索引50，
100-150建100索引，查询55在对应范围查询，密集索引检索效率更高，密集索引耗费空间
50为索引粒度；
clinkhouse默认粒度8192
==========================
orderby唯一一个必填项
要求：主键必须是order by字段的前缀字段。
比如order by 字段是 (id,sku_id)  那么主键必须是id 或者(id,sku_id)
==========================
二级索引：
========================
列级别：
clinkhouse可以设置失效时间，
手动提交命令：optimize table xxxx final;
=========================
表级别：
设置：alter table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND;
=========================
ReplacingMergeTree
一个去重功能：
去重时机：只能在合并的过程中出现；
去重范围：只会在分区内部去重，不会跨分区；只是不定期清除空间，做一些优化
去重是根据orderby去重，跟主键没关系；实际根据orderby做唯一键
保留：
engine =ReplacingMergeTree(create_time)
找版本最大的create_time，版本相同，找最后一个插入的
==========================
引擎SummingMergeTree:
'预聚合'的引擎
手动合并：OPTIMIZE TABLE t_order_smt FINAL;
结论：
	1.汇总列engine =SummingMergeTree(total_amount)
	2.统计多个指标，多个字段必须是数字列，orderby(维度列)，如果没有指定，那么汇总所有除了维度列的
		其他列
	3.除了维度列，汇总列，其他列保留第一条
	4.不在一个分区的数据不会被聚合
=============================
问题：
能不能直接执行以下SQL得到汇总值
select total_amount from  XXX where province_name=’’ and create_date=’xxx’
不行，可能会包含一些还没来得及聚合的临时明细
如果要是获取汇总值，还是需要使用sum进行聚合，这样效率会有一定的提高，
但本身ClickHouse是列式存储的，效率提升有限，不会特别明显。
select sum(total_amount) from province_name=’’ and create_date=’xxx’
===============================
修改删除，不支持事务，比较重
alter table t_order_smt delete where sku_id ='sku_001';
alter table t_order_smt update total_amount=toDecimal32(2000.00,2) where id =102;
	“重”的原因主要是每次修改或者删除都会导致放弃目标数据的原有分区，
	重建新分区。所以尽量做批量的变更，不要进行频繁小数据的操作。
===============================
GROUP BY 操作增加了 with rollup\with cube\with total 用来计算小计和总计。
with rollup：从右至左去掉维度进行小计
with cube : 从右至左去掉维度进行小计，再从左至右去掉维度进行小计
with totals: 只计算合计
================================
csv格式：文本可以用excel打开的文本
JSON  > .json
================================
clink house副本
不具备服务的高可用，只具备数据的高可用
===============================
副本写入流程：
zookeeper通知，副本下载数据
==========================
MySQL通过myCat做MySQL的集群的负载均衡
clink house使用Distribute表引擎做负载均衡
negix、Distribute、myCat不处理数据，只做转发
============================
一般clink house中的数据是轻度聚合，容量够用
分片，查询需要从三台机器查询，效率较低
副本=> 集群的高可用
集群=> 分片，提高存储容量
副本同步数据，但是建表语句得单独创建
分片集群同步数据，也会同步建表语句
=============================
jdbc占位：？
========================
连接4要素
驱动
url
用户名
密码 
======================
*dwd_page_log
	pv_ct=1L
	during_time=xx
	sv_ct=1L
*dwm_unique_visit
	uv_ct=1L
*dwm_user_jump_detail
	uj_ct=1L
-对流进行合并
	union
-指定wantermark以及提取事件时间
-按照维度进行分组
	Tuple4<渠道、版本、地区、新老访客>
-开窗
	window(TumblingEventTime...)
-对窗口中元素进行聚合
	reduce(
		窗口中元素进行增量聚合计算,
		窗口处理函数-指定计算时间
	)
-将数据写到Clickhouse中
	ds.addSink(
		JdbcSink.sink(
			(流中数据都写在一张表中)
			sql,
			(preparestatement,T obj)->{
			反射获取字段值
				通过ps给？赋值
			}，
			指定执行参数(批次，执行次数)，
			指定连接参数(driver,url,user,password)

		)
	)
注释：
	statement
	preparestatement:把参数处理，不会被攻击？预先处理，提高效率
	Driver可能不需要填写，因为有些数据库会根据url推断使用的是哪种数据库
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

##### 147.UDTF函数

```
package com.atguigu.hive.udtf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.json.JSONArray;

import java.util.ArrayList;
import java.util.List;

public class ExplodeJSONArray extends GenericUDTF {

    @Override
    public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {

        // 1 参数合法性检查
        if (argOIs.length != 1) {
            throw new UDFArgumentException("ExplodeJSONArray 只需要一个参数");
        }

        // 2 第一个参数必须为string
        if (!"string".equals(argOIs[0].getTypeName())) {
            throw new UDFArgumentException("json_array_to_struct_array的第1个参数应为string类型");
        }

        // 3 定义返回值名称和类型
        List<String> fieldNames = new ArrayList<String>();
        List<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();

        fieldNames.add("items");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
    }

    public void process(Object[] objects) throws HiveException {

        // 1 获取传入的数据
        String jsonArray = objects[0].toString();

        // 2 将string转换为json数组
        JSONArray actions = new JSONArray(jsonArray);

        // 3 循环一次，取出数组中的一个json，并写出
        for (int i = 0; i < actions.length(); i++) {

            String[] result = new String[1];
            result[0] = actions.getString(i);
            forward(result);
        }
    }

    public void close() throws HiveException {

    }

}

```

```
hadoop fs -mkdir -p /user/hive/jars
hadoop fs -put hivefunction-1.0-SNAPSHOT.jar /user/hive/jars
create function explode_json_array as 'com.atguigu.hive.udtf.ExplodeJSONArray' using jar 'hdfs://hadoop102:8020/user/hive/jars/hivefunction-1.0-SNAPSHOT.jar';
```

##### 148.get_json_object

```
select get_json_object('[{"name":"大郎","sex":"男","age":"25"},{"name":"西门庆","sex":"男","age":"47"}]','$[0]');
结果是：{"name":"大郎","sex":"男","age":"25"}
```

```
SELECT get_json_object('[{"name":"大郎","sex":"男","age":"25"},{"name":"西门庆","sex":"男","age":"47"}]',"$[0].age");
结果是：25
```

##### 149.Partition的作用

```
Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；
```

##### 150.时间戳转换

```
date_format('2020-06-14','yyyy-MM')
date_format(to_utc_timestamp(from_unixtime(sum(duration)/count(visit_count)),'UTC+8'),'HH:mm:ss')
```

##### 151.maxwell和canal区别

```
Maxwell
canal不能对历史数据进行处理
伪装MySQL从机slave，从master复制数据
binlog二进制文件，记录MySQL的操作文件
监控多个在写多列
sudo ls -l
Maxwell
从MySQL中把数据采集到Kafka中
Maxwell有一个bootstrap功能，同步历史数据
Maxwell比canal更加轻量级
canal会带表结构，Maxwell更加简洁；
如果需要带表结构的数据进行分析，那么需要使用canal
maxwell可以支持断点续传，canal不支持
```

```
(1)
binlog_format=statement 语句 执行的sql 
有可能会造成数据的不一致,rand(),now() 
(2)
binlog_format=row   行变化  保证一致性
大量的冗余,一条sql语句能查出来，1-1000，但是得把1000行数据都复制出来
(3)
binlog_format=MIXED 混合
一般情况使用statement 
特殊情况使用row 
```

```
使用Maxwell监控MySQL数据变化，并将其发送到ods层：Kafka主题ods_base_db_m
 *      (1)分配Mysql账号可以监控操作该数据库
 *          GRANT ALL   ON maxwell.* TO 'maxwell'@'%' IDENTIFIED BY '123456';
 *          分配这个账号，可以监控其他数据库的权限
 *          GRANT  SELECT ,REPLICATION SLAVE , REPLICATION CLIENT  ON *.* TO maxwell@'%';
 *      (2)修改Maxwell配置文件config.properties
 *      producer=kafka
 *      kafka.bootstrap.servers=hadoop202:9092,hadoop203:9092,hadoop204:9092
 *      #需要添加
 *      kafka_topic=ods_base_db_m
 *      # mysql login info
 *      host=hadoop202
 *      user=maxwell
 *      password=123456
 *      #需要添加 后续初始化会用
 *      client_id=maxwell_1
```

##### 152.关系建模和维度建模有什么区别

```
(1)
关系建模：范式理论是给关系模型使用的
维度建模：维度和事实
(2)
事实表里边的信息：维度外键+度量值
维度表里边存储的时描述信息
维度模型的优点：
1.结构简单，join少，大数据情况下shuffle少，查询性能就会好
2.维度模型的设计是面向业务的，更容易理解
3.维度模型是以数据分析为出发点，维度模型更容易做数据分析，尤其是多维分析
例如：对分区进行分组，对度量值进行聚合 => 每个地区的订单金额
	 对品类名称进行分组，对salesAmount进行聚合 => 得到的不同品类的订单总额

	 找到事实表，找到对应的维度，对其join，对维度进行分组，对事实表里边的度量值进行聚合
维度模型更适合做数据分析
维度表是对事实的描述信息，事实表是对应不同的业务
```

##### 153.9种窗口

```
- TumblingEventTimeWindows,
- TumblingProcessingTimeWindows
- SlidingEventTimeWindows
- SlidingProcessingTimeWindows
- EventTimeSessionWindows
- ProcessingTimeSessionWindows
- DynamicEventTimeSessionWindows
- DynamicProcessingTimeSessionWindows
- GlobalWindows
```

##### 154.flume怎样把数据平均分到Kafka的不同的分区

```
Flume的官方文档是这么说的：

Kafka Sink uses the topic and key properties from the FlumeEvent headers to send events to Kafka. If topic exists in the headers, the event will be sent to that specific topic, overriding the topic configured for the Sink. If key exists in the headers, the key will used by Kafka to partition the data between the topic partitions. Events with same key will be sent to the same partition. If the key is null, events will be sent to random partitions.

其实以上文档中说的很清楚了，kafka-sink是从header里的key参数来确定将数据发到kafka的哪个分区中。如果为null，那么就会随机发布至分区中。但我测试的结果是flume发布的数据会发布到一个分区中的。
所以，我们需要向header中写上随机的key，然后数据才会真正的向kafka分区进行随机发布。
我们的办法是，向flume添加拦截器，官方文档说有一个UUID Interceptor，会为每个event的head添加一个随机唯一的key。其实我们直接用这个即可。

在flume添加的配置文件如下：
hiveview.sources.tailSource.interceptors = i2
hiveview.sources.tailSource.interceptors.i2.type=org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
hiveview.sources.tailSource.interceptors.i2.headerName=key
hiveview.sources.tailSource.interceptors.i2.preserveExisting=false
```

##### 155.单例设计模式

```
饿汉式单例
Class Single2{
	public static final Single2 INSTANCE = new Single2();
	private Single2(){//构造器私有化，不能直接new对象
} 
懒汉式单例
class Single1{
	private Single1(){//构造器私有化
}
Private static class Inner{
	private static final Single1 INSTTANCE = new Single1(); 
} 
Public static Single1 getInstance(){
	Return Inner.INSTANCE;
}

```

##### 156.kafka

```
(1)
Kafka有内部的topic吗？如果有是什么？有什么所用？
    __consumer_offsets,保存消费者offset
(2)
简述Kafka的日志目录结构？
    每个分区对应一个文件夹，文件夹的命名为topic-0，topic-1，内部为.log和.index文件
(3)
一个topic分为多个partition，一个partition分为多个segment，一个segment对应两个文件.log和.index
(4)
当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？
    1）会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first
    2）触发Controller的监听程序
    3）kafka Controller 负责topic的创建工作，并更新metadata cache
(5)
那些情景会造成消息漏消费？
    先提交offset，后消费，有可能造成数据的重复
(6)
有哪些情形会造成重复消费?
	先消费数据，再提交偏移量
(7)
消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？
    offset+1
(8)
“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？
    正确
(9)
聊一聊Kafka Controller的作用？
    负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。
(10)
失效副本是指什么？有那些应对措施？
    不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入
(11)
Kafka的那些设计让它有如此高的性能？
    分区，顺序写磁盘，0-copy

```

##### 157.为什么把维度数据存到hbase不存到MySQL

```
(1)
维度k,v类型，根据key去查维度，需要从hbase，redis选；但是需要维度常驻，选择hbase
(2)
click house和MySQL，列式存储，分析列式存储更加合适一点
(3)
指标算错可以删除，也可以使用去重引擎做替换;
```

##### 158.数仓流程

###### 1.sqoop  

```
#! /bin/bash

//订单表=>新增及变化
import_order_info(){
  import_data order_info "select      
                        from order_info
                        where (date_format(create_time,'%Y-%m-%d')='$do_date' 
                        or date_format(operate_time,'%Y-%m-%d')='$do_date')"
}

//优惠券使用表=>新增及变化
import_coupon_use(){
  import_data coupon_use "select
                        from coupon_use
                        where (date_format(get_time,'%Y-%m-%d')='$do_date'
                        or date_format(using_time,'%Y-%m-%d')='$do_date'
                        or date_format(used_time,'%Y-%m-%d')='$do_date'
                        or date_format(expire_time,'%Y-%m-%d')='$do_date')"
}

//订单状态表=>增量
import_order_status_log(){
  import_data order_status_log "select
                                from order_status_log
                                where date_format(operate_time,'%Y-%m-%d')='$do_date'"
}

//用户表=>新增及变化
import_user_info(){
  import_data "user_info" "select 
                          from user_info 
                          where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                          or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
}

//订单详情表=>增量
import_order_detail(){
  import_data order_detail "select 
                            from order_detail 
                            where DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date'"
}

//支付表=>新增及变化
import_payment_info(){
  import_data "payment_info"  "select 
                              from payment_info 
                              where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                              or DATE_FORMAT(callback_time,'%Y-%m-%d')='$do_date')"
}

//商品评论表=>增量
import_comment_info(){
  import_data comment_info "select
                            from comment_info
                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

//退款表=>增量
import_order_refund_info(){
  import_data order_refund_info "select
                              from order_refund_info
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

//sku商品表=>全量
import_sku_info(){
  import_data sku_info "select 
                        from sku_info where 1=1"
}

//商品一级分类=>全量
import_base_category1(){
  import_data "base_category1" "select 
                                from base_category1 where 1=1"
}

//商品二级分类=>全量
import_base_category2(){
  import_data "base_category2" "select
                                from base_category2 where 1=1"
}

//商品三级分类=>全量
import_base_category3(){
  import_data "base_category3" "select
                                from base_category3 where 1=1"
}

//省份表=>特殊
import_base_province(){
  import_data base_province "select
                            from base_province
                            where 1=1"
}

//地区表=>特殊
import_base_region(){
  import_data base_region "select
                            from base_region
                            where 1=1"
}

//品牌表=>特殊
import_base_trademark(){
  import_data base_trademark "select
                              from base_trademark
                              where 1=1"
}

//spu商品表=>全量
import_spu_info(){
  import_data spu_info "select
                          from spu_info
                          where 1=1"
}

//收藏=>全量
import_favor_info(){
  import_data favor_info "select
                        where 1=1"
}

//加购=>全量
import_cart_info(){
  import_data cart_info "select
                      from cart_info
                      where 1=1"
}

//优惠券
import_coupon_info(){
  import_data coupon_info "select
                        from coupon_info
                        where 1=1"
}

//活动表=>全量
import_activity_info(){
  import_data activity_info "select
                            from activity_info
                            where 1=1"
}

//活动规则表=>全量
import_activity_rule(){
    import_data activity_rule "select
                                from activity_rule
                                where 1=1"
}


//编码字典表=>全量
import_base_dic(){
    import_data base_dic "select
                          from base_dic
                          where 1=1"
}

//订单明细优惠卷关联表=>增量
import_order_detail_coupon(){
    import_data order_detail_coupon "select
    from order_detail_coupon
    where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

//退款表=>新增及变化
import_refund_payment(){
    import_data refund_payment "select
    from refund_payment
    where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
    or DATE_FORMAT(callback_time,'%Y-%m-%d')='$do_date')"                                 
}

//sku平台属性表=>全量
import_sku_attr_value(){
    import_data sku_attr_value "select
                                                from sku_attr_value
                                                where 1=1"
}

//sku销售属性表=>全量表
import_sku_sale_attr_value(){
    import_data sku_sale_attr_value "select
	from sku_sale_attr_value
	where 1=1"
}


```

###### 2.ODS

```
(1)
日志一张表，一天一个分区
------建表----------
drop table if exists ods_log;
CREATE EXTERNAL TABLE ods_log (`line` string)
PARTITIONED BY (`dt` string) -- 按照时间创建分区
STORED AS -- 指定存储方式，读数据采用LzoTextInputFormat；
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/warehouse/gmall/ods/ods_log'  -- 指定数据在hdfs上的存储位置
;
------数据装载----------
#!/bin/bash

# 定义变量方便修改
APP=gmall

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
   do_date=$1
else 
   do_date=`date -d "-1 day" +%F`
fi 

echo ================== 日志日期为 $do_date ==================
sql="
load data inpath '/origin_data/$APP/log/topic_log/$do_date' into table ${APP}.ods_log partition(dt='$do_date');
"

spark-sql -e "$sql"

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/$APP/ods/ods_log/dt=$do_date
(2)
业务数据，建27张表(维度表[用户维度，省份，地区，品牌，品类，字典]，事实表[下单，支付，退款，加购，收藏，评论])
==========================首日数据装载==========================
每张表一天一个分区
ods_order_info=" 
load data inpath '/origin_data/$APP/db/order_info/$do_date' OVERWRITE into table ${APP}.ods_order_info partition(dt='$do_date');"
ods_order_detail="
load data inpath '/origin_data/$APP/db/order_detail/$do_date' OVERWRITE into table ${APP}.ods_order_detail partition(dt='$do_date');"

==========================每日数据装载==========================
ods_order_info=" 
load data inpath '/origin_data/$APP/db/order_info/$do_date' OVERWRITE into table ${APP}.ods_order_info partition(dt='$do_date');"
ods_order_detail="
load data inpath '/origin_data/$APP/db/order_detail/$do_date' OVERWRITE into table ${APP}.ods_order_detail partition(dt='$do_date');"
```

###### 3.DWD

```
(1)
==========================维度数据==========================
商品维度表（全量）
insert overwrite table ${APP}.dim_sku_info partition(dt='$do_date')
where dt='$do_date'
insert overwrite table dim_sku_info partition(dt='2020-06-15')
where dt='2020-06-15'
优惠券维度表（全量）
dim_coupon_info="
insert overwrite table ${APP}.dim_coupon_info partition(dt='$do_date')
from ${APP}.ods_coupon_info
where dt='$do_date';

活动维度表（全量）

地区维度表（特殊）
insert overwrite table dim_base_province，只导入一次
时间维度表（特殊）

用户维度表（拉链表）
insert overwrite table dim_user_info partition(dt='9999-99-99')
select
    id,
    login_name,
    nick_name,
    md5(name),
    md5(phone_num),
    md5(email),
    user_level,
    birthday,
    gender,
    create_time,
    operate_time,
    '2020-06-14',
    '9999-99-99'
from ods_user_info
where dt='2020-06-14';
	每日
insert overwrite table dim_user_info partition(dt)
select
    nvl(ods_ui.id,dim_ui.id),
    nvl(ods_ui.login_name,dim_ui.login_name),
    nvl(ods_ui.nick_name,dim_ui.nick_name),
    nvl(ods_ui.name,dim_ui.name),
    nvl(ods_ui.phone_num,dim_ui.phone_num),
    nvl(ods_ui.email,dim_ui.email),
    nvl(ods_ui.user_level,dim_ui.user_level),
    nvl(ods_ui.birthday,dim_ui.birthday),
    nvl(ods_ui.gender,dim_ui.gender),
    nvl(ods_ui.create_time,dim_ui.create_time),
    nvl(ods_ui.operate_time,dim_ui.operate_time),
    nvl(ods_ui.start_date,dim_ui.start_date),
    nvl(ods_ui.end_date,dim_ui.end_date),
    nvl(ods_ui.end_date,dim_ui.end_date)最后一个字段分区字段
(
    select
from dim_user_info
where dt='9999-99-99'
)dim_ui
full outer join
(
    select
from ods_user_info
where dt='2020-06-15'
)ods_ui
on dim_ui.id=ods_ui.id

union all

select
	把9999-99-99分区失效的数据放到今天的分区
    date_add('2020-06-15',-1)
from
(
    select
    end_date
from dim_user_info
where dt='9999-99-99'
)dim_ui
join
(
    select
    '9999-99-99' end_date
from ods_user_info
where dt='2020-06-15'
)ods_ui
on dim_ui.id=ods_ui.id;
(2)
==========================日志数据==========================
启动日志表
insert overwrite table dwd_start_log partition(dt='2020-06-14')
select
    get_json_object(line,'$.common.ar'),
    get_json_object(line,'$.start.entry'),
    get_json_object(line,'$.ts')
from ods_log
where dt='2020-06-14'
and get_json_object(line,'$.start') is not null;
页面日志表
insert overwrite table dwd_page_log partition(dt='2020-06-14')
select
    get_json_object(line,'$.common.ar'),
    get_json_object(line,'$.page.during_time'),
    get_json_object(line,'$.ts')
from ods_log
where dt='2020-06-14'
and get_json_object(line,'$.page') is not null; 
动作日志表
public class ExplodeJSONArray extends GenericUDTF {
	initialize(){
        // 1 参数合法性检查
        	if (argOIs.length != 1)
        // 2 第一个参数必须为string
        	if (!"string".equals(argOIs[0].getTypeName()))
        // 3 定义返回值名称和类型
    process(){
        // 1 获取传入的数据
        String jsonArray = objects[0].toString();
        // 2 将string转换为json数组
        JSONArray actions = new JSONArray(jsonArray);
        // 3 循环一次，取出数组中的一个json，并写出
        for (int i = 0; i < actions.length(); i++) {
            String[] result = new String[1];
            result[0] = actions.getString(i);
            forward(result);
        }
    close()
    }
	创建函数
	create function explode_json_array as 'com.atguigu.hive.udtf.ExplodeJSONArray' using jar 'hdfs://hadoop102:8020/user/hive/jars/hivefunction-1.0-SNAPSHOT.jar';
}

数据装载
insert overwrite table dwd_action_log partition(dt='2020-06-14')
select
    get_json_object(line,'$.common.ar'),
    get_json_object(line,'$.page.during_time'),
    get_json_object(action,'$.action_id'),
    get_json_object(action,'$.ts')
from ods_log lateral view explode_json_array(get_json_object(line,'$.actions')) tmp as action
where dt='2020-06-14'
and get_json_object(line,'$.actions') is not null;

==========================业务数据==========================
评价事实表（事务型事实表）
insert overwrite table dwd_comment_info partition (dt)
select
    date_format(create_time,'yyyy-MM-dd')
from ods_comment_info
where dt='2020-06-14';
insert overwrite table dwd_comment_info partition(dt='2020-06-15')
select
    create_time
from ods_comment_info where dt='2020-06-15';

加购事实表（周期型快照事实表，每日快照）
insert overwrite table dwd_cart_info partition(dt='2020-06-14')
select
from ods_cart_info
where dt='2020-06-14';
insert overwrite table dwd_cart_info partition(dt='2020-06-15')
select
from ods_cart_info
where dt='2020-06-15';

优惠券领用事实表（累积型快照事实表）
insert overwrite table dwd_coupon_use partition(dt)
select
    id,
    coupon_id,
    user_id,
    order_id,
    coupon_status,
    get_time,
    using_time,
    used_time,
    expire_time,
    coalesce(date_format(used_time,'yyyy-MM-dd'),date_format(expire_time,'yyyy-MM-dd'),'9999-99-99')
from ods_coupon_use
where dt='2020-06-14';

insert overwrite table dwd_coupon_use partition(dt)
select
    nvl(new.id,old.id),
    nvl(new.coupon_id,old.coupon_id),
    nvl(new.user_id,old.user_id),
    nvl(new.order_id,old.order_id),
    nvl(new.coupon_status,old.coupon_status),
    nvl(new.get_time,old.get_time),
    nvl(new.using_time,old.using_time),
    nvl(new.used_time,old.used_time),
    nvl(new.expire_time,old.expire_time),
    coalesce(date_format(nvl(new.used_time,old.used_time),'yyyy-MM-dd'),date_format(nvl(new.expire_time,old.expire_time),'yyyy-MM-dd'),'9999-99-99')
from
(
    select
        id,
        coupon_id,
        user_id,
        order_id,
        coupon_status,
        get_time,
        using_time,
        used_time,
        expire_time
    from dwd_coupon_use
    where dt='9999-99-99'
)old
full outer join
(
    select
        id,
        coupon_id,
        user_id,
        order_id,
        coupon_status,
        get_time,
        using_time,
        used_time,
        expire_time
    from ods_coupon_use
    where dt='2020-06-15'
)new
on old.id=new.id;
```

###### 4.DWS

```
按天汇总，每天一个分区
访客主题
collect_set(named_struct('page_id',page_id,'page_count',page_count,'during_time',during_time)) page_stats
group by dt,mid_id,brand,model,page_id
用户主题
商品主题
group by date_format(callback_time,'yyyy-MM-dd'),rp.sku_id
[被下单次数、被下单件数、参与活动被下单次数、使用优惠被下单件数、优惠券]dwd_order_detail
[被支付次数、被制服件数、被支付金额]dwd_order_detail,dwd_payment_info
[被退单次数、被退单件数、被退单金额]dwd_order_refund_info
[退款次数、退款件数、退款金额]dwd_order_refund_info
[被加入购物车次数、被收藏次数]dwd_action_log
[好评数、中评数，差评数、默认评价数]dwd_comment_info
优惠券主题
活动主题
地区主题
insert overwrite table dws_area_stats_daycount partition(dt)
group by dt,province_id;
```

###### 5.DWT

```

```

##### 159.InnoDB

```
InnoDB存储引擎提供了具有提交、回滚和崩溃恢复能力的事务安全,
InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保存数据和索引,
Innodb不仅缓存索引还要缓存真实数据，对内存要求较高，而且内存大小对性能有决定性的影响。
```

##### 160.数据库的类型

```
(1)
关系型数据库：底层是二维表结构，有行有列，类似于excel结构，可以表示负责数据的关系等[MySQL][ Structured Query Language][Relational]
(2)
文档型数据库：底层是xml文件[MongoDB][Document]
<root>
	<students>
		<student>
			<id>1</id>
			<name>张三</name>
		</student>
	</students>
</root>
(3)
键值对型：（key,value）,一般是用于内存数据，优势：快[Redis]
 
```

##### 161.NOSQL

```
NoSQL(NoSQL = Not Only SQL )，意即“不仅仅是SQL”，泛指非关系型的数据库。
	性能快[redis]
	容量大【hbase】
	扩展性高[hbase扩展性比MySQL强]
举例：假如我想把表加一个字段，MySQL需要更改表结构（MySQL底层是二维表），hbase不需要更改表结构，有动态列Json格式
NoSQL不适用场景
	（用不着sql的和用了sql也不行的情况，请考虑用NoSql）
用不着sql:特别细小的，比如存一个k,v ,简答的，比如说缓存，redis缓存数据库，用MySQL太笨重了，还需要写磁盘；
用了SQL也不行的：mysql单表能抗500-1000万，orical单表过亿使用的是小型机，大到一定级别考虑使用NoSQL

```

##### 162.hadoopHA

```
通过多个NameNode消除单点故障
自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程
Edits日志只有Active状态的NameNode节点可以做写操作；
所有的NameNode都可以读取Edits；
共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）
```

##### 163.有压缩方式选择

```
 Gzip压缩=>压缩率高
 Bzip2压缩=>支持split
 lzo
 Snappy压缩
```

##### 164.乐观锁和悲观锁

```
悲观锁
执行操作前假设当前的操作肯定（或有很大几率）会被打断（悲观）。基于这个假设，我们在做操作前就会把相关资源锁定，不允许自己执行期间有其他操作干扰。
乐观锁
执行操作前假设当前操作不会被打断（乐观）。基于这个假设，我们在做操作前不会锁定资源，万一发生了其他操作的干扰，那么本次操作将被放弃。Redis使用的就是乐观锁。
```

##### 165.SQL转mr的过程

```
（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。
（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。
（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。
（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。
 (5)
```

##### 166.JVM

```
堆：
java堆是所有线程所共享的一块内存，在虚拟机启动时创建，几乎所有的对象实例都在这里创建，因此该区域经常发生垃圾回收操作。
方法区：
虚拟机栈: 
```

##### 167.数据中台

```
什么是前台？
首先，这里所说的“前台”和“前端”并不是一回事。所谓前台即包括各种和用户直接交互的界面，比如web页面，手机app；也包括服务端各种实时响应用户请求的业务逻辑，比如商品查询、订单系统等等。
什么是后台？
后台并不直接面向用户，而是面向运营人员的配置管理系统，比如商品管理、物流管理、结算管理。后台为前台提供了一些简单的配置。
```

##### 168.数据胡

```
数据湖（Data Lake）是一个存储企业的各种各样原始数据的大型仓库，其中的数据可供存取、处理、分析及传输。 hudi
目前，Hadoop是最常用的部署数据湖的技术，所以很多人会觉得数据湖就是Hadoop集群。数据湖是一个概念，而Hadoop是用于实现这个概念的技术。
```

##### 169.数据仓库和数据胡

```
数据仓库：
主要处理历史的、结构化的数据，而且这些数据必须与数据仓库事先定义的模型吻合。
数据仓库分析的指标都是产品经理提前规定好的。按需分析数据。（日活、新增、留存、转化率）
数据湖：
能处理所有类型的数据，如结构化数据，非结构化数据，半结构化数据等，数据的类型依赖于数据源系统的原始数据格式。非结构化数据（语音、图片、视频等）
根据海量的数据，挖掘出规律，反应给运营部门。
拥有非常强的计算能力用于处理数据。
数据挖掘
```

##### 170.极限情况下，hashMap放多少条数据会形成红黑二叉树

```
极限情况下，放11条数据就会形成红黑二叉树；
为什么会形成红黑二叉树？
因为链表的数据太长了，线性查找的慢，所以希望把链表结构变成红黑二叉树，极限情况，需要把所有的数据放在一个链表上，才会形成红黑二叉树；
往hashmap放数据的规则？
index=hash(key.hashcode) & (length - 1)
用key的hashcode做hash散列算法之后的值跟当前数组长度-1的值做位运算
链表如果两个key的值相同的话，会有覆盖的概念，需要重写equals方法，使其返回值永远为false，则不会相等，不会覆盖；
8（16） => 9(32) => 10(64) => 11(红黑二叉树treeNode)
在一个链表，增加到8个数据，容量16，第9个数据容量32，第10个数据容量64,第11个会形成红黑二叉树treeNode;
如果一个小格子的数据太多了，那么就会先考虑扩容，当11个出现时，扩容已经解决不了了，需要改变数据的结构，使用红黑二叉树；
为什么扩容是16，32，64？
2的N次方保证(length - 1)的二进制前面的树全是1，，意味着每个小格子都会有机会取到的，能够均衡存储的空间，否则有些空间取不到，就会被浪费掉；
新的版本中，如果给的不是2的N次方，那么它会自动帮你变成2的N次方
#为什么HASHMAP的扩容是二倍？
//index=hash(key.hashcode) & (length - 1)
长度为5
（0，1，2，3，4）
 0001 0100
&
 0000 0100 =>
 五个小格子，分布不均匀
 length为2的n次方，2的N次方保证全部为1，数据均匀分布
 0001 0001
&
 0001 1111
 保证2的N次方，使用位&运算,KAFKA取余%分区
 扩展？
 kafka中topic有三个partition，如何把a,b,c存起来，这块绝对不可能使用位运算，位运算一定要保证2的N次方才行，Kafka中的主题使用的是取余；
 redis使用的是位运算？
 redis框架存贮数据跟hashMap存储是完全相同的；保证容量是2的N次方，就可以使用位运算决定数据在什么位置，如果保证不了的话就使用取余
 有16384 hash slot小格子
 crc16(key,keylen) & 0x3FFF(16383)
 hash(key.hashCode) & (length-1)
 16k => 16384
```

##### 171.设计模式

```
(1)
单例设计模式

(2)
构造者设计模式

(3)
装饰者设计模式

```



##### 172.锁

```
同步:synchronized
同步锁：当在一个java虚拟机多个线程操作一个变量的时候就会出现线程安全问题，这个时候就会用到同步锁。
异步锁：就是多个java 虚拟机或者说是服务器，操作同一个变量是，会出现线程安全问题，使用需要使用异步锁来处理。
```

##### 173.分布式事务

```

```

##### 174.线程池

```
空闲线程数量，当达到空闲线程数量时，线程会被销毁；多余的线程会在多长时间内销毁
最大线程数量，当达到高峰时，会自动申请线程，但不会超过最大线程数量
```

##### 175.jvm内存分配及回收策略

```
(1)
栈区：栈分为Java虚拟机栈和本地方法栈
(2)
堆区：堆被所有线程共享区域，在虚拟机启动时创建，唯一目的存放对象实例。堆区是gc的主要区域，通常情况下分为，年轻代和老年代
(3)
方法区：被所有线程共享区域，用于存放已被加载的类信息，常量，静态变量等数据
(4)
程序计数器：当前线程执行的行号指示器。通过改变计数器的值来确定下一条指令，比如循环，分支，跳转，异常处理等，线程恢复也是通过计数器来完成的；
回收策略以及Minor GC和Major GC：
1)	对象优先在堆的Eden区分配。
2)	大对象直接进入老年代。
3)	长期存活的对象将直接进入老年代。
当Eden区没有足够的空间进行分配时，虚拟机会执行一次Minor GC.Minor GC通常发生在新生代的Eden区，在这个区的对象生存期短，往往发生GC的频率较高，回收速度比较快;Full Gc/Major GC 发生在老年代，一般情况下，触发老年代GC的时候不会触发Minor GC,但是通过配置，可以在Full GC之前进行一次Minor GC这样可以加快老年代的回收速度。

```

##### 176.类加载过程

```
(1)加载
a.	通过一个类的全限定名获取该类的二进制流。

(2)验证
a.	文件格式验证
c.	字节码验证
(3)准备
为类的静态变量分配内存并初始化默认值，这些内存都在方法区中进行分配；
(4)解析
解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。 	
(5)初始化
到了初始化阶段，才真正开始执行类中定义的Java程序代码。


```



### 二、Flink

##### 1.运行架构

```
	(1)
	Flink 架构
	• 一个 JobManager 和一个或者多个 TaskManager。
	• 典型的 Master-Slave 架构。
	(2)
	作业管理器 (JobManager)
	• 控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的 JobManager 所控制执行。
	• JobManager 会先接收到要执行的应用程序，这个应用程序会包括：作业图 (JobGraph)、逻辑数据流图和打包了所有的类、库和其它资源的 JAR 包。
	• JobManager 会把 JobGraph 转换成一个物理层面的数据流图，这个图被叫做“执行图”(ExecutionGraph)，包含了所有可以并发执行的任务。
	• JobManager 会向资源管理器 (Flink 的资源管理器) 请求执行任务必要的资源，也就是任务管理器 (TaskManager) 上的任务插槽（slot）。一旦
	它获取到了足够的资源，就会将执行图 (DAG) 分发到真正运行它们的 TaskManager 上。而在运行过程中，JobManager 会负责所有需要中央协调的操作，
	比如说检查点 (checkpoints) 的协调。
	(3)
	资源管理器
	• 资源管理器 (ResourceManager)：ResourceManager 负责Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调
	度的单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone部署）实现了对应的 ResourceManager。在 standalone 
	设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。
	• 分发器 (Dispatcher)：Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 
	Flink WebUI 用来提供作业执行信息。
	• JobMaster：JobMaster 负责管理单个 JobGraph 的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的JobMaster。
	• 主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManager 插槽是 Flink 中定义的处理资源单元。
	• Flink 为不同的环境和资源管理工具提供了不同资源管理器，比如 YARN、Mesos、Kubernetes（管理 docker 容器组成的集群），以及 Standalone（独立集群）部署。
	• 当 JobManager 申请插槽资源时，Flink 的资源管理器会将有空闲插槽的 TaskManager 分配给 JobManager。如果 Flink的资源管理器没有足够的插槽来满足 JobManager 的请求，
	它还可以向 Yarn 的资源管理器发起会话，以提供启动TaskManager 进程的容器。
	(4)
	分发器
	• 可以跨作业运行，它为应用提交提供了 RESTful 接口(GET/PUT/DELETE/POST)。
	• 当一个应用被提交执行时，分发器就会启动并将应用移交给一个 JobManager。
	• Dispatcher 也会启动一个 Web UI(localhost:8081)，用来方便地展示和监控作业执行的信息。
	• Dispatcher 在架构中可能并不是必需的，这取决于应用提交运行的方式。
	(5)
	TaskManager 和 Slots
	• Flink 中每一个 TaskManager 都是一个 JVM 进程，每一个任务插槽都会启动一个线程，它可能会在独立的线程上执行一个或多个 subtask，每一个子任务占用一个任务插槽
	（Task Slot）
	(6)
	图数据结构的转化
	• StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。
	• JobGraph：StreamGraph 在编译的阶段经过优化后生成了JobGraph，提交给 JobManager 的数据结构。主要的优化为，
	将多个符合条件（窄依赖，没有 shuffle）的算子 chain 在一起作为一个节点。
	• ExecutionGraph：JobManager 根据 JobGraph 生成
	ExecutionGraph。ExecutionGraph 是 JobGraph 的并行化版本，是调度层最核心的数据结构。
	• 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个 TaskManager 上部署 Task 后形成的“图”，
	并不是一个具体的数据结构。 
	(7)
	并行度 (Parallelism)
	• 一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）。一般情况下，一个 stream 的并行度，可以认
	为就是其所有算子中最大的并行度。
		• One-to-one
			map、filter、flatMap 等算子都是 one-to-one的对应关系。
		• Redistributing
			stream 的分区会发生改变。
			例如，keyBy 基于 hashCode 重分区、而 broadcast 和rebalance 会随机重新分区，这些算子都会引起 redistribute
			过程，而 redistribute 过程就类似于 Spark 中的 shuffle 过程。

	(8)
	任务链 (Operator Chains)
	• Flink 采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个
	算子设为相同的并行度，并通过本地转发（localforward）的方式进行连接
	• 相同并行度的 one-to-one 操作，Flink 这样相连的算子链接在一起形成一个 task，原来的算子成为里面的 subtask

	(9)
	Flink vs Spark Streaming
	• 流（stream）和微批
	• 数据模型
		• Spark 采用 RDD 模型，Spark Streaming 的 DStream 实际上也就是一组组小批数据 RDD 的集合
		• Flink 基本数据模型是数据流，以及事件（Event）序列
		（Integer、String、Long、POJO Class）

	• 运行时架构
		• Spark 是批计算，将 DAG 划分为不同的 Stage，一个 Stage完成后才可以计算下一个 Stage
		• Flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理
	(10)
	Flink 的主要特点
		• 支持事件时间（event-time）和处理时间（processing-time）语义
		• 精确一次（exactly-once）的状态一致性保证
		• 低延迟，每秒处理数百万个事件，毫秒级延迟（实际上就是没有延迟）
		• 与众多常用存储系统的连接（ES，HBase，MySQL，Redis…）
		• 高可用（zookeeper），动态扩展，实现 7*24 小时全天候运行
		• 事件驱动（Event-driven）
		• 基于流的世界观
			• 在 Flink 的世界观中，一切都是由流组成的，离线数据是有界的流；实时数据是一个没有
			界限的流：这就是所谓的有界流和无界流

```

##### 2.窗口

```
	(1)
	• 窗口（Window）就是将无限流切割为有限流的一种方式，它会将流数据分发到有限大小的桶（bucket）中进行分析
	(2)
	Window 类型
	• 时间窗口（Time Window
		• 滚动时间窗口
		• 滑动时间窗口
		• 会话窗口（只有 Flink 支持
			• 由一系列事件组合一个指定时间长度的 timeout 间隙组成，
			• 也就是一段时间没有接收到新数据就会生成新的窗口
			• 特点：时间无对齐
			• 只有 Flink 支持会话窗口
	• 计数窗口（Count Window）
		• 滚动计数窗口
		• 滑动计数窗口
	(3)
	• 处理时间窗口
		• 滚动窗口
			.window(TumblingProcessingTimeWindows.of(Time.seconds(5)))
		• 滑动窗口
			.window(SlidingProcessingTimeWindows.of(Time.seconds(10),,→ Time.seconds(5)))
		• 会话窗口
			.window(ProcessingTimeSessionWindows.withGap(Time.seconds(10)))

	• 事件时间窗口
		• 滚动窗口
			.window(TumblingEventTimeWindows.of(Time.seconds(5)))
		• 滑动窗口
		.	window(SlidingEventTimeWindows.of(Time.seconds(10),
		,→ Time.seconds(5)))
		• 会话窗口
			.window(EventTimeSessionWindows.withGap(Time.seconds(10)))
	(4)
	窗口聚合函数
	• 增量聚合函数
		• 每条数据到来就进行计算，只保存一个简单的状态（累加器）
		• ReduceFunction, AggregateFunction
		• 当窗口闭合的时候，增量聚合完成
		• 处理时间：当机器时间超过窗口结束时间的时候，窗口闭合
		• 来一条数据计算一次
	• 全窗口聚合函数
		• 先把窗口所有数据收集起来，等到计算的时候会遍历所有数据
		• ProcessWindowFunction
	(5)
	其他可选 API
	• .trigger() ——触发器
		• 定义窗口什么时候关闭，触发计算并输出结果
	• .allowedLateness() ——允许处理迟到的数据
	• .sideOutputLateData() ——将迟到的数据放入侧输出流
	• .getSideOutput() ——获取侧输出流
	• .evictor() ——移除器
	• 定义移除某些数据的逻辑
```

##### 3.时间语义和WaterMark

```
	(1)
	时间（Time）语义
	• Event Time（事件时间）：事件创建的时间（必须包含在数据源中的元素里面）
	• Ingestion Time（摄入时间）：数据进入 Flink 的 source 算子的时间，与机器相关
	• Processing Time（处理时间）：执行操作算子的本地系统时间，与机器相关 
	(2)
	乱序数据的影响
	• 乱序数据会让窗口计算不准确
	• 由于网络、分布式等原因，会导致乱序数据的产生
	• 当 Flink 以 Event Time 模式处理数据流时，它会根据数据里的时间戳来处理基于时间的算子
	(3)
	水位线（Watermark）
	• 数据流中的 Watermark 用于表示 Timestamp 小于Watermark 的数据，都已经到达了，因此，Window 的执行
		也是由 Watermark 触发的（水位线 >= 窗口结束时间）。
	• Watermark 是一种衡量 Event Time 进展的机制（逻辑时钟），可以设定延迟触发
	• Watermark 是用于处理乱序事件的，而正确的处理乱序事件，通常用 Watermark 机制结合 Window 来实现；
	(4)
	Watermark 的特点
	• Watermark 是一条特殊的数据记录，由程序员编程产生
	• 水位线是流中的特殊事件，由程序员编程插入到数据流中
	• Watermark 与数据的时间戳相关
	• Watermark 必须单调递增，以确保任务的事件时间时钟在向前推进，而不是在后退，（Watermark 就是当前数据
	流的逻辑时钟）
	(5)
	•延迟触发
		.<SensorReading>forBoundedOutOfOrderness(Duration.ofSeconds(5))
	•对于排好序的数据，不需要延迟触发，可以只指定时间戳就行了。
		.WatermarkStrategy.<SensorReading>forMonotonousTimestamps()
	(6)
	•自定义水位线
		1 @Public
		2 public interface WatermarkGenerator<T> {
		3
		4 /**
		5 * 每来一个事件都会调用, 允许水位线产生器记忆和检查事件的时间戳。
		6 * 允许水位线产生器基于事件本身发射水位线。
		7 */
		8 void onEvent(T event, long eventTimestamp, WatermarkOutput output);
		9
		10 /**
		11 * 周期性的调用（默认 200ms 调用一次）, 可能会产生新的水位线，也可能不会。
		12 *
		13 * 调用周期通过 ExecutionConfig#getAutoWatermarkInterval() 方法来配置。
		14 */
		15 void onPeriodicEmit(WatermarkOutput output);
		16 }

	•周期性产生水位线
		1 public class BoundedOutOfOrdernessGenerator implements WatermarkGenerator<MyEvent> {
		2
		3 private final long maxOutOfOrderness = 3500; // 最大延迟时间是 3.5s
		4
		5 private long currentMaxTimestamp;
		6
		7 @Override
		8 public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) {
		9 currentMaxTimestamp = Math.max(currentMaxTimestamp, eventTimestamp);
		10 }
		11
		12 @Override
		13 public void onPeriodicEmit(WatermarkOutput output) {
		14 // 产生水位线的公式：观察到的最大时间戳 - 最大延迟时间 - 1ms
		15 output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1));
		16 }
		17
		18 }

	•不规则水位线的产生
		1 public class PunctuatedAssigner implements WatermarkGenerator<MyEvent> {
		2
		3 @Override
		4 public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) {
		5 if (event.hasWatermarkMarker()) {
		6 output.emitWatermark(new Watermark(event.getWatermarkTimestamp()));
		7 }
		8 }
		9
		10 @Override
		11 public void onPeriodicEmit(WatermarkOutput output) {
		12 // 不需要做任何事情，因为我们在 onEvent 方法中发射了水位线
		13 }
		14 }
```

##### 4.Flink状态管理

```
	(1)
	 算子状态（Operator State）：
	算子状态当前的任务槽可见；
	(2)
	键控状态（Keyed State）：
	键控状态只针对当前的key，分区可见;

	• 值状态（ValueState）：将状态表示为单个的值
	• 列表状态（List State）：将状态表示为一组数据的列表
	• 字典状态（MapState）：将状态表示为一组 Key-Value 对
	• 聚合状态：将状态表示为一个用于聚合操作的列表

	(3)
	 状态后端（State Backends）
	 A.
	 由于有效的状态访问对于处理数据的低延迟至关重要，因此每个并行任务都会在本地维护其状态，以确保快速的状态访问,
	 状态的存储、访问以及维护，由一个可插入的组件决定，这个组件就叫做状态后端（state backend）
	 状态后端负责的两件事:
	 	<1>本地的状态管理，
	 	<2>将检查点（checkpoint）状态写入远程存储（HDFS、RocksDB 之类的）
	 B.
	 状态后端的分类
	 • MemoryStateBackend(Default)
	 	• 特点：快速、低延迟，但不稳定
	 • FsStateBackend
	 	•  存到远程的持久化文件系统（FileSystem），本地状态，跟 MemoryStateBackend 一样，也会存在TaskManager
	 		 的 JVM 堆上
	 	• 同时拥有内存级的本地访问速度，和更好的容错保证
	 • RocksDBStateBackend
	  	• 将所有状态序列化后，存入本地的 RocksDB 中存储。
		• RocksDB 是一个硬盘 KV 数据库，LevelDB，RocketDB
```

##### 5.容错机制

```
1.
故障恢复机制的核心?
	A.
	Flink 故障恢复机制的核心，就是应用状态的一致性检查点
	B.
	有状态流应用的一致检查点，其实就是所有任务的状态，在某个时间点的一份拷贝（一份快照）；这个时间点，应该是所有任务都恰好处理完一个相同的输入数据
	（其实就是检查点屏障）的时候
2.
怎样从检查点恢复状态?
	(1)遇到故障之后，第一步就是重启应用
	(2)第二步是从 checkpoint 中读取状态，将状态重置
		从检查点重新启动应用程序后，其内部状态与检查点完成时状态完全相同
	(3)开始消费并处理检查点到发生故障之间的所有数据
3.
检查点的实现算法?
简单想法(同步思想)
	 A.
	 暂停应用，保存状态到检查点，再重新恢复应用（SparkStreaming）
改进实现(异步思想)
	 A.
	 基于 Chandy-Lamport 算法的分布式快照算法
	 B.
	 将检查点的保存和数据处理分离开，不暂停整个应用
4.
检查点算法：
	(1) 分界线对齐：barrier 向下游传递，sum 任务会等待所有输入分区的 barrier 到达
		A. 对于 barrier 已经到达的分区，继续到达的数据会被缓存
		B.而 barrier 尚未到达的分区，数据会被正常处理
		C. 当收到所有输入分区的 barrier 时，任务就将其状态保存到状态后端的检查点中，然后将 barrier 继续向下游转发
	(2)Sink 任务向 JobManager 确认状态保存到 checkpoint 完毕
		A.当所有任务都确认已成功将状态保存到检查点时，检查点就真正完成了
	(3) 保存点是一个强大的功能。除了故障恢复外，保存点可以用于：有计划的手动备份，更新应用程序，版本迁移，暂停和重启应用，等等
```

##### 6.端到端状态一致性?

```
	(1)整个端到端的一致性级别取决多有组件中一致性最弱的组件
	(2)Source端 => 可重设数据读取的位置(kafka,FileSyatem)
	(3)Sink端 => 从故障恢复时,数据不会重复写入外部系统
				 <1>幂等写入
				 	（k,v）字典数据结构 => 一个操作重复多次,但只导致一次结果更改
				 <2>事务写入
				 	A.
				 	ACID => 原子性（atomicity)
							一致性（consistency)
							隔离性（isolation）
							持久性（durability）
					B.
					实现思想:
					构建的事务对应着checkpoint,等checkpoint真正的完成的时候,才把所有对应的结果真正的写入到Sink系统中
					C.
					实现方式:
						*1*
						预写日志(WAL,Write Ahead Log)
						State Backend => MySQL => atleastOnce
						*2*
						两阶段提交(2PC,two Phase Commit)
							<1>对于每个checkpoint,Sink都会启动一个事务(下游设备的事务,MySQL,Kafka),并将接下来接收的数据添加到事务里
							<2>将这些数据写入外部 Sink 系统，但不提交它们——这时只是“预提交”
							<3>当收到checkpoint完成的通知时,它们才正式的提交事务,实现结果的真正写入
								a.这种方式真正实现了 Exactly-Once，它需要一个提供事务支持的外部 Sink 系统。Flink 提供了TwoPhaseCommitSinkFunction 接口。
								b.有可能在一段时间内看不到 Sink 的结果
							<0>2PC,flink对外部 Sink系统的要求
								a.提交事务必须是幂等操作
								b.Sink任务必须能够在进程失败后恢复事务
								C.收到checkpoint完成之前,Sink必须是"等待提交";
								d.在checkpoint的间隔期间,必须能够开启一个事务并接受数据的写入
								e.外部的Sink系统必须提供事务支持

	(4)Flink+Kafka 端到端状态一致性的保证
		<1>内部
			利用checkpoint机制,把状态存盘HDFS,发生故障时可以恢复,保证内部的一致性
		<2>Source
			Kafkaconsumer作为Source,将偏移量保存下来,如果后续出现了问题,可以由连接器重置偏移量,重新消费数据,保证一致性
		<3>Sink
			KafkaProducer,使用两阶段Sink2PC提交,需要实现一个 TwoPhaseCommitSinkFunction
	(5)Exactly-Once 两阶段提交
		<1>第一条数据来了之后，开启一个 Kafka 的事务（transaction）,正常写入 Kafka 分区日志但标记为未提交，这就是“预提交”
		<2>JobManager 触发 checkpoint 操作，barrier 从 source 开始向下传递，遇到 barrier(分界线) 的算子将状态存入状态后端，并通知JobManager
		<3>Sink 连接器收到 barrier，保存当前状态，存入 checkpoint，通知 JobManager，并开启下一阶段的事务，用于提交下个检查点的数据
		<4>JobManager 收到所有任务的通知，发出确认信息，表示checkpoint 完成
		<5>Sink 任务收到 JobManager 的确认信息，正式提交这段时间的数据
		<6>外部 Kafka 关闭事务，提交的数据可以正常消费了
```

##### 7.一致性检查点?

```
	(1)轻量级快照机制 => 检查点 => 保证exactly-once
	(2)有状态应用的一致检查点 => 所有任务的状态,在某个时间的一份拷贝(快照)[这个时间点,所有任务都恰好处理完一个相同数据输入的时候]
		[使用了检查点屏障]
	(3)Flink故障恢复机制的核心 => 有状态的一致检查点

总结:
	flink流处理内部一致性的保证 => 检查点一致性(分布式异步快照算法)
```

##### 8.什么是状态一致性?

```
	(1)有状态的流处理,内部的每个算子任务都可以有自己的状态
	(2)对于流处理器内部来说,所谓的状态一致性,就是我们说的计算结果要保证准确
	(3)一条数据不应该丢失,也不应该重复计算
	(4)在遇到故障可以恢复状态,恢复以后的重新计算,结果应该也是完全正确的
```

### 三、SparkCore

##### 1.Yarn模式运行机制

```
    (1)执行脚本提交任务，启动一个SparkSubmit的JVM进程
    (2)SparkSubmit的main方法反射调用YarnClusterApplication的main方法
    (3)YarnClusterApplication创建Yarn客户端，然后向Yarn服务器发送执行指令：bin/java ApplicationMaster
    (4)Yarn框架收到指令后会在指定的NM中启动ApplicationMaster
    (5)ApplicationMaster启动Driver线程，执行用户的作业
    (6)AM向RM注册，申请资源
    (7)获取资源后AM向NM发送指令： bin/java YarnCoarseGrainedExecutorBackend
    (8)CoarseGrainedExecutorBackend进程会接收消息，跟Driver通信，注册已经启动的Executor;然后启动计算对象Executor等待接收任务
    (9)Driver线程继续执行完成作业的调度和任务的执行
    (10)Driver分配任务和监控任务的执行
    注意:
         0.
         rpcEnv.setupEndpoint("Executor"，new CoarseGrainedExecutorBackend)
            11.自己给自己发送消息能不能接收呢？
            override def receive: PartialFunction[Any, Unit] = {
                case RegisteredExecutor =>
                  logInfo("Successfully registered with driver")
                  try {
                    executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false,
                      resources = _resources)
                    driver.get.send(LaunchedExecutor(executorId))

                接收后，executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false,
                      resources = _resources)
            12.之前画图的Executor其实画的是 ExecutorBackend,是进程；真正的Executor是一个计算的对象
         
         A.
         SparkSubmit、ApplicationMaster、CoarseGrainedExecutorBackEnd是独立的进程
         Driver是独立的线程
         Executor和YarnClusterApplication是对象
         因此Driver线程在反射调用自己的main方法
         rpcEnv:当前Spark框架中组件与组件之间的通信环境
         处理(移动数据不如移动计算)如果计算任务发给某个节点的话那么计算任务可以不需要传输，性能会得到提高
         这一步处理，就是匹配各个节点的关系
         handleAllocatedContainers(allocatedContainers.asScala)

         B.
         创建几个Executor
            20.只要2个executor，但是yarn给了10个容器？
            选择就需要靠节点等之间的关系
            runAllocatedContainers(containersToUse)
            //把每个可用的container进行循环遍历
            for (container <- containersToUse) {}
            21.启动一个线程来构造这个对象；launcherPool线程池，execute执行
            launcherPool.execute
            //new一个能运行的Executor
            new ExecutorRunnable
            //返回资源可用列表
            那么会创建几个Executor对象，看container <- containersToUse
            循环了几次，有几个container就会循环几次，循环几次就会创建几个Executor对象
            因此一个container中就会运行一个Executor。
            一般不会说启动了几个container，都会说启动几个Executor,因为Executor与container都是一对一对应的

        C.
        Executor运行
            22.运行
            Executor.run
            点击run
            nodeMaganer的客户端(有client就会有服务器server)
            nmClient = NMClient.createNMClient()
            有container，那么找到container所在的nodeManager
            startContainer()
            23.谁发动的startContainer()，是ApplicationMaster
                startContainer()
                //准备指令，为了能够在容器中运行
                prepareCommand
                点击prepareCommand
                //是一个Java虚拟机的启动参数
                javaOpts += "-Xmx" + executorMemoryString
                Seq(Environment.JAVA_HOME.$$() + "/bin/java", "-server") ++
                javaOpts ++
                  Seq("org.apache.spark.executor.YarnCoarseGrainedExecutorBackend",
```

##### 2.spark内部通信

```
    (1)
    A.
    driver => NettyRpcEnv
        通信服务器TransportServer ->endpoint终端        通信客户端TransportClient ->endpointRef终端引用
        收件箱 ->Inbox                                  发件箱 -> Outbox
        Inbox : protected val messages = new java.util.LinkedList[InboxMessage]()
        Outbox : private val messages = new java.util.LinkedList[OutboxMessage]使用链表结构去做
        每个OutBox跟TransportClient绑定的，Inbox有一个，Outbox有多个，给每一个通信的终端都准备一个发件箱
        客户端通过Rpcaddress，找到另一个终端endpoint的收件箱，进行消息的发送
        rpc为进程与进程之间的通信协议
    executor => NettyRpcEnv
        通信服务器TransportServer ->endpoint终端         通信客户端TransportClient ->endpointRef终端引用
    (2)
    A.
    通信原理
        通信客户端向服务器发送连接 通信
        服务器与客户端的通信原理是IO
    B.
    通信的方式
    IsolatedRpcEndpoint//Driver和Executor都是通信的终端
    每个通信终端都有一个生命周期* {@code constructor -> onStart -> receive* -> onStop}
    TransportServer => endpoint终端，用来做接收消息端
    TransportClient => endpointRef终端引用,用来发送消息端
    发送消息后还不断询问ask()
    BackEnd:后台
    Endpoint:终端
    Netty:通信框架
    (3)
    Netty通信框架，异步通信
    底层通信代码
    driver和executor采用netty(Epoll/NIO)的通信框架进行通信
    默认是NIO，但是可以通过IO.mode的模式配置，EPOLL或者NIO
    因为异步(AIO)是微软做的，支持windows操作系统，不支持Linux
    switch (mode) {
          case NIO:
            return NioServerSocketChannel.class;
          case EPOLL://模仿的AIO
            return EpollServerSocketChannel.class;
```

##### 3.RDD\任务阶段的划分

```
    A.
    17.当阶段划分完成之后，提交任务
    submitStage(finalStage)
    点击submitStage
    提交Task
    submitMissingTasks
    Task是如何生成的？
    val tasks: Seq[Task[_]] = try {
      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match {
        case stage: ShuffleMapStage =>
          stage.pendingPartitions.clear()
          partitionsToCompute.map { id =>
            val locs = taskIdToLocations(id)
            val part = partitions(id)
            stage.pendingPartitions += id
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
          }
    Task跟阶段有关，一个阶段中最后一个RDD有多少个分区，就是它的任务数量
    stage.pendingPartitions => partitionsToCompute.map => new ShuffleMapTask

    B.
    假如当前有4个任务，那么这4个任务是怎样发给Executor去执行的呢？
    (1)当前RDD会形成一个一个的Task
    (2)tasks包装成任务集：new TaskSet(tasks.toArray
    (3)def submitTasks(taskSet: TaskSet): Unit //TaskSet为任务集，把多个Task给包装起来了
    (4)任务集管理器，调度：决定任务谁先执;val manager = createTaskSetManager(taskSet, maxTaskFailures)
    (5)点击addTaskSetManager,//任务池def rootPool: Pool
    C.
    放到任务池中怎样来调度呢？
    (1)给Executor的通信后台发送消息：启动任务(把任务序列化后传进去)
    (2)计算对象Executor开始执行,计算对象当中有线程池：threadPool.execute(tr)
    (3)执行 TaskRunner 里边的run方法
        a.在run方法中干什么了?
            task.run
            一个线程一个task.run，那么两个线程就会有两个Task.run,当你有了多线程之后，现在资源有3个核，并行度是3
            底层使用的是线程池，当资源足够的情况下，是可以并行计算的；

            <1>任务的执行涉及到文件的读写，shuffle阶段
                val res = task.run
                runTask(context)
            <2>ShuffleMapTask:写write
                dep.shuffleWriterProcessor.write(rdd, dep, mapId, context, partition)
            <3>ResultTask：读read
```

##### 4.一源码-SparkSubmit

```
    A.
    YarnClusterApplication
    <1>建立RM的连接对象
        --yarnClient = YarnClient.createYarnClient
    <2>提交应用 
        submitApplication
            --yarnClient.start()
                //创建容器启动(launch)对象
                //command(集群) => /bin/java org.apache.spark.deploy.yarn.ApplicationMaster
                //command(客户端) => /bin/java org.apache.spark.deploy.yarn.ExecutorLauncher
                //Seq(amClass) ++ userClass
                --createContainerLaunchContext
                //创建Application环境对象
                --createApplicationSubmissionContext

                //通过RN的连接对象来连接应用
                --yarnClient.submitApplication(appContext)
    <3>客户端已提交完成，剩下为yarn集群内部提交，会在yarn内部的NodeManager里边启动一个ApplicationMaster进程

    B.
    <1>
        16.当前在org.apache.spark.deploy.yarn.YarnClusterApplication中
        new Client(new ClientArguments(args), conf, null).run()
        点击new ClientArguments(args)
        case ("--class") :: value :: tail =>
                  userClass = value
                  args = tail
    <2>
        向RM服务器来提交应用，提交请求；
        rmClient.submitApplication(request);
        但是提交都提交了哪些东西？
            a.
                //appContext应用程序的环境
                //创建一个容器启动的环境(Yarn中所有应用程序都是在容器中做的)
            b.
                //create一个Application的环境对象
            c.
                20.将command放到容器的环境当中，所以这是一个指令，这个指令会随着容器的环境发给yarn,yarn得到指令，通过指令来执行
                val printableCommands = commands.map(s => if (s == null) "null" else s).toList
                amContainer.setCommands(printableCommands.asJava)
            d.
                这个指令command很重要，但是这个指令是什么呢？
                val commands = prefixEnv ++
                  Seq(Environment.JAVA_HOME.$$() + "/bin/java", "-server") ++
                  javaOpts ++ amArgs ++
                  Seq(
                    "1>", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout",
                    "2>", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr")
                commands先是一个预先环境prefixEnv，然后在增加JAVA_HOME和/bin/java
                -server表示服务器环境

                javaOpts ++ amArgs ++Java的配置和amArgs准备好了
                Java虚拟机的基本配置
                javaOpts += ("-Dspark.yarn.app.container.log.dir=" + ApplicationConstants.LOG_DIR_EXPANSION_VAR)

            e.
                ApplicationMaster解耦合使用将计算和资源解耦合
                23.command封装好之后会发送给给yarn，yarn会执行这个指令，yarn会在自己的集群中通过RM找到nodeManager执行
                RM会在NM中启动一个comtainer，执行指令
                24.启动AM(ApplicationMaster),是启动一个Java虚拟机进程
                bin/java org.apache.spark.deploy.yarn.ApplicationMaster
                所以NM中有个ApplicationMater进程
                启动一个ApplicationMaster进程，那么它一定有main方法

            f.
                -Xmx128mJava虚拟机的启动参数，用来设定Java虚拟机的运行内存的大小
                "$RUNNER" -Xmx128m $SPARK_LAUNCHER_OPTS其实是一个Java虚拟机的启动指令
```

##### 5.spark内核

```
    A.
        Driver是一个线程，它在程序运行的过程中执行了所提供应用程序的main方法，在main方法中创建了spark的上下文环境，创建完成后内部
        可以管理Task和它的阶段以及对任务执行情况的监控
    B.
        Executor其实有2个
        Executor:计算对象
        ExecutorBackend:通信对象,但是它注册到通信环境的终端的名称叫Executor
        一般Executor指的是通信对象，但是真正的Executor是一个计算对象
        当注册申请资源的时候，main方法已经执行，如果没有执行main方法，哪来的上下文对象，如果没有上下文对象哪能够去申请资源
    C.
        上下文环境如果没有准备好，那么是await阻塞状态
        val sc = ThreadUtils.awaitResult(sparkContextPromise.future,
            Duration(totalWaitTime, TimeUnit.MILLISECONDS))
          if (sc != null) {
    D.
        当把上下文对象创建好了以后Driver线程就会被暂停，被阻塞  ，然后走申请资源
        恢复Driver线程，让Driver线程继续执行
        resumeDriver()
        val sc = ThreadUtils.awaitResult(sparkContextPromise.future,Duration(totalWaitTime, TimeUnit.MILLISECONDS))
```

##### 6.spark shuffle解析

```

    (0)shuffle 的核心要点
        <1>在划分stage的时候，最后一个stage成为finalstage,它本质是一个ResultStage对象,前面所有的stage称为ShuffleMapStage
        <2>ShuffleMapStage的结束伴随着shuffle文件的写磁盘
        <3>ResultStage基本上对代码中的action算子，即将一个函数应用在RDD的各个partition的数据集上，以为着一个job的结束
    (1)HashShuffle[早期的]
        <1>从Task开始各自把自己进行hash计算(分区器:hash,numreduce取模),分出3个不同的类别，每个Task都分成3种类别的数据,
            想把不同的数据汇聚后计算出最终的结果
        <2>reduce会在每个Task上把属于自己的类别数据收集起来，汇集一个同类别的大集合
        <3>每个Task输出3份本地文件，这里4个MapperTasks,所以共输出了4个Takss*3个分类文件=12个本地小文件
    (2)优化后的hashshuffle[小文件变少，但是也没有变得那么的少]
        <0>优化后的hashshuffle过程就是启用合并机制
        <1>合并机制就是复用budder
        <2>开启合并机制的参数spark.shuffle.consolidateFiles,true;如果我们使用HashShuffleManager那么都建议开启这个选项
        <3>4个Tasks,3种数据类型;
        <4>Hash算法会根据你的key进行分类
        <5>在同一个进程中,无论是有多少个Task,都会把相同的key放到同一个buffer里
        <6>然后把buffer写入以core数量为单位的本地文件中，
        <7>这里4个MapperTasks,所以总共输出是2个cores*3 = 6个本地小文件
    (3)SortShuffle[有索引文件m有数字文件，文件数量就会少很多,性能就可以得到提高;里边会有排序,排序会有一些临时的溢写磁盘的文件,最终形成最后的文件]
        <0>数据会先写入一个数据结构,reduceByKey写入Map,一边通过Map局部聚合,一边写入内存;
        <1>join算子写入ArrayList直接写入内存,然后判断是否达到阈值,如果达到阈值就会将内存的数据写入磁盘，清空内存数据结构
        <2>在溢写磁盘前,先根据key进行排序,排序过后的数据会分批写入磁盘文件中
        <2.0>排序规则,先按照分区排序,再按照key排序
        <3>默认数据是1万条,数据会以每批1万条写入到磁盘文件中;
        <4>写入文件通过缓冲区溢写的方式,每次溢写都会产生一个磁盘文件
        <5>也就是说,一个Task过程会产生多个临时文件
        <6>最后每个Task中,将所有的临时文件合并,这就是merge过程
        <7>此过程就所有的临时文件读取出来,一次写入到最终文件
        <7.0>底层采用的是归并排序
        <8>同时单独写一份索引文件,标识下游各个Task在文件中的索引;startOffset和endOffset
    (4)bypass SortShuffle
        bypass机制的触发条件:
            <1>shuffle reduce task的数量小于spark.shuffle.sort.bypassMergeThreshold,默认200
            <2>不是聚合类的shuffle算子(比如reduceByKey)

            A.
            此时Task会为每个reduce端的Task创建一个临时磁盘文件，并将key进行hash,然后根据key的hash,
            将key写入对应的磁盘文件中
            B.
            写到磁盘文件也是先写到内存缓冲，缓冲满了之后再溢写到磁盘文件中
            C.
            最后同样将所有的临时文件都合并成一个磁盘文件，并创建一个单独的索引
        对比?
        byPass???未优化的HashShuffleManager
            一摸一样：
            因为都要创建惊人数量的磁盘文件
            区别:
            byPass最后做一个磁盘文件的合并
            因此少量的最终磁盘文件,也让该机制相对未优化的HashShuffleManager来说,Shuffle read的性能会更好溢写
        bypass???普通的SortShuffleManager
            byapss不会进行排序
            优点:
            shufflewrite的过程中不需要数据的排序操作，也就节省了这部分的性能开销
```

##### 7.shuffle写操作

```
    (1)UnsafeShuffleWriter
        <1>序列化规则中是否支持重定位，Kyro支持
        <2>有预聚合功能不能使用
        <3>下游分区数大于16777216，不能使用
    (2)BypassMergeSortShuffleWriter
        <1>有预聚合功能不能使用
        <2>下游分区配置参数spark.shuffle.sort.bypassMergeThreshold>200，不能使用;
            工作中200有点小,我们可以将它设置为400-500
    (3)SortShuffleWriter
        <1>其他情况下能使用

SortShuffleWriter:
    (1)排序器:ExternalSorter
    (2)预聚合:有预聚合和没有预聚合的区别
        <1>预聚合中含有聚合器对象aggregation
        <2>预聚合中含有排序器对象keyOrdering
        <0>没有预聚合功能:aggregation=none,keyOrdering=none
        <3>预聚合中使用的结构：PartitionedAppendOnlyMap
        <4>没有预聚合使用的结构:PartitionedPairBuffer
    (3)排序时,溢写磁盘[溢写指的是排序过程中的溢写,并不是shuffle落盘的溢写]
        <1>读取数据是否可以整除32 & 当前需要的内存是否大于内存的极限值(5M),
            A.
            它是判断条件，并不是决定条件;判断有没可能需要溢写，去申请更多的内存,申请资源后,然后再判断条件是否满足,是否溢写
            if (elementsRead % 32 == 0 && currentMemory >= myMemoryThreshold)
                myMemoryThreshold=5 * 1024 * 1024BYTE
            B.
            如果申请后的内存还是不够,那么会溢写
            shouldSpill = currentMemory >= myMemoryThreshold
        <2>读取数据超过了数据的极限值：Int的最大值
            shouldSpill = shouldSpill || _elementsRead > numElementsForceSpillThreshold

        /*1*/当达到溢写条件后：溢写，释放内存
            溢写：
            spill(collection)
            此部分为排序的溢写，如果内存大，数据量小那么就没有溢写的概念
        /*1.1*/溢写完成后需要产生一个数据文件,需要怎样做？
            《1》SortShuffleWriter
            《2》sorter.insertAll(records)//排序溢写
            《3》溢写成临时文件，最后产生一个数据文件
                sorter.writePartitionedMapOutput(dep.shuffleId, mapId, mapOutputWriter)
            《4》如果没有溢写文件，里边全部都是内存中的数据
                // Case where we only have in-memory data
                if (spills.isEmpty)
                //有溢写文件和内存中的数据
                //归并排序
                else{
                //归并排序，将多个有顺序的临时文件合并成一个大的文件
                for ((id, elements) <- this.partitionedIterator) {
                     // Merge spilled and in-memory data
                    //归并内存中的数据
                merge(spills, destructiveIterator()
                //合并排序，
                mergeSort(iterators, ordering.get))
                优先级队列
                val heap = new mutable.PriorityQueue[Iter]()(
                优先级队列主要用来做堆排序，归并排序很大程度是用堆排序
            《5》排序后，将溢写的文件合并,并将临时文件更改为文件
                一个数据文件indexTmp.renameTo(indexFile)
                一个索引文件dataTmp.renameTo(dataFile
                if (!indexTmp.renameTo(indexFile)) {
                if (dataTmp != null && dataTmp.exists() && !dataTmp.renameTo(dataFile))


        <3>排序规则为,先按照分区排序,再按照key进行排序[partitionKeyComparator]
            A.
            keyComparator.map(partitionKeyComparator)
            val partitionDiff = a._1 - b._1
                  if (partitionDiff != 0) {
                    //分区比较
            partitionDiff
                  } else {
                    Key比较大小
                    keyComparator.compare(a._2, b._2)
                  }
                }
        /*2*/排序后溢写:先根据分区排序,再根据key进行排序,排序后溢写内存中的迭代数据到磁盘中
            spillMemoryIteratorToDisk(inMemoryIterator)
        /*3*/创建临时文件
            val (blockId, file) = diskBlockManager.createTempShuffleBlock()
        /*4*/循环,将内存中的数据写入磁盘
            while (inMemoryIterator.hasNext)
        /*5*/如果达到批处理大小10000会刷写磁盘
            if (objectsWritten == serializerBatchSize) {
                flush()
            }
        /*6*/释放内存
            releaseMemory()
```

##### 8.shuffle读操作

```
    (1)Reader到底是什么?
        计算数据，在迭代数据的时候就会等同于计算这条数据,读取这条数据
        override def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {
        //getReader.read()
        SparkEnv.get.shuffleManager.getReader(
              dep.shuffleHandle, split.index, split.index + 1, context, metrics).read()
        def read(): Iterator[Product2[K, C]]抽象

    (2)拉取数据时最大了可以拉取到48m的数据读
        SparkEnv.get.conf.get(config.REDUCER_MAX_SIZE_IN_FLIGHT) * 1024 * 1024,
        .createWithDefaultString("48m")
    (3)得到写入磁盘的磁盘路径
    (4)把数据往排序器里边放
        sorter.insertAll(records)
```

##### 9.shuffle其他

```
    (1)
    预聚合使用的结构：PartitionedAppendOnlyMap&没有预聚合时使用的结构：PartitionedPairBuffer
        因为Map中有key,value;相同的key能够根据value进行更新,key只有一份，value能够聚合
        预聚合就是在落盘之前,在内存中把相同的数据做了一个聚合;
        map.changevalue(),有这个key就更新，没有这个key就新增
        没有预聚合的直接buffer.insert(),直接想里边加
    (2)
    判断聚合器是否声明
        val shouldCombine = aggregator.isDefined
        if (shouldCombine) {
        预先聚合使用的结构PartitionedAppendOnlyMap
        val mergeValue = aggregator.get.mergeValue
              val createCombiner = aggregator.get.createCombiner
              var kv: Product2[K, V] = null
              val update = (hadValue: Boolean, oldValue: C) => {
                if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)
              }
        map.changeValue((getPartition(kv._1), kv._1), update)
        Map是k,v键值对的形式，相同的k,把v做更新，整个数据结构中k只有一份，v可以做聚合(落盘前把相同的k做聚合)
        }else{
        //没有预先聚合采用的是PartitionedPairBuffer
        //insert往里边加就完了，没有预聚合的概念
        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])

        maybeSpillCollection(usingMap = false)
        if (maybeSpill(buffer, estimatedSize))

        }
    (3)
    处理器：
        case other: BaseShuffleHandle[
        对象：SortShuffleWriter
        new SortShuffleWriter(
                    条件

        }
    (4)
    Shuffle两个阶段
        ShuffleMapstage
        ResultStage
        现在方式：sortShuffle:有索引文件，有数据文件，这样文件数量就会少很多，性能就会得到提高，里边会有排序，排序会有一些临时的溢写磁盘的文件，最终形成最后的文件
        Bypass SortShuffle
        面试：如何能够提高shuffle的效率？
        其中有一项bypass SortShuffle
        Bypass运行机制触发条件
        1.不是聚合类shuffle算子
        2.shuffle reduce task数量小于spark.shuffle.sort.bypassMergeThreshold    参数的值，默认值200
        工作中200有点少，我们可以将它设置为400-500。
    (5)
    假如数据量很小
        (a,1)
        (b,1)
        (c,1)
        (a,1)
        预聚合，排序
        (a,2)
        (b,1)
        (c,1)
        数据量很小预聚合还行，排序其实没有必要，把这四条数据直接写入文件，没有顺序可以照样很快的读出来，使用hash值也能很快读出来，因为数据量很小
        (a,2)  => hash => 0
        (b,1)  => hash => 1
        (c,1)  => hash => 2
        Bypass忽略排序的目的就是为了不要在排序上耗费太多的性能；
        当然排序是好的，在某些情况下不用排序也挺好，也能实现同样的功能；
        所以需要判断：
        1.  只要有预聚合肯定不能忽略，没有预聚合可能bypass
        2.  下游的分区数量如果大于默认值200也不行
        Bypass不排序，某些情况通过hash也能快速定位；可以简单的理解为bypass就是通过优化后hash的这种操作
    (6)
    spark shuffle影响shuffle性能:
        (1)磁盘IO,
        (2)数据量,因为数据需要落盘
        优先级队列主要用来做堆排序，归并排序很大程度是用堆排序
```

##### 10.shuffle补充

```
shuffle写文件的压缩格式:
Shuffle写磁盘文件使用的压缩格式zstd(spark3.0.0版本)
private[spark] val MAP_STATUS_COMPRESSION_CODEC =
    .version("3.0.0")
    .createWithDefault("zstd")

    (1)使用序列化的shuffle    
    else if (SortShuffleManager.canUseSerializedShuffle(dependency))
    new SerializedShuffleHandle[K, V](
    点击canUseSerializedShuffle
        //当前序列化是否支持重定位操作，Kryo支持
    //shuffle支持序列化操作，那么文件数量就会减少，有利于压缩，需要把你挪动位置，所以有一个重定位操作
```

##### 11.SparkDriver端的任务调度

```
    (1)Spark提供了2个任务调度
    (2)FIFO[默认],Fair
        FIFO =>先进先出策略,谁先提交谁先执行,后边的任务等待前边的任务执行
        Fair =>模式支持在调度池中为任务进行分组,不同的调度池权重也不同,任务可以按照权重来决定执行顺序
    (3)Task => TaskSet(stage) => TaskSetManager(stage) => RootPool
                                 任务集管理器              任务池
        DAGScheduler
        TaskSet:
            <?>调度任务
            askScheduler.submitTasks(new TaskSet, tasks.toArray)把任务Task封装了任务集
            override def submitTasks(taskSet: TaskSet): Unit = {}
            val manager = createTaskSetManager(taskSet, maxTaskFailures)
        TaskSetManager:
            <?>增加任务集管理器
            schedulableBuilder.addTaskSetManager(manager
            抽象def addTaskSetManager(manager: Schedulable, properties: Properties): Unit
            FIFOSchedulableBuilder
        任务池:
            <?>rootPool.addSchedulable(manager)    
            class Pool{
              val schedulableQueue = new ConcurrentLinkedQueue[Schedulable]
              val schedulableNameToSchedulable = new ConcurrentHashMap[String, Schedulable]
            }
```

##### 12.spark任务执行

```
    (1)任务的发送
        <1>编码,序列化
        <2>根据Executor信息获取Executor的RPC连接引用
        <3>想Executor终端send消息:LaunchTask(事件)&序列化后的任务信息
    (2)任务的接收
        <1>解码，反序列化
        <2>线程池中获取线程,执行TaskRuner
        <3>TaskRunner的run方法执行Task.run
    (3)任务的执行
        <1>ShuffleMapTask执行runTaask方法
        <2>ResultTask执行runTask方法
        <3>每一个new出来的TaskRunner都是线程来完成的，每个线程都会执行TaskRunner,
            既然是线程来执行，那么TaskRunner一定会有run方法
            threadPool.execute(tr)
            new TaskRunner()
            TaskRunner的run方法中执行task.run(),因此每一个线程Thread都会执行一个Task

            3.任务的执行
            val res = task.run()
            runTask(context)
            在ShuffleMapTask中搜索runTask
            override def runTask(context: TaskContext): MapStatus = {}
            在ResultTask中搜素runTask
            override def runTask(context: TaskContext): U = {}
            }

        Executor => ThreadPool => Thread => Task
```

##### 四、Hadoop

##### 1.xsync脚本中常用linux命令

```
对于有软链接的目录，如：/var/lock - -> /run/lock/
cd 只是切换到 /var/lock 目录，pwd也只是查看到 /var/lock 地址
pwd -P 可以查看这个目录链接的地址 /run/lock/
cd -P /var/lock 可以直接切换到这个目录链接的地址 /run/lock/
mkdir -p 创建多级文件夹
```

##### 2.格式化集群

```
如果集群是第一次启动，需要在hadoop102节点格式化NameNode（
注意格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，
集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，
一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，
然后再进行格式化。）
```

##### 3.HDFS的写数据流程

```
(1)客户端通过DistributedFileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在
(2)NameNode返回是否可以上传
(3)客户端请求第一个Block上传到哪几个DataNode服务器上
(4)NameNode返回3个DataNode节点，分别为dn1,dn2,dn3
(5)客户端通过FSDataOutputStream模块请求dn1上传数据，dn1接到请求会继续调用dn2,然后dn2调用dn3,这个通信通道建立完成
(6)dn1,dn2,dn3逐级应答客户端
(7)客户端开始往dn1上传第一个Block(先从磁盘读取数据，放到一个本地内存缓存)，以Packet为单位，dn1收到一个Packet就会传给dn2,dn2传给dn3;dn1每传一个packet会放入一个应答队列等待应答
(8)当一个Block传输完成后，客户端再次请求NameNode上传第二个Block的服务器(重复执行3-7步)
```

##### 4.HDFS的读数据流程

```
(1)客户端通过DistributeFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址
(2)挑选一台DataNode(就近原则，然后随机)服务器，请求读取数据
(3)DataNode开始传输给客户端(从磁盘里边读取数据输入流，以Packet为单位做校验)
(4)客户端以Packet为单位接收，先本地缓存，然后写入目标文件
```

##### 5.inputformat做了两件事

```
(1)切片，在客户端
(2)把切片变成key，value值在maptask recordreader方法中：nextKeyValue(),getCurrentKey(),getCurrentValue()
```

##### 6.主键唯一键

```
：数据重写后能覆盖数据
sqoop导出时，只能识别hive中的一个路径，不能识别表
sqoop脚本执行时间比较长，一张表一张表的导，穿行导入
没有逻辑依赖，理论可以并行执行，提高任务的并行度，更改azkabam工作流程图
```

### 四、Kafka

##### 1.kafka压力测试

```
kafka生产者压力测试
出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。
kafka-consumer-perf-test.sh
bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
说明：
record-size是一条信息有多大，单位是字节。
num-records是总共发送多少条信息。
throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。

生产者影响压力测试非常重要的参数是batchsize:生产者往Kafka发送消息的批处理的大小,Batch默认16kb
bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=500
batch.size=500就是500bytes字节，打满默认带宽是15M，现在batchsize限制，不能打满，影响带宽，吞吐量；
想要提高吞吐量，提高batchsize的参数
测试结果说明：
100000 records sent, 95877.277085 records/sec (9.14 MB/sec), 187.68 ms avg latency, 424.00 ms max latency, 155 ms 50th, 411 ms 95th, 423 ms 99th, 424 ms 99.9th.
参数解析：本例中一共写入10w条消息，吞吐量为9.14 MB/sec，每次写入的平均延迟为187.68毫秒，最大的延迟为424.00毫秒。

读数据测试：
kafka-consumer-perf-test.sh
Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。
bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1
--zookeeper 指定zookeeper的链接信息
--topic 指定topic的名称
--fetch-size 指定每次fetch的数据的大小
--messages 总共要消费的消息个数
----threads 1 源码中没有使用这个线程参数，设置多少对测试结果没有影响	
batchsize影响消费者吞吐量的非常重要的参数，指的是一次最多拉取多少，批量越大吞吐量越高，批量越小吞吐量越小
测试结果说明：
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153
开始测试时间，测试结束数据，共消费数据9.5368MB，吞吐量2.0714MB/s，共消费100010条，平均每秒消费21722.4153条。

影响Kafka吞吐量的参数
生产者：batchsize,linger.ms
 //一批的等待时间
 props.put("linger.ms", 1); 
 等待时间长，
 等待时间很短，批还没达到就发出去，也会影响吞吐量
 消费者：fetchsize,真实参数，Kafka官网docs，
官网为：fatch.max.bytes

kafka发送消息流程：
Kafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，
以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从
RecordAccumulator中拉取消息发送到Kafka broker。
相关参数：
batch.size：只有数据积累到batch.size之后，sender才会发送数据。
linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。
RecordAccumulator默认32M
Batch默认16kb
同步发送：调完send方法有一个返回值，返回值调一个get（）方法，把当前处理的线程给阻塞了，后边的消息发送都得排队。


读出的带宽应该达到30M才对，但是测试结果为20M，尝试增加fatchsize大小，此次测试结果为30M【一个字节是8位】；
测试步骤：
(1)设置每台节点的带宽为100Mbps,小b单位是位，转换成字节大B，需要除以8，为12.5，除去损耗，差不多10M/s
(2)向集群发送数据的节点带宽更改为不受限，为了不影响测试结果
(3)发送消息的吞吐量为=10*分区数3/副本数2=3*10/2=15m/s为传输瓶颈
(4)消费消息的吞吐量为=10*3=30M/s，适当调整fetchsize的大小，避免影响吞吐量的测试
调整节点带宽：右键Hadoop105 => 设置 => 网络适配器 => 高级 => 调整带宽
```

```
搭建Kafka集群，集群的规模如何计算
向Kafka数据写入速度会有波动，峰值指的是最大的那个
峰值生产速度的单位为M/s
```

### 五、Redis

##### 1.配置参数

```
redis配置文件
1.
# 1k => 1000 bytes
# 1kb => 1024 bytes
# 1m => 1000000 bytes
# 1mb => 1024*1024 bytes
# 1g => 1000000000 bytes
# 1gb => 1024*1024*1024 bytes
2.
include引用
# include /path/to/local.conf
# include /path/to/other.conf
【一样的可以写在common.conf,不一样的写在1.conf和2.conf里边】
3.
bind谁能够引用我的redis
4.
保护模式
protected-mode no
# When protected mode is on and if:
#
# 1) The server is not binding explicitly to a set of addresses using the
#    "bind" directive.
# 2) No password is configured.
保护模式默认开启的：
server没有绑定一个地址、没有密码配置时保护模式会生效
解决：（1）加密码
	  (2)绑定一个地址，生产环境下一般不会绑定；
	  (3)保护模式取消
控制台查看当前的配置项
config get *
临时修改，不存盘，重启后，控制台改的临时项会被无效
config set protected-mode no

5.
端口号
port 6379
6.
默认连接超时
# Close the connection after a client is idle for N seconds (0 to disable)
timeout 0
idle闲置闲散的
一个客户端去连接服务器端，设置超时时间需要保证对面没有设置连接池；
如果设置连接池，那么redis需要跟连接池一直保持连通的，如果用户A，B,C使用完后不会把连接给断掉，会把连接还给连接池，连接池会一直放在这块与redis连接，如果设置连接超时，A，客户使用完后，连接池中对象还在，但是连接池与redis的连接已经断掉；
A
B    =>  连接池  =>  redis
C
timeout 客户端连接空闲的超时时间【连接什么都不干，空闲的挂在我这】，如果访问端使用了连接池，建议不要设置。
客户端      连接池      						  服务器
A           
B           Pool  [没有客户端连接时，     =>                          Server
		也会与服务器空闲连接]    
C

6.
多长时间检查一下连接
tcp-keepalive 300
7.
后台启动
daemonize yes
8.
启动进程的编号
pidfile /var/run/redis_6379.pid
9.
日志级别的设置
# Specify the server verbosity level.
# This can be one of:
# debug (a lot of information, useful for development/testing)
# verbose (many rarely useful info, but not a mess like the debug level)
# notice (moderately verbose, what you want in production probably)
# warning (only very important / critical messages are logged)

如果设置级别为INFO，则优先级高于等于INFO级别（如：INFO、WARN、 ERROR）的日志信息将可以被输出,小于该级别的如DEBUG将不会被输出。
10.
日志输出路径
# Specify the log file name. Also the empty string can be used to force
# Redis to log on the standard output. Note that if you use standard
# output for logging but daemonize, logs will be sent to /dev/null
logfile "/home/atguigu/redis.log"
11.
设置密码：配置加载程序时才会生效
# requirepass foobared
requirepass 123123
生效：
redis-server redis.conf
redis-cli
AUTH 123123

12.
连接最多的客户端数：
# maxclients 10000
如果达到了此限制，redis则会拒绝新的连接请求，并且向这些连接请求方发出“max number of clients reached”以作回应。
13.
最大内存数
# maxmemory <bytes>

假如卡了整个系统运转不顺，redis宕掉后，数据会丢失，缓存挂掉后，所有流量都会打到数据库上，数据库就会宕。
如果操作系统内存满了后，它会跟磁盘借内存，原来cpu会跟内存做交互，当内存满了后，就会卡，现在有一部分内存会用磁盘当内存叫做虚拟内存【windows】，交换分区swap【Linux】	
在虚拟机中敲top命令
KiB Swap:  4063228 total,  4063228 free,        0 used.  3395788 avail Mem 
其中0used说明内存从来没有不够用的时候
假如redis内存不够用，向磁盘借，redis是一个高速缓存，自己没有内存向磁盘中借，可能比MySQL还慢
14.
maxmemory-policy
# volatile-lru -> Evict using approximated LRU, only keys with an expire set.
# allkeys-lru -> Evict any key using approximated LRU.
# volatile-lfu -> Evict using approximated LFU, only keys with an expire set.
# allkeys-lfu -> Evict any key using approximated LFU.
# volatile-random -> Remove a random key having an expire set.
# allkeys-random -> Remove a random key, any key.
# volatile-ttl -> Remove the key with the nearest expire time (minor TTL)
# noeviction -> Don't evict anything, just return an error on write operations.
默认策略不淘汰，抛异常
# maxmemory-policy noeviction

volatile淘汰所有有过期时间的key，only keys with an expire set
allkeys淘汰所有的key
random随机
LRU:最近最少被使用，优先淘汰最近没有被使用的
LRU means Least Recently Used
LFU:很少被使用的，看使用频次
LFU means Least Frequently Used
ttl：最近要过期的the nearest expire time
其淘汰策略是不是把整个大数据进行淘汰，而是按照抽取样本中的数据进行淘汰策略，# maxmemory-samples 5

假如maxmemory最大设置90G，但是有100G数据往里边流
（1）抛异常
	noeviction：不进行移除。针对写操作，只是返回错误信息
 (2)内存淘汰策略
=============================
maxmemory  内存上线  防止redis把机器内存全部耗尽
如果达到上线可以启用内存淘汰策略
lru 推荐 较长时间未访问【计算较简单，就是按照时间】
lfu      访问频率低【复杂会消耗更多cpu进行计算，本来cpu还不够用】
random
ttl
allkey  所有的key
volatile 有失效时间的 推荐

 15.
 抽取样本策略，以供淘汰策略使用，样本设置的越大淘汰的越准	。
 # maxmemory-samples 5

```

##### 2.数组链表集合结构

```
时间复杂度？
linkedlist：底层是链表，增减值；通过指针查询；两个指针，头指针和尾指针，想要查到某个值，需要一个一个的倒；是双链表，单端双链表，一头写入一头出
arraylist:底层是数组，优势，通过下标直接查询出数据；往后边加值可能面临长度不够，需要扩容
Redis双端双链表，可以是先进后出：栈；可以是先进先出：队列
ps -ef | grep redis
lpush l1 v1 v2 v3 v4
l是双关，即代表是list，也代表是left从左边放
rpop l1
如果value的值全部被弹出后，那么key的值也会被回收掉
rpoplpush  <key1>  <key2>    从<key1>列表右边吐出一个值，插到<key2>列表左边。

lrange l1 0 2前三个，从左边开始取
lrange l1 0 -1 查询所有
lrange <key> <start> <stop>按照索引下标获得元素(从左到右)

set
set一对多，集合里边没有顺序，不重复【去重】
java
时间复杂度
o(1):通过下表来查找数据
o(N)：链表中查询某一个值，链表有多长，执行次数就可能有多少次
o(logN)：二叉树，数据翻一倍，查找次数增1；性能介于o(1)和o(N)之间的
o(N2)：一般查找不会有这种级别；一般为排序，冒泡排序
Redis的时间复杂度是o(1)查询速度是非常快的；其实其类似于Java里边的hashmap,,hash表里的key存，v没有值
hash表的key是按照数组，字符串取hash值计算出在数组的位置；查询准确来说是下标查询，通过hash计算出下标来，再查询；
zset有序的不重复

```

##### 3.数据备份

```
一、
服务器连接池中的连接坏掉：
1.服务器端重启过
2.网断过
3.服务器端维持空闲时间超时 timeout


List
双端双链表
【先进先出】队列，【先进后出】栈
场景：小型的消息队列
lpush 左进 
rpop 右出
lrange范围读取
llen长度
llen lindex lrem
lpush/rpush  <key>  <value1>  <value2>  <value3> 
rpoplpush  <key1>  <key2> 
lrange <key> <start> <stop
llen <key>
lindex <key> <index>
lrem <key> <n>  <value>

set
集合 无序不重复的hash集合
场景：一个key对应多个值 去重
可以指定查询某个值
sadd：加数
smembers：取全集
sismember：判断是否存在
sinter sunion sdiff 交并差

sadd <key>  <value1>  <value2>
smembers <key>
sismember <key>  <value>
sinter <key1> <key2> 
sunion <key1> <key2> 
sdiff <key1> <key2>  

hash
类似于hashmap 一个key对应多个键值对
场景1：通常可以存储一个复合的对象，每个字段需要单独维护
uesr_info 账户 头像 基本信息...任何一个信息的变化可单独修改

场景2：提供了一种2层的树结构
比如班级-- 学生--各种信息
	key:班号 field：学号 value：学生信息（json）
		id => Json
班级 => id => Json
		id => Json

hset(hmset等效)==> 扩展了set，把set的value值增加了
hget查某个field
hmget查多个field
hgetall 查所有的field
hincrby 对某一个filed做步长增长
注意：所有的涉及 incr incrby 的字段 set k1 v1 incr k1 v1是字符串加1会报错


hset <key>  <field>  <value> 
hget <key1>  <field>
hgetall <key> 
hincrby <key> <field>  <increment>

zset 
==> 扩展了set，为set集合中的每个value增加了一个score评分
根据评分进行排序
有序集合
场景：需要排序的
zadd
zrange 按下标截取范围 正序
zrevrange 按下标 倒叙
zrangebyscore 按评分截取 正序
zrevrangebyscore 按评分截取 倒叙
zincrby 增加评分

zadd  <key> <score1> <value1>  <score2> <value2>
zrange <key>  <start> <stop>11  [WITHSCORES] 
zrevrange <key>  <start> <stop>  [WITHSCORES] 
zrangebyscore key min max [withscores] [limit offset count]
zrevrangebyscore key max min [withscores] [limit offset count] 
zincrby <key> <increment> <value>

```

##### 4.高级部分

```
redis的使用情况
1.没有给你现成的服务
	需要你自己搭建
		1：买阿里云的redis服务【在windows装一个客户端】，不通过远程 		安装配置 阿里云提供了界面操作
		2：租阿里云的普通服务（ECS）需要自己安装redis,包括搭建集群
		3：自己的服务器 自己的机房 需要自己搭建集群
2.有现成、蹭业务端的redis集群
RDB [Redis DataBase]
一、持久化【备份】
内存不能断电，进程不能关闭，否则数据丢失
在某些情况下会触发存盘
RDB：Snapshot快照
redis fork一个子进程进行存盘
进程 ps -ef | 能看见
因为redis是一个单线程
全量备份，备份频率不能太快
rdb
全量 ==> 备份工作量大 ==> 不能频繁备份 ==> 备份周期长 ==> 有数据丢失风险

进程之间内存独立，备份进程需要拥有数据
线程与线程之间在一个进程，能够共享
进程与进程之间不使用同一个内存，资源不共享
总共内存35，原进程20，再新增加一个进程，则共需要40
Copy on write写时复制技术，
读取父进程内存，不需要内存备份，翻倍，导致内尺不够用

Snapshot
dump.rdb启动时加载，
模拟宕机
kill 有条不紊的关闭数据，会给你存盘
kill -9 粗暴杀死，可能数据会丢失

kill和shuttdown有条不稳的关闭数据，会给你存盘时间
kill 杀死客户端，数据不会丢失
flushall 数据全部清除后存盘，数据丢失
shutdown 不会丢失数据，会把文件存盘
kill -9 杀死客户端，有可能会丢失
killall -9 redis-server其实就是宕机，意外关闭【粗暴关闭】
触发存盘时间：30秒&达到10条
stop-writes-on-bgsave-error
当备份无法写入磁盘时，直接关掉redis的写操作
bgsave=> folk
主线程save,主线程强制备份
会导致阻塞当前业务因为redis为单线程
压缩文件
rdbcompression
压缩主要提高IO，但会占用cpu
如果cpu不够可以不设置压缩
redis对cpu压力比较小，redis平时主要一个cpu1进程，
备份需要cpu2，cpu2不一直工作，可以用cpu2压缩
检查完整性
rdbchecksum yes
redis优势：
节省磁盘空间
恢复速度快
劣势：
1.Redis在fork使用了写时拷贝技术，但是数据庞大时比较消耗性能
2.一定间隔做一次备份，如果redis意外宕掉的话，就会丢失最后一次快照的所有修改。

config get *
config set save ""[轻易不要这样]
==========================
AOF Append Only File[增量]
在原始文件上持续的追加
劣势：
aof
set zhangsan 10000
incryby zhangsan 100
incryby zhangsan 200
decryby zhangsan 100
decryby zhangsan 100
记录整个过程
RDB
set zhangsan 10100
记录结果

既有AOF又有RDB情况下，
 1.以AOF为准
 2.启动刚刚开始，会产生一个0字节的AOF文件
 此时shutdown后会造成rdb数据变成0，造成数据丢失
开启AOF,然后shutdown，数据彻底没了

如果你想在已经使用一段时间的时候 开启aof
正确的姿势
现在命令行执行config set appendonly yes
可以直接产生有大小的aof文件，再进行修改配置文件
config get append*
config set appendonly yes就会生成跟rdb等大的文件
此时shutdown

【aof存储的是命令，rdb存储的是数据】
新生成的aof文件
全量+增量
上半部分rdb数据
下半部分aof命令

RBD最怕flushall内存会丢，磁盘也会清空
加上aof
执行flushall
appendonly.aof即可以读，又可以编译的文件，直接把flushall命令删除
如果误操作：
	如遇到AOF文件损坏，通过/usr/local/bin/redis-check-aof --fix appendonly.aof进行恢复
aof真的是写一条记一吗？

			实时进入buffer             fsync(flush)
redis内存  ================>  buffer  ============>    aof文件

fsync(flush)
1.always(每条)【会直线影响性能】
2.everysec(以秒为单位，周期性备份)
3.no(由OS决定，分钟级)【分钟级不靠谱，跟rdb差不多】

buffer缓冲区是内存

aof短板==>越积越多，恢复慢，影响性能
周期性把
set zhangsan 10000
incryby zhangsan 100
incryby zhangsan 200
decryby zhangsan 100
decryby zhangsan 100
压缩成
set zhangsan 10100

周期性的把dump.rdb覆盖掉appendonly.aof
某一个时间点的全量+增量
最小阈值需要压缩之前的
最小触发大小：auto-aof-rewrite-min-size 64mb
触发的比例：auto-aof-rewrite-percentage-100
生产环境建议设置到G级别

```

##### 5.主从复制

```
保证服务的高可用
日志地址
保存文件地址
aof
不一样

include /home/bigdata/redis-0921/redis.conf
port 6379
dbfilename dump6379.rdb
pidfile /var/run/redis_6379.pid进程号
logfile /home/bigdata/redis-rep0921/redis6379.log

:%s/6379/6380

redis-cli -p 6381
info replication查看主从复制信息
slaveof hadoop102/192.168.118.102/127.0.0.1 6379

默认情况主机挂了，从机死等
redis-cli -p 6380 shutdown
REPLICAOF host port看官网
【一般每4G内存配置一个cpu】
大哥冒烟
slaveof no one

哨兵模式
vim sentinel.conf
sentinel monitor mymaster 192.168.118.102/127.0.0.1 6379 1
一个哨兵认为大哥挂了就挂了
ll /ser/local/bin查看命令
replica-priority 10
优先级值越小，它的优先级越大，越容易被选作为皇帝
默认：replica-priority 100

127.0.0.1本机地址
要使用真实的物理地址：192.168.118.102
killall redis-server

```

##### 6.集群

```
killall redis-server
传输
scp -r /opt/module hadoop103:/opt/module

rsync -av /opt/module hadoop103:/opt/module
-a	归档拷贝
-v	显示复制过程
rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。
:set nu	显示行号
:set nonu	关闭行号
:%s/old/new/g	替换内容   /g 替换匹配到的所有内容
/要查找的词	n 查找下一个，N 往上查找
1+shift+g	移动到页头，数字
dd删除光标当前行
dw删除一个词
cluster nodes 查看节点状态
killall redis-server 杀掉进程
redis-cli -c

mkdir redis-cluster0921
cp ./redis-redp0921/redis.conf ./
cp ./redis-redp0921/redis6379.conf ./
clueter-enabled yes 打开集群模式
cluster-config-file nodes-6379.conf
cluster-node-timeout 15000会做主从给切换，不用哨兵
%s/6379/6381


redis-cli --cluster create --cluster-replicas 1 192.168.118.102:6379 192.168.118.102:6380 192.168.118.102:6381 192.168.118.102:6389 192.168.118.102:6390 192.168.118.102:6391

必须写地址192.168.118.102， 不能写127.0.0.1 不能写localhost\hadoop102

故障恢复：
jedis.close()退还连接池[自己从池子里得到连接]
jedisCluster.close()把整个连接池关闭【图书馆关闭】

事务：
事务：把多条命令放在一起同时执行同时失败，不被打断

redis
redis 是nosql，nosql不支持传统的事务acid
1.	ACID：
2.	Atomicity原子性
3.	Consistency一致性
4.	Isolation隔离性
5.	Durability耐久性

redis事务 解决多条命令整体性执行的问题
mysql如果3条1条失败，支持回滚
redis执行3条1条失败，不支持回滚
redis集群不支持事务：
set k1 v1 k2 v2 k3 v3，不能保证k1 k2 k3 在一个槽里
不能保证3个key在一个机器上执行，不能保证同时执行
lua脚本：
lua脚本：更复杂，可以有判断 循环 类似shell脚本包含多个命令的脚本
redis可以视为一个整体，不会被其他命令打断 但是不支持回滚
redis集群不支持lua脚本，不能保证shell里边的多条命令在同一个机器执行，因为可能分散在不同的机器上，无法保证事务。
redis在执行这个大的脚本，不被打断，但是不支持回滚


不能使用 set k1 v1 k2 v2 k3 v3 不能保证k1 k2 k3 在一个槽里
多键操作，可以通过{}变通的解决
key中设置{}会从{}中的值取hash
mset k1{user} v1 k2{user} v2 k3{user} v3
比如{user:info}:0101,{user:info}:0102
【以{}内的值做hash计算，这些key可以分布到一台机器上，可以保证原子性】
【redis单线程保证单条命令的原子性，如果分布在不同的机器上不能保证原子性，可能并行，并发】
【原子性：只能等到这条命令执行完后，才能执行线下一条命令】

【ok】 All 16384 slots covered
16384个槽
为什么要搞这么多槽呢？
16384槽是固定的，算法%16384是固定的，如果增加一台机器，只需要部分槽内数据迁移
直接设置三大槽0 1 2 ==>数据迁移麻烦

为什么会做成三主三从？
redis-cli --cluster create --cluster-replicas 1 192.168.118.102:6379 192.168.118.102:6380 192.168.118.102:6381 192.168.118.102:6389 192.168.118.102:6390 192.168.118.102:6391
因为参数设置为1，表示一个master主机有一个slave从机

```

##### 7.高级复习

```
RDB内存快照  ==> 把内存的全量数据记录在硬盘上
===》比较重的操作 ==》不能频繁做 ==》操作时间拉长===》丢失数风险比较大
--------------------------------------------------------
kill 不会丢失 等同于shutdown 进程结束前存盘
kill-9 等同于宕机 会丢失，在两次存盘间隙宕机，会丢失数据
flushall  会丢失全部内存数据和磁盘备份【先清空内存数据，再做磁盘存盘】
shutdown 不会丢失 进程结束前会存盘
-----------------------------------------------------------
fork中redis启用了写时拷贝技术  redis后台备份不会阻塞主线程，会单独启动一个子进程，而且两个进程共享一段内存
备份频率 可以设置多个 多长时间内改变了多少key
save

优点 跟aof比较 数据没有冗余，快照会速度快
缺点 操作重 备份间隙长，会丢失数据

AOF
操作日志，4.0以后，内存快照+操作日志 某点的内存快照+从该点至今的操作日志
appenonly yes 默认是不开启的
没有历史数据的情况下，直接在redis.conf修改，然后开启
有历史数据 在命令行中 执行config set appendonly yes【生成有大小的appengonly.aof】 开启 同时也要把redis.conf中的开启
用文本编辑器可以编辑查看appengonly.aof文件，用来修改误操作
如果aof损坏 ，redis-check-aof --fix 来尝试修复

aof日积月累 可能会非常的冗余而且恢复速度慢
aof提供了 rewrite机制，能够周期性的把当前时点的RDB变为aof
rewrite周期取决于设定的aof增长幅度 还要看 备份起始点 一般设到G级别
aof的写入频率 不建议设置每条写入 可以设置everysecond 每秒写入
优点：备份频率高 不容易丢失数据 可以恢复误操作
缺点：有冗余 即使是有rewrite，官方说的高频写入会造成aof文件损坏

两个备份机制
1.开RDB 单开
2.AOF+RDB

Jeids 主从复制
早期 读写分离 （第三方中间件） 主写 从读
目前 主读写 从机 standby待命
主从复制核心命令 slaveof slaveof no one
slaveof(replicaof) host port 用命令行执行 也要在redis.conf中加入该配置 避免重启失效【从机执行】

一旦确定主从关系 
主机会把历史数据打包成rdb发送给从机 从机是可以继承历史数据的
如果主机宕机
从机原地待命等待主机恢复
或者 升级为主机 slaveof no one 反客为主

主从结构、
	星型 主机太累
	链型 容易中间断开
	树型 最稳 贵
哨兵
自动的监控主机状态
一台主机宕机 可以协调从机上位
1.如何确定主机宕机
	1.主观下线 某个哨兵认为主机挂了
	2.客观下线 通知其他哨兵确认主机状态 投票 达到票数的阈值的时候 确认要做切换
2.如何切换
	1.选太子 （优先级小的（设定），偏移量大的（看数据），runid小的（本质	随机））
	2.其他的从机改换主机
	3.如果旧机器恢复的话 改投新主机

哨兵处理 负责监控切换主从还有 引导客户端访问主机
程序客户端要使用哨兵池（JedisSentinelPool）,来连接，哨兵会根据自身了解的信息给与客户端正确的主机地址	

Redis启动，查看进程：
不能使用jps，需使用ps -ef | grep redis
Redis.conf配置文件放置在home路径下

```

### 六、Hive

##### 1.hive的基本操作

```
1.启动hive
bin/hive
2.查看数据库
show databases;
3.打开默认数据库
use default;
4.显示default数据库中的表
show tables;
5.创建一张表：
create table student(id int,name string);
6.查看表的结构
desc student;
7.向表中插入数据
insert into student values(1000,"ss");
8.查看表中的数据
select * from student;
9.推出hive
quit;
```

##### 2.将本地文件导入hive的案列

```
将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。
实操：
1.数据准备：
cd opt/module
mkdir datas
cd /opt/module/datas
touch student.txt
vi student.txt
1001	zhangsan
1002	lisi
1003	zhaoliu
【注意以tab间隔】
2.hive实际操作
(1)启动hive：bin/hive
(2)显示数据库：show databases;
(3)使用数据库：use default;
(4)显示default数据库中的表：show tables;
(5)删除已创建的student表drop table student;
(6)创建student表，并声明文件分隔符'\t'
create table student(id int,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
(7)加载文件到student数据库表中
load data local inpath '/opt/module/datas/student.txt' into table student;
(8)hive 查询结果
select * from student;
```

##### 3.Hive常用交互命令

```
bin/hive -f /opt/module/datas/hivef.sql > /opt/module/datas/hive_result.txt
```

### 七、Flink源码

#### 1.提交

```
=================================================================
CliFrontend程序客户端入口
	参数解析
	封装CommandLine：三个，依次添加
	配置的封装
	集群执行器:
		执行用户代码： execute()
		生成StreamGraph
	Executor：生成JobGraph
	集群描述器：上传jar包、配置， 封装提交给yarn的命令
	yarnclient提交应用
	
YarnJobClusterEntryPoint：AM执行的入口类
	1、Dispatcher的创建和启动
	2、ResourceManager的创建、启动：里面有一个 slotmanager（真正管理资源的、向yarn申请资源）
	3、Dispatcher启动JobMaster：生成ExecutionGraph（里面有一个slotpool，真正去发送请求的）
	4、slotpool向slotmanager申请资源， slotmanager向yarn申请资源（启动新节点）
	
YarnTaskExecutorRunner：Yarn模式下的TaskManager的入口类
	1、启动 TaskExecutor
	2、向ResourceManager注册slot
	3、ResourceManager分配slot
	4、TaskExecutor接收到分配的指令，提供offset给JobMaster（slotpool）
	5、JobMaster提交任务submitTask()给TaskExecutor去执行
=============================================================
```

```
per-job一个job一个集群，事先不需要启动集群，只有在提交的时候yarn才会帮助我们启动一个集群
```

```
===========================================================
yarn-per-job模式的提交流程
(1)提交命令行命令
	{bin/flink run -t }
(2)CliFrontend程序客户端入口
	{

	(1)
	参数的解析，选择了命令行客户端
	解析参数，-t,-c,run}解析除了flink外的其他参数,解析后对命令行提供了接口，叫做commandline,里边添加了三个GenericCli，
	flinkYarnSessionCLI，DefaultCLI，最后挨个遍历，挨个判断，优先判断GenericCli,就是指定的-t;如果是YarnSessionCli是基于yarn的
	有没有应用ID，yarn能不能找到这个应用，如果有的话就用yarnsessionCli,命令行使用-m的形式，-t应该是generic；选择了这种接口之后，最终
	封装了一些配置，最终调用invoke用户的代码；
	最后一行执行execute，根据代码生成流图；
	(2)
	集群执行器YarnJobClusterExecutor,生成作业图
	(3)
	集群的描述器YarnClusterDescriptor
		上传jar包，配置，作业图到HDFS
		封装ApplictionMaster的各种环境信息和配置
		yarnClient{执行提交submitApplication到ResourceManager}
	(4)
	启动APPMaster
		创建启动转发器Dispatcher
		启动ResourceManager
		转发器启动JobMaster,JobMaster里边有插槽池，调度器DefaultScheduler，把作业图转换成执行图
		JobMaster里边的槽池slotpool向resourceManager里边的槽管理器soltManager申请，注册请求slot
		槽的管理器sotManager向yarn申请新的worker，选择某些节点去启动容器
	(5)
	RM启动容器，启动TaskManager
		里边的入口类YarnTaskExecutorRunner
		底层真正的名字是runTaskManager启动TaskExecutor，TaskExecutor里边有一些槽，向ResourceManager里边的槽管理器slotManager注册
		slotManager分配槽，TaskManager提供slot给JobMaster里边的槽池slotPool
	(6)
	JobMaster提交执行submitTask()给TaskExecutor里边的槽

=================================================================
```

###### <1>TaskExecutorRunner

```
TaskManager运行的进程:TaskExecutorRunner,以前叫做TaskManagerRunner
```

###### <2>三大进程:

```
	客户端：CliFrontend
	JobManager：YarnJobClusterEntrypoint
	TaskManager:TaskExecutorRunner
```

###### <3>脚本入口

```
脚本入口:CliFrontend
```

```
这里依次添加了 Generic、Yarn和Default三种命令行客户端（后面根据isActive()按顺序选择）：
```

```
命令行会封装成3种类型commandline，
命令行入口：CliFrontend
commandline => 在命令行输入的bin/flink run -t yarn-per-job /opt/module/flink-1.12/examples/....jar --port 9999
flink对输入的命令做了一个封装，封装之后叫做command line
	commandline.add(第一个) GenericCli    指定的-t
	commandline.add(第二个)	flinkYarnSessionCLI  yarn有没有应用id,或者命令行有没有指定应用id,其实就是yarn能不能找到这个应用，
		如果有就用yarnsessionCli,这个命令行使用-m的形式，命令行中的-t就是GenericCli的形式
	commandline.add(第三个)	DefaultCLI    Stand alone

	为什么要讲究顺序：
	因为不管是per-job、yarn session、Stand alone，提交时候都是flinkrun,怎样区分是独立集群还是yarn的per-job还是yarn的session呢
	我们如果不指定-t可以是Standlone提交，也以是yarn提交；按照顺序一次添加
	封装配置，最后invoke用户的代码，最后执行Executor,生成流图
	YarnJobClusterExecutor集群执行器:
		生成作业图
	集群描述器上传用户代码，jar包，配置文件，作业图传到HDFS，
	封装ApplictionMaster的各种环境信息和配置
	yarnClient执行提交，给YarnResourceManager
	启动容器
```

```
	jar包的配置
	类的配置
	类路径
	并行度
```



###### <4>yarnClient跟yarn做交互

```
集群描述器:创建了YarnClient ,包含了一些yarn，flink的配置和环境信息
集群特有资源配置:JobManager内存，TaskManager内存，每个Tm的slot数
部署前检查:jar包路径，conf路径，yarn最大核数
检查指定的yarn队列是否存在
检查yarn是否有足够的资源
```

###### <5>上传文件到HDFS

```
启动APPMaster
初始化文件系统 Initialize the file systems,即HDFS,将jar，conf配置等写到HDFS上边
把job相关的包放到这里
yarn应用的文件上传器:FS,对应的HDFS路径
高可用配置:重试次数，默认2次
上传:
上传flink的配置文件-flink-conf.yaml  fileUploader
fileUploader.close(),一共上传了，用户jar包，上传flink的jar包，依赖，上传了flink的配置
```

```
创建Map,用来存储AM的环境变量和类路径
	Map<String,String> appMasterEnv = new HashMap<>()
	appMasterEnv.put(Flink_DISK_JAR)FLINK核心包	
将之前设置的环境信息设置到容器中
创建完容器之后
	前面上传了很多，环境配置等，终于可以提交应用了
	yarnClient.submitApplication(appContext)
	之后启动AM容器，执行包里边的核心组件的代码，在里边起集群
	然后通过rmClient做了一个提交
	rmClient.submitApplication(request)
```

###### <6>AM里边怎样来启动？

```
启动APPMaster成功后，开始入口类YarnJobClusterEntrypoint
执行程序的入口:
	clusterEntrypoint.runClusterEntrypoint(yarnJobClusterEntrypoint)
	runCluster()
		{
			初始化服务：
				(1)远程的Rpc通信
				(2)组件在不同的节点，之间通信调用
				(3)交互，里边有地址，端口
			创建和启动JobManager里边的组件:dispatcher,resourceManager,JobMaster
			dispatcher,resourceManager里边都有高可用，返回leader
			dispatcher.start()做了什么事情？
				(1)接收用户的作业
				(2)启动JobMaster	
					启动JobMaster服务
					重置和启动调度器	
			resourceManager启动？
				初始化做了那些事情？
					创建yarn的ResourceManager的客户端，并且初始化和启动
					创建yarn的小弟NodeManager的客户端，并且初始化的启动
				通过选举服务启动ResourceManager
				启动心跳服务{startHeartServices()}：跟TaskManager,JobMaster
				启动slotManager{slotManager}:resourceManager内部真正管理槽资源的是SlotManager
		}	

		//JobManager进程里边包含的组件
			(1)dispatcher
			(2)flink内部管理资源的Resourcemanager
			(3)JobMaster
		//RPC是基于Akka的组件通信，它如果条用start，那么对应的对方会执行onStart()
		//AM里边整体的进程是JobManager,体现的进程名是YarnJobClusterEntrypoint,	这个进程里边有三个组件dispatcher,resourceManager,JobMaster，
			JobMaster是由dispatcher转发器来创建的
```

###### <7>我们看一下JobMaster启动完做了什么事情

```
	(1)我们看一下JobMaster启动完做了什么事情
		启动JobMaster服务{startJobMasterService}
			<1>启动心跳服务，和taskManager,resourceManager
			心跳{startHeartServices}和taskManager,resourceManager;
				身为老大跟小弟taskManager有个交互，把任务分给他，定时汇报一下进展
				resourceManager跟它交互，资源怎么样，需不需要申请；、
				resourceManager向一个管家一样
			<2>启动slotPool
			JobMaster内部有一个槽池slotPool
			resourceManager内部是SlotManager
			<3>slotPool与resourceManager开始建立连接，slotPool开始请求资源
			JobMaster里边的slotPool开始请求插槽
			JobMaster里边有一个内部类，ResourceManagerLeaderListener，ResourceManager监听器
				创建注册对象：JobMaster跟ResourceManager连接，向ResourceManager注册
				开始注册：注册成功之后，调用onRegistrationSucess()
				slotPool连接到resourceManager请求资源{slotPool.connectToResourceManager}
			<4>RPC远程调用
				JobMaster命令ResourceManager给他提供资源，涉及到RPC远程调用
			<5>requestSlot找实现
				resourceMnager内部的SlotManager向Yarn的resourceManager申请资源
					{slotManager.registerSlotRequest(requestSlot)}

		重置和启动调度器
```

###### <8>JobManager三大组件已经启动完成，接下来启动小弟TaskManager。

```
Yarn容器中运行TaskExecutor的入口点YarnTaskExecutorRunner
	runTaskManagerSecurely()
		TaskManagerRunner.runClusterEntrypoint

		TaskExecutor.start()
			{	
				//通过rpc服务，启动TaskExecutor,找它的onStart()方法
				rpcServer.start()
			}
		源码里边真正的小弟是TaskExecutor,TaskManager是我们通用的来讲的

		TaskExecutor启动后做了哪些事情？
			开始注册，向ResourceManager注册
			开始注册调用的是invokeRegistration方法
		接下来的是ResourceManager接到这些请求后干了一些什么事情？
			TaskExecutor启动之后向ResourceManager注册slot，来到slotManager边
			registerSlot{
				(1)删除老的slot
				(2)创建和注册TaskManagerSlot:注册新过来的slot {createAndRegisterTaskManagerSlot}
				(3)


			}
		//TaskManager中的slotPool，向ResourceManager中的slotManager申请，经过申请资源，启动容器，TaskExecutor向
			slotManager注册；
			分配slot:
				slotPool中等待的slot为空，不需要分配；否则分配挂起的请求给槽，被分配的槽标记为used,使用中
				slotPool申请了3个slot，但是有一个申请到了，剩下的还在等待，这种情况叫做挂起；马上到你了还没到你，还在排队
				如果slotPool中等待(挂起)的槽为空，那么说明不需要被分配，如果不为空，那么说明有些槽还等着被分配，把确定要分配的
				从等待的列表里边删掉，获取到等待的请求，然后分配给你槽

			分配好槽后，通知TM把槽提供给JM
				分配好槽后，小弟告诉老大,之前管家(resourceManager)告诉我这几个slot提供给谁谁使用了，老大把Task发送过去

			{allocateSlot}
			根据RM的命令分配TaskExecutor自己的slot
				resourceManager告诉TaskExecutor已经分配好槽，比如告诉TaskExecutor需要2slot，那么TaskExecutor就会告诉手底下人安排安排

			//向JobManager提供slot
			offerSlotToJobManager(jobid)

			老大开始分发任务
```

#### 2.Flink通信过程



```
===================================================================
	RPC动图交互关系
		JobMaster和TaskExecutor进行交互，比如提交任务的时候，
		终端在构造的时候由一个叫RPCService,
		RPCService会调用startServer(),这里边启动了RPCserver，同时也创建了RPCActor
		终端需要明确的触发启动，去启动终端
		RpcServer调用start()方法，启动RPCEndpoint
		会转发到代理InvokeHandler,代理把自己启动起来
		两者状态启动起来了，只有启动起来了才能互相的交流
		RpcService通过connect()到对方的RPCServer得到对方的代理客户端RpcGateway
		RpcService通过网关RpcGateway与对方交流
		(2)
		通过网关RpcGateway远程调用对端的方法，连到对方的server
		(3)
		对方的server把请求转发给代理InvokeHandler
		(4)
		代理里边invoke()方法，如果是rpc请求invokeRpc()
		判断发送地址是否为本地，本地不需要序列化；不是本地调用远程；
		无返回值使用tell
		有返回值使用ask,类型是CompletableFuture?是，不阻塞，返回Future，否，阻塞，等待返回值
===============================================
```



###### <1>三个进程里边的相互通信？

```
	CliFrontend
	YarnJobClusterEntrypoint
	TaskExecutorRunner

	组件之间互相通知:Akka
	数据的传递使用的是Netty
	0.9版本开始使用Akka,所有的远程调用都为异步的

	RPC和Akka和Netty
	RPC是一个统称远程方法调用
	Akka某一种实现
	Netty某一种实现
```

###### <2>Akka核心ActorSystem和Actor

```
	Actor模型
	Actor{演员}
	每一个actor是一个单一的线程，它不断的从邮箱拉取数据，他有状态启动状态和停止等状态
	Actor系统：一个actor系统包含所有存活的actors，它里边有线程池，是通过线程池来管理的
	Actor系统可以在一台节点上共存，一台节点上可以有多个actor系统
	可以接收本地，也可以接收远程
	actor有继承机制，监督，找一个人干活，你监督
	actor系统，有共存，继承，单一线程

Akka核心ActorSystem和Actor
	本地：本地Actor路径为 akka://sys/user/helloActor
	远程：远程Actor路径为 akka.tcp://sys@l27.0.0.1:2020/user/remoteActor
	若需要与远端Actor通信，路径中必须提供ip:port。
Akka的两种通信方式:
	两种核心都是异步的
	tell:不需要回应
	ask:可以通过异步获取回应，通过futher的方式获取
```

###### <3>FlinkRpc的通信过程

```
Flink自己封装的组件{用于通信}
		(1)RpcGateway网关=>定义通信的行为
			JobManager和TaskManager并不能直接通信，他们是通过GateWay的方式，老大向小弟发送消息，先发送给网关，再由网关
			向小弟发送，调用网关固定的方法，让小弟执行
			网关里边有一些动作、方法，这些动作、方法就是告诉对方要执行的事情
			(每一台电脑也有一个网关，从互联网接收消息，先进入网关，之后再转发到本台电脑)
		(2)
		JobMaster
		resourceManager
		dispatcher
		TaskExecutor
		四个组件都实现了对应的网关接口GateWay

		(2)RPCEndpoint通信终端，提供RPC服务组件的生命周期管理
			RPCEndpoint每一个路径都对应一个Actor,其实现RPCGateway接口
			再flink设计中，对于同一个Endpoint,所有的调用都运行在主线程，因此不会有并发问题
		(3)RPCServer和RPCService

			RpcService 和 RpcServer是RpcEndPoint的成员变量
			RpcService根据提供的RpcEndpoint来启动和停止RpcServer（Actor）
			RpcService 和 RpcServer是RpcEndPoint的成员变量。
			最终使用动态代理将所有的消息转发到InvocationHandler
			Jm和TM的终端RPCEndpoint里边都有一个server，server是自身的代理，用来调用自己的方法
			JobManager要和TaskManager通信，首先由发起者service连接对方的server，server提供网关，service再调用网关，网关再去
			调用方法
			server：调用自己的网关，接收网关的请求
			终端的启动实际是由自身的网关(rpcServer)来启动

			JM自身启动service去startserver，start() => server.start() => tell
			server是自身的网关，自身的代理
			三种处理是RPCActor处理，在server里边，server是自身的网关，自身的代理
		(4)InvocationHandler代理做了什么事情
```

#### 3.flink任务调度机制？



```
=======================================================
Task调度：
	(1)流图
		在CliFontent客户端，根据代码里边的算子顺序，挨个添加到流图的结构里边去；并且把一些不是对数据做
		转换的算子，把他变成一些边；比如keyby变成了Hash边；流节点，流的边，并且把他们之间的前后出路串起来；
		keyby，union，connect算子没有对数据进行转换，不会生成流图的一个圈，其对应的是分区相关的
		map,sum，flatmap会对数据进行转换，OneInput
		区分map之类的转换算子(OneInput)
		keyby之类的分区算子(partition)
	(2)作业图
		在CliFontent客户端，
		<1>可以优化为操作链的就串在一起
		<2>流节点转换为作业图的顶点Vertex
		<3>把流边转换成作业图里边的作业边
		<4>生成某个节点的中间数据集
		<5>把数据集，边，节点前后顺序串联起来
	(3)执行图
		在JobMaster里，创建JobMaster时候，也创建了一个调度器，调度器里边把作业图转换为执行图
		<1>把作业图的顶点转换为执行作业图的顶点ExecutionJobVertex
		<2>执行作业顶点，根据并行度展开，展开成执行顶点ExecutionVertex,一个并行一个顶点
		<3>把中间数据集转换为中间结果，根据走向划分为多个中间结果分区
		<4>根据中间结果分区和执行顶点的关系去创建执行边
		<5>flink是从source到sink,spark是从后往前推
	(4)物理执行图
		有了执行图，task开始调度，调度器生成的执行图，开始调度
		<1>把Task根据执行图部署到相应的节点上去
		<2>通过RPC远程调用，TaskManager网关,最后真正执行的是TaskExecutor处理
		<3>底层是执行processElement,里边调用定义函数类，重写的方法对数据进行处理,最终通过采集器往下游发送（这是最底层的东西）
		<4>物理执行图没有明确的一个类，一个小的执行顶点对应一个Task，一个执行边对应一个InputGate(输入大门)，大门里边封装了输入
			通道
		<5>中间结果分区封装为结果子分区，大的中间结果对应的是结果分区
===================================================================
Task任务调度执行:
	JobManager里边有一个JobMaster,里边有一个默认调度器，这些组件都有一个ActorSystem,或者都是RPCEndpoint终端，
	<1>代码执行，main方法生成流图
	<2>在客户端的内部，经过操作链的优化，顶点，边中间数据集的转换生成作业图；上传到HDFS,AM启动时会加载这些东西
	<3>JobMaster启动时候，会创建默认调度器，将作业图转换为执行图
	<4>根据ExecutionGraph,使用PipelineRegion策略对执行图节点进行部署Deploy;通过RPC相关服务告诉TaskManager,TaskManager
		告诉TaskExector
	<5>TaskExecutor在自己的槽里边把任务启动，线程	new 一个Task,启动一个线程
===========================================================
```



###### <1>图

```

流图(StreamGraph):最开始解析代码，有什么算子，有什么先后顺序，生成的一张图；标识程序的拓扑结构
作业图(JobGraph):把流图操作链的优化，把关系为one-to-one，并且并行度相同的算子，串到一起
执行图(ExecutionGraph):在作业图的基础上，把它的并行化展现出来，并行度是几，就划几个点
物理执行图():根据作业图调度分发后，Task执行时候看起来的一个全局的效果，并没有某一个类来对应这个
```



```
流图(StreamGraph)：每个算子一个节点，中间连接线表示他们之间的关系，并行度；操作链的优化Operator Chain => 作业图
	三个细节:
		StreamNode名字改变
		多个IntermediateDataSet
		边的名字改了
```

###### <2>作业图转换为执行图:

```
作业图转换为执行图:
	并行化
	Vertext顶点的意思
	边的名字改了，作业图就是作业边，执行图就是执行边，流图就是流边
	StreamEdge
	JobEdge
	ExecutionEdge

	节点StreamNode => JobVertex(多个StreamNode节点chain一起生成一个Vertex)

	作业图:输入作业边，输出中间数据集；中间数据集的输入是作业顶点，输出是边；作业边，输入是中间结果集，输出是顶点

	中间结果集(IntermediaDataSet) => IntermediaResult => 变成中间结果分区(IntermediaResultPartition)

	执行图:
		ExecutionJobVertex执行作业点
		对于一个执行顶点来讲，输入是边，输出是中间结果分区
		中间结果的每个中间结果分区，输入是执行顶点，输出是执行边
		执行边(管道)，输入是中间结果分区，输出是执行顶点
```

###### <3>物理执行图：

```
		Task是调度分配的Task,对应算子经过操作链优化串在一起，即子任务串在一起的整体

		边 => 通道(channel) => InputGate (输入大门)
		中间结果，中间结果分区 => 结果子分区，整体叫做结果分区
```

###### <4>我们看顶点变化，边变化，中间结果变化

```
(1)流图：
	在执行用户代码execute的时候生成的
	 把每个算子一次遍历，该做成节点的做成节点，做成边的做成边，屡清上下游关系
```

```
(2)
作业图在客户端Client生成：
	(1)把可以串在一起的串在一起了，
	(2)转换，把流节点转换成作业图的顶点
	(3)把流边转换成作业的边
	(4)生成中间数据集
	
	可以串的边和不可以串的边，one-one并且并行度相同的可以chain，否则是不可以串的边
	把可串的粗边和不可串的分类
	边保存了输入和输出的信息，输入中间数据集，输出节点
```

```
(3)
执行图在JobManager中生成
	(1)顶点的转换:作业点=>执行作业点
	(2)作业边细化成执行边
	(3)中间数据集变成中间结果，中间结果细化成中间结果分区
	(4)forward模式就是one-to-one模式
```

```
(4)
物理执行图:
	Task的调度和执行
	JobMaster里边的是重置和启动调度器
	1.12新版本的调度策略是局部管道式调度，PipelinedRegion
	部署执行图的节点，部署的就是一个一个的小黄圈到每一个槽上边
	转换
		把中间结果，中间结果分区转换成结果分区，结果子分区
		把边转换成channel，inputGate
		把执行节点转换成Task
		通过网关调用提交Task,taskManagerGateway.submitTask()
	Task在slot上边执行的，一个Task有一个独立的线程
```

###### <5>调度

```
1.12;策略：局部流水线，局部管道；调度模式：Eager模式；调度器有哪几个类型：1个DefaultScheduler 
```

```
	调度器：DefaultScheduler 
	调度策略：
		(1)EagerScheduler:适用于流计算，同时调度所有的Task
		(2)LazyFromSourcesSchedulingStrategy:适用于批处理，
		(3)PipelinedRegionSchedulingStragy：以流水线的局部为粒度进行调度；从1.11开始加入，1.12开始使用；
			如果局部满足能跑，那么久先把局部放上去先跑起来；可以流处理，也可以批处理
	调度模式：
		流式
		Eager{饥渴}调度:适用于流计算，一次性申请所有的资源，如果资源不足，则作业启动失败；
		批式
		分阶段调度：适用于批处理;Lazy
```

#### 4.Flink内存管理:



```
	必须了解:
	内存段在 Flink 内部叫 MemorySegment，是 Flink 中最小的内存分配单元，默认大小32KB。
```



```
Flink内存模型:
		JobManager：
			堆内内存
			堆外内存
		TaskManager在1.10对内存做了重大的改动(重点)
			(1)Flink即使用了堆内存，也使用了堆外内存
			(2)框架内存:flink本身运行需要的内存
				包含堆内存
				堆外内存
				不记如slot资源
			(3)Task执行的内存
				堆内内存
				堆外内存
			(4)网络缓冲内存
				堆外内存
				数据传输需要网络缓冲，涉及到数据交换使用的网络缓冲区
			(5)直接内存
				框架堆外内存，Task堆外内存，网络缓冲内存统称直接内存
			(6)管理内存
				堆外内存
				flink管理的堆外内存
				用于管理排序，hash表，缓存中间结果，RocksDB State Backend的本地内存存储
				{RocksDB State Backend{状态后端}关注的两件事
					(1)本地状态存在哪里？本地内存存在RocksDB里边，RocksDB用到内存+磁盘
					(2)checkpoint存在哪里
				}
			(7)JVM特有内存
				堆外内存
				包含JVM元空间+JVM执行开销
			(8)Flink使用内存
				除了JVM元空间和执行开销，即抛开JVM本身的开销
			(9)进程内存
				Flink使用内存+JVM特有内存
```



###### <1>JVM内存管理的不足：

```
		1）Java 对象存储密度低
		2）Full GC 会极大地影响性能{大扫除}
		3）OOM 问题影响稳定性。
		4）缓存未命中问题
```

###### <2>内存管理器

```
	必须了解:
	内存段在 Flink 内部叫 MemorySegment，是 Flink 中最小的内存分配单元，默认大小32KB。
	可以看出这种序列化方式存储密度是相当紧凑的
	内存页是MemorySegment之上的数据访问视图

	Task算子之间在网络层面上传输数据，使用的是Buffer，申请和释放由Flink自行管理
	每个Task拥有自己的LocalBufferPool。

	RocksDB受内存管理器管的，RocksDB自己负责内存申请和释放

	内存管理器=>管理管理内存
=========================================

```

###### <3>内存数据结构

```
	内存数据结构
		HeapMemorySegment：用来分配堆上内存
		HybridMemorySegment：用来分配堆外内存和堆上内存，2017年以后的版本实际上只使用了HybridMemo`rySegment。
```

### 八、Sqoop使用

```
sqoop使用
sqoop的底层是mapreduce
hdfs://hadoop102:9820/test 当没有配置hadoophome时
bin/sqoop import \
--connect jdbc:mysql://hadoop102:3306/gmall \
--username root \
--password 123456 \
--table user_info \
--columns id,login_name \
--where "id>=10 and id<=30" \
--target-dir /test \
--delete-target-dir \
--fields-terminated-by '\t' \
--num-mappers 2 \
--split-by id

sqoop另外一种使用方式
bin/sqoop import \
--connect jdbc:mysql://hadoop102:3306/gmall \
--username root \
--password 123456 \
--query "select id,login_name from user_info where(id>=10 and id<=30) and \$CONDITIONS"
--target-dir /test \
--delete-target-dir \
--fields-terminated-by '\t' \
--num-mappers 2 \
--split-by id

$CONDITIONS 是占位符，在切片时候使用；$CONDITIONS在运算执行sql时替换(id>=10 and id<=20),(id>=20 and id<=30)
在不同的切片范围内进行运算

do_date=`date -d '-1 day' +%F`漂号的作用将一个shell命令的输出值赋值给前一个变量
echo $do_date
或者
do_date=$(date -d '-1 day' +%F)
echo $do_date
--null-string '\\N' \
--null-non-string '\\N'
hive中的null值需要\N表示
\在Java中有特殊含义需要转义

对文件建建索引
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
}

if [ -n "$2" ] ;then
    do_date=$2
else
    do_date=`date -d '-1 day' +%F`
fi

-n表示nozero
-z表示zero
```

###### hdfs_to_mysql.sh

```
#!/bin/bash

hive_db_name=gmall
mysql_db_name=gmall_report

export_data() {
/opt/module/sqoop/bin/sqoop export \
--connect "jdbc:mysql://hadoop102:3306/${mysql_db_name}?useUnicode=true&characterEncoding=utf-8"  \
--username root \
--password 123456 \
--table $1 \
--num-mappers 1 \
--export-dir /warehouse/$hive_db_name/ads/$1 \
--input-fields-terminated-by "\t" \
--update-mode allowinsert \
--update-key $2 \
--input-null-string '\\N'    \
--input-null-non-string '\\N'
}

```

### 九、算法

##### 1.冒泡排序

```
	public static void bubbleSort1(int[] data) {
		System.out.println("开始排序");
		int arrayLength = data.length;
		for (int i = 0; i < arrayLength - 1; i++) {
			boolean flag = false;
			for (int j = 0; j < arrayLength - 1 - i; j++) {
				if (data[j] > data[j + 1]) {
					int temp = data[j + 1];
					data[j + 1] = data[j];
					data[j] = temp;
					flag = true;
				}
			}
			System.out.println(java.util.Arrays.toString(data));
			if (!flag)
				break;
		}
	}
```

##### 2.二分查找

```
(1)首先确定数组的中间下标mid(left+right)/2
(2)然后让查找的数findVal与array[mid]比较
	<1>findval>array[mid],说明你要查找的数在mid的右边，因此需要递归向右查找
	<2>findval<array[mid],说明你要查找的数在mid的左边，因此需要递归向左查找
	<3>findval=array[mid],说明找到，就返回
(3)什么时候需要接收递归
	<1>找到就结束
	<2>递归完整个数组，仍没有找到，结束递归，left>right就退出
```

```
/**
 * 二分查找 时间复杂度O(log2n);空间复杂度O(1)
 */
 
def binarySearch(arr:Array[Int],left:Int,right:Int,findVal:Int): Int={
  if(left>right){//递归退出条件，找不到，返回-1
    -1
  }

  val midIndex = (left+right)/2

  if (findVal < arr(midIndex)){//向左递归查找
    binarySearch(arr,left,midIndex-1,findVal)
  }else if(findVal > arr(midIndex)){//向右递归查找
    binarySearch(arr,midIndex+1,right,findVal)
  }else{//查找到，返回下标
    midIndex
  }
}
```



```
def binarySearch(arr:Array[Int],left:Int,right:Int,findVal:Int): Int={
  if(left>right){//递归退出条件，找不到，返回-1
    -1；
  }

  val midIndex = (left+right)/2

  if (findVal < arr(midIndex)){//向左递归查找
    binarySearch(arr,left,midIndex-1,findVal)
  }else if(findVal > arr(midIndex)){//向右递归查找
    binarySearch(arr,midIndex+1,right,findVal)
  }else{//查找到，返回下标
  	ArrayList<Integer> resArr = new ArrayList<Integer>();
  	
  	//向左查找
  	int temp = midIndex -1;
  	while(true){
        if(temp<0 || arr[temp]!=findval){
			break;
		}
		resArr.add(temp);
		temp-=1
  	}
  	resArr.add(midIndex)
   
   //向右查找
   temp = midIndex + 1;
  	while(true){
        if(temp > arr.length-1 || arr[temp]!=findval){
			break;
		}
		resArr.add(temp);
		temp+=1
  	}
  	return resArr;
  }
}
```

##### 3.快排

```
计算机底层调一些线程的方法，通常是快速排序
(1)
选一个值pvoid,这个值是任何一个都可以,我们不妨选第一个值，后边定义两个指针，pvoid后边的叫做low,从最后往前叫做high,low指针不断的往后走，high指针不断的往前走，low指针的元素比pvoid小，high指针的元素比pvoid元素大，比如第一项比5小，接着往后走，7比5大，往前走；走到5这跟5一样就不用关了；
(2)
对于low来讲首次出现比5大，high来讲，首次出现比5小，出现以后，1跟9对调；
根据pvoid，把数组元素分为2半，一半比5小，一半比5大
(3)
到low为8，high为0时候再进行交换
(4)
low往后走，走过4，走到8位置；high往前走，走到4位置；此时high<low，结束查找
(5)
此时高位比5大，不管；high比5小，4跟5交换一下
(6)
交换之后，5的左边全是比5小的，5的右边全是比5大的
(7)
经过1轮数组被劈成两半，接下来把两个数组，分别定义一个low,一个high，独立运算一下；运算之后再劈成2半；1轮之后变成2，再来一轮变成4个，再一轮成8个；2的0，1，2，3，4...次方，很快的把要比的数摊的很细，是指数级的，增长的很快，排序也会很快；就像细胞分裂一样，效率很高；
```

![](D:\Code\My_Java_Code\总结\大数据笔记图片\批注 2021-05-09 104943.png)

```
public class QuickSort {
	private static void swap(int[] data, int i, int j) {
		int temp = data[i];
		data[i] = data[j];
		data[j] = temp;
	}

	private static void subSort(int[] data, int start, int end) {
		if (start < end) {
			int base = data[start];
			int low = start;
			int high = end + 1;
			while (true) {
				while (low < end && data[++low] - base <= 0)
					;
				while (high > start && data[--high] - base >= 0)
					;
				if (low < high) {
					swap(data, low, high);
				} else {
					break;
				}
			}
			swap(data, start, high);
			
			subSort(data, start, high - 1);//递归调用
			subSort(data, high + 1, end);
		}
	}
	public static void quickSort(int[] data){
		subSort(data,0,data.length-1);
	}
	
	
	public static void main(String[] args) {
		int[] data = { 9, -16, 30, 23, -30, -49, 25, 21, 30 };
		System.out.println("排序之前：\n" + java.util.Arrays.toString(data));
		quickSort(data);
		System.out.println("排序之后：\n" + java.util.Arrays.toString(data));
	}
}
```

```
例如：4,2,9,3,1,6,7
一开始以4为基准，经过上面的while，数组变为：4,2,1,3,9,6,7，4右边的数被分为两半，左边都比4小，右边都比4大，现在swap交换的是4与3，数组变为：3,2,1,4,9,6,7，此时4到达正确位置，但是左右两边还未排好，分别对左边[3.2.1]和右边[9.6.7]通过递归重复刚才的过程，实现排序
```



##### 4.归并排序

```
该算法采用经典的分治策略，把大的问题分解成小的问题，然后再递归求解；
分=>治，分的过程没有做实质的东西，主要为了下一步的治提供条件；合并的过程有很多次；
假如8条数据，需要排8-1次，如果8万数据，需要8万-1次，是一个线性增长，如果是冒泡排序，是一个平方的增长
```

```
一个索引i，指向左边有序序列的最前边，j指向右边有序序列的最前边，将i和j指向的数据进行比较，最小的放在另一个数组里边，这个数组是一个中转数组；放进来后，把j向后移动一下，再跟i比较，一直比较，一直放进元素；
当j往后移动后已经没有数据了，这时把i后边的数据拷贝到临时的temp数组里边去，再把临时数组里边的元素拷贝到原数组
```

```
	// 将索引从left到right范围的数组元素进行归并排序
	private static void sort(int[] data, int left, int right) {
		if(left < right){
			//找出中间索引
			int center = (left + right)/2;
			sort(data,left,center);
			sort(data,center+1,right);
			//合并
			merge(data,left,center,right);
		}
	}
	
		// 将两个数组进行归并，归并前两个数组已经有序，归并后依然有序
	private static void merge(int[] data, int left, int center, int right) {
		int[] tempArr = new int[data.length];
		int mid = center + 1;
		int third = left;
		int temp = left;
		while (left <= center && mid <= right) {
			if (data[left] - data[mid] <= 0) {
				tempArr[third++] = data[left++];
			} else {
				tempArr[third++] = data[mid++];
			}
		}
		while (mid <= right) {
			tempArr[third++] = data[mid++];
		}
		while (left <= center) {
			tempArr[third++] = data[left++];
		}
		while (temp <= right) {
			data[temp] = tempArr[temp++];
		}
	}
	
```

### 十、数据结构

```
https://blog.csdn.net/weixin_44617285/article/details/105507811
```

```
(1)
数组是最常用的数据结构，创建数组必须要内存中一块连续的空间，并且数组中必须存放相同的数据类型。比如我们创建一个长度为 10，数据类型为整型的数组，在内存中的地址是从 1000 开始，那么它在内存中的存储格式如下。
(2)
链表
不同于数组必须要连续的内存空间，链表可以使用零散的内存空间存储数据。不过，因为链表在内存中的数据不是连续的，所以链表中的每个数据元素都必须包含一个指向下一个数据元素的内存地址指针。如下图，链表的每个元素包含两部分，一部分是数据，一部分是指向下一个元素的地址指针。最后一个元素指向 null，表示链表到此为止

因为链表是不连续存储的，要想在链表中查找一个数据，只能遍历链表，所以链表的查找复杂度总是 O(N)。但是正因为链表是不连续存储的，所以在链表中插入或者删除一个数据是非常容易的，只要找到要插入（删除）的位置，修改链表指针就可以了。如图，想在 b 和 c 之间插入一个元素 x，只需要将 b 指向 c 的指针修改为指向 x，然后将 x 的指针指向 c 就可以了
相比在链表中轻易插入、删除一个元素这种简单的操作，如果我们要想在数组中插入、删除一个数据，就会改变数组连续内存空间的大小，需要重新分配内存空间，这样要复杂得多。
(3)
Hash 表
前面说过，对数组中的数据进行快速访问必须要通过数组的下标，时间复杂度为 O(1)。如果只知道数据或者数据中的部分内容，想在数组中找到这个数据，还是需要遍历数组，时间复杂度为 O(N)。

事实上，知道部分数据查找完整数据的需求在软件开发中会经常用到，比如知道了商品 ID，想要查找完整的商品信息；知道了词条名称，想要查找百科词条中的详细信息等。

这类场景就需要用到 Hash 表这种数据结构。Hash 表中数据以 Key、Value 的方式存储，上面例子中，商品 ID 和词条名称就是 Key，商品信息和词条详细信息就是 Value。存储的时候将 Key、Value 写入 Hash 表，读取的时候，只需要提供 Key，就可以快速查找到 Value。

Hash 表的物理存储其实是一个数组，如果我们能够根据 Key 计算出数组下标，那么就可以快速在数组中查找到需要的 Key 和 Value。许多编程语言支持获得任意对象的 HashCode，比如 Java 语言中 HashCode 方法包含在根对象 Object 中，其返回值是一个 Int。我们可以利用这个 Int 类型的 HashCode 计算数组下标。最简单的方法就是余数法，使用 Hash 表的数组长度对 HashCode 求余， 余数即为 Hash 表数组的下标，使用这个下标就可以直接访问得到 Hash 表中存储的 Key、Value

1、为什么JDK1.8会出现红黑树和链表共存呢？

因为当冲突比较严重时，table[index]下面的链表就会很长，那么会导致查找效率大大降低，而如果此时选用二叉树可以大大提高查询效率。
但是二叉树的结构又过于复杂，如果结点个数比较少的时候，那么选择链表反而更简单。
所以会出现红黑树和链表共存

2、get(key)
（1）计算key的hash值，用这个方法hash(key)
（2）找index = table.length-1 & hash;
（3）如果table[index]不为空，那么就挨个比较哪个Entry的key与它相同，就返回它的value

```

### 十一、HR面试准备

```
(1)
上海多维度网络科技有限公司  徐汇万科中心-F座 地铁1号线
漕宝路站 馨逸公寓	3人间，3000块钱每月
公司法人:雷勇，雷总
(2)
大一-大四课程
机械与电子工程学院
c语言，单片机原理与接口技术，自动控制原理，数据结构，信号与系统，数字电子技术
[电路，模拟电子技术，数字电子技术，信号与系统，电磁场与电磁波，通信原理，高频电子线路，数字信号处理，数据结构，自动控制原理，单片机原理与接口技术。]
(3)
实时2人，离线4人，组长1人
公司一共80左右人，
(4)
入职时间点
2018.8-2019.02
2019.04-2020.05
2020.07-2021.04
(5)
未来发展方向和职业规划？
大数据实时领域，离线领域更深层次研究，主要看重平台的发展
(6)
你的优缺点，从上家公司学到什么？跟谁交流最多？
思维严谨，爱钻研，比较喜欢数学，比较爱学习新的技术；
跟实时的同事交流比较多，互相学习，交流，解决一些技术难题
从上一家公司学到，实时数据流的处理，解决优化问题的能力
(7)
绩效是怎样考核的，你的职级
不是绩效考核给发的，是固定薪资是22k*14，我们是小公司，没有标准的评级
(8)
入职
1300 - 2000 - 4000 - 1500 - 1500 => 22000
(9)
离职原因：家是辽宁的，想要到北方这边发展
(10)
为什么选择字节？
首先我觉得字节是一个比较大的公司，有好的发展空间
其次我觉得自己对大数据开发有着几年相关的开发经验，有信心能够胜任咱们公司这个大数据岗位，有着较强的适应能力，相信自己能够快速熟悉公司业务，发展
(11)
对自己未来的规划？
希望自己在大数据开发领域不断的深入研究，相信用不了多久能够成为大数据方向的专家
(12)
项目后期通过什么进行维护?
代码维护一般是建立在高质量的代码上
服务器维护一般是建立在高质量的代码上
做数据的话，可能后期还是会定期查看备份数据的可用性
redis的rdb文件，要定期查看是否可用
(13)
平时通过什么方式来学习?
官网，社区，博客，b站，阿里一整套视频。Java编程思想，Java核心技术，鸟哥私房菜，算法，算法导论
(14)
项目上线的流程?
做完之后，提交代码测试，测试没问题了，然后就提交了，有问题再修改；
<1>
确定指标的业务口径，问清楚指标的具体定义，比如活跃用户，指的是设备id，还是用户id
<2>
需求评审，由产品经理设计主导原型，对于活跃主题，我们最终要展示的是最近n天的活跃用户数变化趋势，
大数据开发工程师，后端开发工程师，前端开发工程师一起参与，一起说明整个功能的价值和详细的操作流程，确保大家理解的一致性
<3>
大数据开发，大数据开发工程师，通过数据的同步的工具如flume，sqoop,等将数据同步到ODS层，然后就是一层一层的通过SQL计算到DWD，DWS，层，最后形成可为应用直接服务的数据，填充到ADS层
<4>
后端开发，
后端工程师负责，为大数据提供业务数据接口，
同时还负责读取ADS层数据写入MySQL中的数据
<5>
前端工程师负责，前端埋点，对分析的数据结果进行可视化展示
<6>
联调大数据开发工程师，前端开发工程师，后端开发工程师都要参与进来，此时会要求大数据开发工程师基于历史的而数据执行计算任务，大数据开发工程师承担数据准确性的校验。
前后端解决用户操作的相关BUG保证不出现低级的问题完成自测
<7>
测试
测试工程师对整个大数据系统进行测试，测试的手段包括，边界值，等价类等
提交测试的软件，
一周开发代码，2周测试时间
<8>
上线运维工程师会配合我们的前后端开发工程师，更新最新版本到服务器；
此时的产品经理要找到该指标的负责人长期跟进指标的准确性；重要的指标还需要一个周期内部再次验证，从而保证数据的准确性
(15)
刚入职的话一个需求大概需要7天，对业务熟悉后，平均一天一个需求
影响时间的因素:对业务的熟悉，开会讨论需求，敲定一些细节字段，测试
(16)
敏捷开发
一个月迭代一次，每月都有节日，元旦，春节，情人节，3.8妇女节，端午节，618，国庆，中秋，周末，新产品，新区域
新产品，我们提出优化需求，然后评估时间。每周我们都会开会做下周计划，本周总结，任务完成情况；
(日报，周报，月报，季度报，年报)需求一周的时间，周三一定完成，自己学习工作额外的技术，flink,hudi
(17)
项目开发过程中每天都在做什么事
新需求，活动，优化，新产品，新市场
故障分析:数仓任何的步骤出现问题，都需要查看问题，比如日活，月活快速下降或者快速上升
新技术的预言：flink，数仓建模，数据质量，元数据管理
临时任务
晨会，做操
(18)
项目是否上线，上线的时间？
已上线，
(19)
老根分享
写一些实时指标，临时需求东西销量订单这东西能不能做，大概怎么做，哪几个字段聚合哪几张表；
约一个小会，产品找到你，哪个部门的哪些需求，这东西能不能做，约一个小会，对一些需求；如果没有不能做的可能，产品描述需求的时候，统计UV从哪个表的哪个字段能拿到，怎样关联哪个表，把这些东西东西拿到，反馈能做，评估下时间，排期；几个相似的字段，不清晰按照哪个统计，还需要再向需求方敲定字段；需求并行，同时写很多需求
```

### 十二、git

```
(1)初始化仓库
git init => 显示隐藏文件夹 => .git 就是git的整个仓库[包括仓库的代码，历史的代码，配置项]
(2)用户签名
主要目的，知道是谁的代码
git config user.name "tongwei"
git config user.email "tongweifly0@163.com"
全局生效：
git config --global user.name "tongwei" => 默认路径C:\Users\lenovo\.gitconfig
用global配置签名，是全window生效，本地和global都配置，优先使用本地
(3)
git status
红色：未追踪
绿色：进入 暂存区(未提交状态)
git add 跟文件或者目录
git	commit 提交所有进入暂存区的文件 => create 01 => :wq
git	commit -m "update 02"
(4)
git status 显示新增/修改的(modified),不显示已提交成功的
(5)
提交目录的场景谨慎使用，因为可能提交一些不太想要提交的东西，比如.bak，.tmp可能是生成的临时文件 => 粗糙，让人觉得你做事情马马虎虎
(6)
查看提交的版本,一共提交了几次
git log
谁什么时间提的，
每次提交都有一个版本的唯一编号
git log --pretty=oneline
master是一个主分支
head是一个指针，指向整个版本的最新记录
(7)
还原上一个版本
git reset --hard HEAD^ => 回退一个版本
git reset --hard HEAD~2 => 回退二个版本
hard => 全局性的修改，改了3个地方，git库里边指针改到前一版本，缓存区和本地就是修改成为前一版本
(8)
git log:服务器版本状态,实际真是存在的
git reflog:操作日志，记录每次干的什么事，
git reset --hard 版本号 => 直接到达这个版本
(9)
留痕的特征都放在.git里边
(10)
建立免密登录账号
私钥和公钥
cd到windows的user账号
先查找一下，有没有.ssh目录
cd .ssh => ssh-keygen -t rsa -C tongyipush777888@126.com => 一直回车
cd .ssh => vim id_rsa.pub => 复制里边内容 => github账号下找setting => SSH and GPG keys => title(随便起个名字ssh),key[把公钥复制进来]
原来往仓库提交代码都使用https的,事=是一次性的，每次都得填用户名，密码；
要使用带加密的方式的使用ssh的，把路径复制下来 => 提交代码[git add => git commit -m "xxxx" => git origin master(非免密)/git remote add originssh ssh的路径 => git push originssh master]
(11)
git的删除和还原
git reset --hard head 指的是把整个库的文件按照最新的版本还原，但是有的文件更改后，不想还原
git checkout -- xxx.txt 还原 拿暂存区的代码 覆盖工作区
假如提交代码的时候不小心把没用的代码提交了，怎样把库里的文件删除呢？
先在本地把那个不需要的文件删除[src/xxx.bak] => git status => git add src/xxx.bak[把删除动作当作一种变化] => git commit -m "update00" => git log --pretty=oneline => 只能猫盖屎，把之前的版本覆盖了一下 
```

```
reset --hard HEAD^ 指针还原针对整个版本库全部还原
删除文件就是把一个删除操作进行提交，暂存区会覆盖，本地库不会覆盖掉，会再增加一个不包含删除文件的新文件，并把指针指向这个新文件
```

![](D:\Code\My_Java_Code\总结\大数据笔记图片\批注 2021-05-23 213829.png)

```
(12)
分支
git branch 分支名
git branch -v => 查看分支 dev分支和master分支 => git checkout dev[切换某一个分支] 

创建分支：git branch dev
切换到dev分支:git checkout dev
查看分支情况：git branch -v
在该分支下创建文件:touch src/com/Order.java
添加文件到暂存区：git add src/com.Order.java
提交文件到本地库：git commit -m "Order"
只有dev分支有Order.java代码，当git checkout master[切换到master分支后，没有Order.java代码；它把代码放到了本地库里边]
两个分支的代码是彼此独立的，相互之间不影响的
(13)
当分支开发完成之后，怎样回归到主线？
合并的过程是拉取的过程，不是推送的过程
切换到主线gti checkout master
站在master上把新功能拉取过来，
站在master上，把当前dev的最新功能拉取到我这里 => git merge dev
git bracmch -v => 两条支流都已经同步成最新的了
(14)
冲突：两个分支可能改的是同一个文件，修正bug和另外一个程序员开发程序有可能是改的同一个类，同一行，可能修改了bug或者在sql中增加了一个字段，在合并的时候，假设修改bug是先到的，程序是后到的，版本管理软件不能认为后到的比前面的版本新，后到的把先到的给覆盖掉，不能；
(15)
两个分支各自修改了各自文件的同一串代码?怎样解决冲突
主线修改代码
git branch -v
vim pom.xml => 增加一串AAAAAAAAAAA => :wq => git add pom.xml => git commit -m "AAA"
分支dev修改代码
git branch -v
git checkout dev => vim pom.xml => 增加一串BBBBBB => :wq => git add pom.xml => git commit -m "BBBBB" 
有一天要把dev的数据往master上边合？
git checkout master => git merge dev => 
[conflict:Merge conflict in pom.xml Automatic merge failed;fix conflicts and then commit the result]冲突，在pom文件里边冲突，自动合并失败，需要手动合并
master|Merging merging的情况下，不允许做任何其他提交代码的操作的，必须优先把冲突解决掉
(16)
解决冲突
检查出现冲突的文件 => 上签，中签，分隔 => HEAD表示当前所在的分支就是master，下边是dev的代码 => 手工修改代码 => git add pom.xml => git commit "merge"
(17)
github远端操作
<1>
[岳不群]
搭建代码库 => git init git config
提交代码  => git add xxx => git commit
github准备工作 => 注册github账号 在GitHub搭建项目
推送代码到远端 => 1.git remote add origin <url> /2.git push origin master
<2>
[令狐冲]
入职把基础代码down下来，git clone <url>把整个库克隆到本地/克隆到工作空间
=> 改代码
=> git add => git commit 传到本地库
=> git push 推回到远端
<3>
[岳不群]
git pull 把代码拉取下来
<4>
git pull 和 git clone <url>的区别？
都是拉取，区别是，自己本地有没已经存在的库，新来的没有库，就只能使用git clone <url>,把库复制沾到本地
git pull 是更新，从有变成更有
(18)
实战
<1>
建库 => git init
设置签名 => git config user.name "tongwei"
设置签名 => git config user.email "tongweifly0@163.com"
添加到暂存区 => git add bixiejianfa.java
添加到本地库 => git commit -m "create jianfa"
Repositories仓库的意思
git remote add origin https://github.com/tongyipush/tuxidabaojian.git
git push origin master
用户名(github用户名) => tongyipush
密码(github密码) => tongyipush777888
<2>
从远端拉取代码
git log查看签名
从本地文件地址打开git => git clone https://github.com/tongyipush/tuxidabaojian.git [可重新命名] => down 下来之后，目录有了，代码有了仓库也有了 => 修改代码
修改签名 => git config user.name "linghuchong" git config user.email "zhangwuji777888@126.com" => git add => git commit -m "genggai"
在岳不群账号 => setting => manage access => invite a collaborator => 在令狐冲界面粘贴邀请函地址 => 接受邀请
git push origin master => 令狐冲的用户名，令狐冲的密码
<3>
令狐冲更新完代码后，岳不群从远端更新修改过的代码
再从远端拉取令狐冲修改的密码，git pull origin master
<远端操作总结>
上传 => git push origin 分支名
更新 => git pull origin 分支名
设置别名 => git remote add origin url
远端复制 => git clone url 目录名
(19)
协作冲突
两个人同时修改代码，改的是同一个地方，一个人把代码oush到远端后，另一个人再push的时候就会push不上去[不会报错，但是会告诉你版本变低了] 
hint: Updates were rejected because the remote contains work that you do
hint: not have locally. This is usually caused by another repository pushing
hint: to the same ref. You may want to first integrate the remote changes
hint: (e.g., 'git pull ...') before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.
令狐冲本地更新一下代码 => git pull origin master
一更新不要紧，出现状况，有冲突
remote: Enumerating objects: 21, done.
remote: Counting objects: 100% (21/21), done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 11 (delta 3), reused 11 (delta 3), pack-reused 0
Unpacking objects: 100% (11/11), done.
From https://github.com/tongyipush/tuxidabaojian
 * branch            master     -> FETCH_HEAD
   edc2739..c61b603  master     -> origin/master
Auto-merging src/main/java/com/atguigu/redis/RedisUtil.java
CONFLICT (content): Merge conflict in src/main/java/com/atguigu/redis/RedisUtil.java
Automatic merge failed; fix conflicts and then commit the result.
解决冲突 => git add => git commit -m "merge conflict" => git push origin master


```



![](D:\Code\My_Java_Code\总结\大数据笔记图片\批注 2021-05-29 134706.png)

```
(20)
fork & pull request
[这种审查模式避免让非信任团队直接进入项目组]
对于开发人员来说有一个非信任团队，非信任人员，这样就不能把他加入到合作伙伴的组中，每次它提交代码的时候必须经我们这边审查
=> 利用GitHub的另外的机制fork，将代码fork一个镜像的文件，到这个非信任的人的名下
=> git clone url
=> 修改代码
=> git config user.name
=> git config user.email
=> git add => git commit -m "" 
=> git push origin master
=> pull request
(21)
岳不群端
审查 => 解决冲突resolve conflicts => mark asresolved
=> commit merge => commiting merge => 合并代码merge pull request，那么东方不败成为代码的贡献者
```



```
(22)
idea git 提交
create new project 
=> meaven 
=> groupid:com.betes,Artifactld:huashan_jianfa,
=> project name:huashanjianfa,project location:d:\git\linghuchong\huashanjianfa
=> 在Java包里创建一些代码
=> file > setting > version control > git > 检查地址D:\ProgramFiles\Git\bin\bash.exe > test
=> 把项目初始化 > VCS > import into version control > create git repository > 选择项目
[idea 文件状态的颜色
红色：未追踪的文件
绿色：进入暂存区
黑色：已提交到本地库中，而且未被修改过
蓝色：已提交到本地库中，之后被修改过]
=> 右键代码块 ，.pom.xml=> Git => add 
.idea和.iml看公司的要求，不一定需要提交
=> git => commit file => 选择提交的文件，commit message(提交的信息，版本说明)：增加三仙剑
=> 签名Aythor:tongyipush<tongyipush777888@126.com> => commit
=> 远端github => new => repository name:huashanjianfa0921 
=> 传远端 => 右键代码块 => git => reppository => remotes => + => origin,url
=> git => repository => push => username:tongyipush ,password:tongyipush777888 => 
=> file => setting => Appearance&behavior => system setting => password => 
do not save,forget passwords after restart[表示idea不关，这个密码是一直记下来的，一旦idea关了，密码需要重新填写一下]

(23)
idea git下载代码 => new => project from version control => git => 

(24)
idea冲突解决
第一次手动合并 => merge => apply => 二次提交push
```



```
git checkout -- xxx.txt => 还原，拿暂存区的代码 覆盖 工作区
git 删除
先物理delete掉 => git add + git commit => 用户新的版本覆盖老的版本
只把暂存区的文件删除，只保留工作区中的原始文件
git rm --cached xxx.txt => git commit
分支，并行任务
git branch 分支名 新建分支
Git branch -v  查看所有的分支 当前所在的位置
git checkout 分支名 切换分支
git checkout -b 分支名 即新建分支名，又同时切换
git merge 分支名 要站在目的分支上合并功能分支 进行合并

git hub
提交远端
要在远端建立账号和空仓库
git remote add origin 仓库地址 => 给仓库地址启用别名
git push origin master => 推送分支（实质只是推送分支新增加的几次版本变化，是增量推送）
下载（全量）
git clone 仓库地址 本地目录名
更新（更新）
git pull origin master

关于提交远端失败
1 缺少权限（申请权限，加入项目成员）
2 提交的版本旧了，(git pull 拉取最新的代码，然后再次提交（拉取时有可能出现冲突，解决冲突）)

冲突解决
分支合并 多人同时提交一个文件
1 修改发生冲突的文件，仔细阅读代码
2 git add => git commit => git push origin master
非信任提交
folk pullrequest
folk到自己的名下
把修改提交到自己名下的项目上
再用自己名下的项目发送pr 到 上游项目[等待审核]

idea使用git
命令变成使用菜单了
设置好git.exe

关于分支
master 主分支
develope 开发分支
release 分布分支
hotfix 修理分支
feature 功能分支

开发一个功能：
1 开发人员建立该功能的分支
2 在该功能分支上进行代码的编写
3 开发完成中，提交分支
4 开发完成，由骨干人员下载该分支，合并到主分支中
5 骨干人员上传主分支
```



```
(25)
gitflow
工作流
简单分支：主分支+开发分支

```

![](D:\Code\My_Java_Code\总结\大数据笔记图片\批注 2021-05-31 224308.png)



```
git 实操
怎样down下来分支
huanshanjianfa0921 => git => reposity => pull => origin/new master => Checkout as =>
新建一个本地重名分支，跟远端仓库是对应的 => new jianfa => 
合并功能
站在master分支上merge => merge into current => git push 
```

### 十三、面试

```
(1)
离线4，实时2，组长1人，前端2
我原来是做离线的，去年7月份从离线转到实时
1000多万用户
```

十四、

```
数仓脚本调度
指标填写
跑指标数据任务
```

